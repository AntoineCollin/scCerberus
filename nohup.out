2024-10-22 22:24:04.591264: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-10-22 22:24:04.604372: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-10-22 22:24:04.619821: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-10-22 22:24:04.624646: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-10-22 22:24:04.636245: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-10-22 22:24:07.565879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 27627 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:37:00.0, compute capability: 8.0
/home/becavin/scMusketeers/scmusketeers
tf [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
Use Neptune.ai log : True
Use Neptune project name = sc-musketeers
[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/becavin-lab/sc-musketeers/e/SCMUS-5
/home/becavin/scMusketeers/scmusketeers/transfer/dataset_tf.py:238: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.
  self.adata_train_extended.obs["train_split"] = spl.values
2024-10-22 22:31:13.246426: W external/local_tsl/tsl/framework/bfc_allocator.cc:482] Allocator (GPU_0_bfc) ran out of memory trying to allocate 15.70GiB (rounded to 16862984704)requested by op MatMul
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 
Current allocation summary follows.
Current allocation summary follows.
2024-10-22 22:31:13.246959: I external/local_tsl/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for GPU_0_bfc
2024-10-22 22:31:13.247014: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (256): 	Total Chunks: 2943, Chunks in use: 2934. 735.8KiB allocated for chunks. 733.5KiB in use in bin. 15.5KiB client-requested in use in bin.
2024-10-22 22:31:13.247042: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (512): 	Total Chunks: 13, Chunks in use: 10. 7.2KiB allocated for chunks. 5.8KiB in use in bin. 5.0KiB client-requested in use in bin.
2024-10-22 22:31:13.247062: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (1024): 	Total Chunks: 4, Chunks in use: 1. 5.8KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.
2024-10-22 22:31:13.247087: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (2048): 	Total Chunks: 1, Chunks in use: 0. 2.0KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-10-22 22:31:13.247106: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (4096): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-10-22 22:31:13.247128: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (8192): 	Total Chunks: 3, Chunks in use: 1. 35.8KiB allocated for chunks. 13.0KiB in use in bin. 10.8KiB client-requested in use in bin.
2024-10-22 22:31:13.247148: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (16384): 	Total Chunks: 3, Chunks in use: 3. 53.2KiB allocated for chunks. 53.2KiB in use in bin. 42.8KiB client-requested in use in bin.
2024-10-22 22:31:13.247186: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (32768): 	Total Chunks: 7, Chunks in use: 2. 319.5KiB allocated for chunks. 64.0KiB in use in bin. 64.0KiB client-requested in use in bin.
2024-10-22 22:31:13.247206: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (65536): 	Total Chunks: 7, Chunks in use: 0. 497.5KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-10-22 22:31:13.247228: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (131072): 	Total Chunks: 6, Chunks in use: 1. 979.2KiB allocated for chunks. 172.5KiB in use in bin. 172.4KiB client-requested in use in bin.
2024-10-22 22:31:13.247249: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (262144): 	Total Chunks: 3, Chunks in use: 1. 1.05MiB allocated for chunks. 373.2KiB in use in bin. 373.2KiB client-requested in use in bin.
2024-10-22 22:31:13.247273: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (524288): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-10-22 22:31:13.247291: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-10-22 22:31:13.247309: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-10-22 22:31:13.247328: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-10-22 22:31:13.247349: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): 	Total Chunks: 1, Chunks in use: 1. 15.67MiB allocated for chunks. 15.67MiB in use in bin. 15.67MiB client-requested in use in bin.
2024-10-22 22:31:13.247370: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): 	Total Chunks: 4, Chunks in use: 4. 80.61MiB allocated for chunks. 80.61MiB in use in bin. 68.56MiB client-requested in use in bin.
2024-10-22 22:31:13.247395: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): 	Total Chunks: 4, Chunks in use: 3. 180.38MiB allocated for chunks. 121.10MiB in use in bin. 91.51MiB client-requested in use in bin.
2024-10-22 22:31:13.247414: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-10-22 22:31:13.247437: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): 	Total Chunks: 1, Chunks in use: 0. 233.71MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-10-22 22:31:13.247458: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): 	Total Chunks: 2, Chunks in use: 1. 16.00GiB allocated for chunks. 15.70GiB in use in bin. 15.70GiB client-requested in use in bin.
2024-10-22 22:31:13.247481: I external/local_tsl/tsl/framework/bfc_allocator.cc:1062] Bin for 15.70GiB was 256.00MiB, Chunk State: 
2024-10-22 22:31:13.247512: I external/local_tsl/tsl/framework/bfc_allocator.cc:1068]   Size: 302.20MiB | Requested Size: 10.8KiB | in_use: 0 | bin_num: 20, prev:   Size: 15.70GiB | Requested Size: 15.70GiB | in_use: 1 | bin_num: -1
2024-10-22 22:31:13.247530: I external/local_tsl/tsl/framework/bfc_allocator.cc:1075] Next region of size 17179869184
2024-10-22 22:31:13.247551: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7eec80000000 of size 16862984704 next 42
2024-10-22 22:31:13.247575: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7ef06d1cba00 of size 316884480 next 18446744073709551615
2024-10-22 22:31:13.247607: I external/local_tsl/tsl/framework/bfc_allocator.cc:1075] Next region of size 268435456
2024-10-22 22:31:13.247626: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd0000000 of size 382208 next 1649
2024-10-22 22:31:13.247670: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd005d500 of size 22212864 next 148
2024-10-22 22:31:13.247687: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158c600 of size 256 next 22
2024-10-22 22:31:13.247702: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158c700 of size 256 next 621
2024-10-22 22:31:13.247715: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158c800 of size 256 next 684
2024-10-22 22:31:13.247729: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158c900 of size 256 next 816
2024-10-22 22:31:13.247744: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158ca00 of size 256 next 353
2024-10-22 22:31:13.247759: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158cb00 of size 256 next 850
2024-10-22 22:31:13.247773: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158cc00 of size 256 next 818
2024-10-22 22:31:13.247787: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158cd00 of size 256 next 828
2024-10-22 22:31:13.247802: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158ce00 of size 256 next 729
2024-10-22 22:31:13.247816: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158cf00 of size 256 next 811
2024-10-22 22:31:13.247830: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158d000 of size 256 next 666
2024-10-22 22:31:13.247845: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158d100 of size 256 next 743
2024-10-22 22:31:13.247859: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158d200 of size 256 next 585
2024-10-22 22:31:13.247874: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158d300 of size 256 next 661
2024-10-22 22:31:13.247888: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158d400 of size 256 next 589
2024-10-22 22:31:13.247902: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158d500 of size 256 next 740
2024-10-22 22:31:13.247917: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158d600 of size 256 next 842
2024-10-22 22:31:13.247931: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158d700 of size 256 next 693
2024-10-22 22:31:13.247945: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158d800 of size 256 next 717
2024-10-22 22:31:13.247959: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158d900 of size 256 next 734
2024-10-22 22:31:13.247976: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158da00 of size 256 next 809
2024-10-22 22:31:13.247990: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158db00 of size 256 next 798
2024-10-22 22:31:13.248006: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158dc00 of size 256 next 776
2024-10-22 22:31:13.248020: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158dd00 of size 256 next 748
2024-10-22 22:31:13.248035: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158de00 of size 256 next 847
2024-10-22 22:31:13.248049: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158df00 of size 256 next 754
2024-10-22 22:31:13.248063: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f1dd158e000 of size 512 next 745
2024-10-22 22:31:13.248077: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158e200 of size 256 next 819
2024-10-22 22:31:13.248092: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158e300 of size 256 next 868
2024-10-22 22:31:13.248108: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158e400 of size 256 next 815
2024-10-22 22:31:13.248131: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158e500 of size 256 next 915
2024-10-22 22:31:13.248146: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158e600 of size 256 next 861
2024-10-22 22:31:13.248161: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158e700 of size 256 next 844
2024-10-22 22:31:13.248176: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158e800 of size 256 next 784
2024-10-22 22:31:13.248189: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158e900 of size 256 next 893
2024-10-22 22:31:13.248203: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158ea00 of size 256 next 908
2024-10-22 22:31:13.248218: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158eb00 of size 256 next 910
2024-10-22 22:31:13.248233: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158ec00 of size 256 next 917
2024-10-22 22:31:13.248247: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158ed00 of size 256 next 900
2024-10-22 22:31:13.248261: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158ee00 of size 256 next 914
2024-10-22 22:31:13.248275: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158ef00 of size 256 next 796
2024-10-22 22:31:13.248289: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158f000 of size 256 next 876
2024-10-22 22:31:13.248303: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158f100 of size 256 next 843
2024-10-22 22:31:13.248317: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158f200 of size 256 next 682
2024-10-22 22:31:13.248332: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158f300 of size 256 next 845
2024-10-22 22:31:13.248346: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158f400 of size 256 next 878
2024-10-22 22:31:13.248360: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158f500 of size 256 next 953
2024-10-22 22:31:13.248374: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158f600 of size 256 next 901
2024-10-22 22:31:13.248389: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158f700 of size 256 next 542
2024-10-22 22:31:13.248404: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158f800 of size 256 next 888
2024-10-22 22:31:13.248419: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158f900 of size 256 next 890
2024-10-22 22:31:13.248433: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158fa00 of size 256 next 891
2024-10-22 22:31:13.248448: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158fb00 of size 256 next 832
2024-10-22 22:31:13.248462: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158fc00 of size 256 next 836
2024-10-22 22:31:13.248477: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158fd00 of size 256 next 859
2024-10-22 22:31:13.248491: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158fe00 of size 256 next 867
2024-10-22 22:31:13.248506: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd158ff00 of size 256 next 821
2024-10-22 22:31:13.248521: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1590000 of size 256 next 714
2024-10-22 22:31:13.248535: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1590100 of size 256 next 872
2024-10-22 22:31:13.248549: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1590200 of size 256 next 782
2024-10-22 22:31:13.248564: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1590300 of size 256 next 938
2024-10-22 22:31:13.248579: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1590400 of size 256 next 939
2024-10-22 22:31:13.248601: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1590500 of size 256 next 669
2024-10-22 22:31:13.248616: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1590600 of size 256 next 614
2024-10-22 22:31:13.248630: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1590700 of size 256 next 695
2024-10-22 22:31:13.248645: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1590800 of size 256 next 658
2024-10-22 22:31:13.248659: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1590900 of size 256 next 751
2024-10-22 22:31:13.248673: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1590a00 of size 256 next 721
2024-10-22 22:31:13.248688: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1590b00 of size 256 next 674
2024-10-22 22:31:13.248703: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1590c00 of size 256 next 712
2024-10-22 22:31:13.248717: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1590d00 of size 256 next 454
2024-10-22 22:31:13.248731: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1590e00 of size 256 next 452
2024-10-22 22:31:13.248746: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1590f00 of size 256 next 785
2024-10-22 22:31:13.248761: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1591000 of size 256 next 487
2024-10-22 22:31:13.248775: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1591100 of size 256 next 611
2024-10-22 22:31:13.248789: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1591200 of size 256 next 706
2024-10-22 22:31:13.248804: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1591300 of size 256 next 639
2024-10-22 22:31:13.248818: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1591400 of size 256 next 605
2024-10-22 22:31:13.248832: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1591500 of size 256 next 758
2024-10-22 22:31:13.248847: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1591600 of size 256 next 722
2024-10-22 22:31:13.248862: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1591700 of size 256 next 732
2024-10-22 22:31:13.248877: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1591800 of size 256 next 952
2024-10-22 22:31:13.248891: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1591900 of size 256 next 882
2024-10-22 22:31:13.248905: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1591a00 of size 256 next 787
2024-10-22 22:31:13.248920: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1591b00 of size 256 next 788
2024-10-22 22:31:13.248934: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1591c00 of size 256 next 750
2024-10-22 22:31:13.248948: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1591d00 of size 256 next 961
2024-10-22 22:31:13.248962: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1591e00 of size 256 next 789
2024-10-22 22:31:13.248977: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1591f00 of size 256 next 746
2024-10-22 22:31:13.248991: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1592000 of size 256 next 726
2024-10-22 22:31:13.249006: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1592100 of size 256 next 675
2024-10-22 22:31:13.249020: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1592200 of size 256 next 772
2024-10-22 22:31:13.249034: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1592300 of size 256 next 803
2024-10-22 22:31:13.249049: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1592400 of size 256 next 886
2024-10-22 22:31:13.249071: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1592500 of size 256 next 781
2024-10-22 22:31:13.249085: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1592600 of size 256 next 820
2024-10-22 22:31:13.249100: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1592700 of size 256 next 822
2024-10-22 22:31:13.249114: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1592800 of size 256 next 727
2024-10-22 22:31:13.249129: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1592900 of size 256 next 825
2024-10-22 22:31:13.249142: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1592a00 of size 256 next 718
2024-10-22 22:31:13.249157: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1592b00 of size 256 next 812
2024-10-22 22:31:13.249172: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1592c00 of size 256 next 283
2024-10-22 22:31:13.249186: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1592d00 of size 256 next 581
2024-10-22 22:31:13.249200: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1592e00 of size 256 next 780
2024-10-22 22:31:13.249215: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1592f00 of size 256 next 851
2024-10-22 22:31:13.249229: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1593000 of size 256 next 894
2024-10-22 22:31:13.249243: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1593100 of size 256 next 906
2024-10-22 22:31:13.249257: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1593200 of size 256 next 436
2024-10-22 22:31:13.249272: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1593300 of size 256 next 835
2024-10-22 22:31:13.249287: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1593400 of size 256 next 562
2024-10-22 22:31:13.249301: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1593500 of size 256 next 778
2024-10-22 22:31:13.249315: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1593600 of size 256 next 805
2024-10-22 22:31:13.249330: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1593700 of size 256 next 800
2024-10-22 22:31:13.249345: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1593800 of size 256 next 797
2024-10-22 22:31:13.249359: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1593900 of size 256 next 814
2024-10-22 22:31:13.249373: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1593a00 of size 256 next 874
2024-10-22 22:31:13.249388: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1593b00 of size 256 next 770
2024-10-22 22:31:13.249403: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1593c00 of size 256 next 685
2024-10-22 22:31:13.249417: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1593d00 of size 256 next 762
2024-10-22 22:31:13.249431: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1593e00 of size 256 next 779
2024-10-22 22:31:13.249445: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1593f00 of size 256 next 786
2024-10-22 22:31:13.249460: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1594000 of size 256 next 735
2024-10-22 22:31:13.249474: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1594100 of size 256 next 736
2024-10-22 22:31:13.249488: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1594200 of size 256 next 791
2024-10-22 22:31:13.249503: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1594300 of size 256 next 708
2024-10-22 22:31:13.249528: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1594400 of size 256 next 760
2024-10-22 22:31:13.249543: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1594500 of size 256 next 773
2024-10-22 22:31:13.249557: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1594600 of size 256 next 183
2024-10-22 22:31:13.249572: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1594700 of size 256 next 193
2024-10-22 22:31:13.249586: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1594800 of size 256 next 391
2024-10-22 22:31:13.249601: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1594900 of size 256 next 534
2024-10-22 22:31:13.249615: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1594a00 of size 256 next 538
2024-10-22 22:31:13.249630: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1594b00 of size 256 next 593
2024-10-22 22:31:13.249645: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1594c00 of size 256 next 501
2024-10-22 22:31:13.249659: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1594d00 of size 256 next 358
2024-10-22 22:31:13.249673: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1594e00 of size 256 next 504
2024-10-22 22:31:13.249695: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1594f00 of size 256 next 584
2024-10-22 22:31:13.249710: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1595000 of size 256 next 268
2024-10-22 22:31:13.249725: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1595100 of size 256 next 628
2024-10-22 22:31:13.249739: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1595200 of size 256 next 607
2024-10-22 22:31:13.249754: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1595300 of size 256 next 618
2024-10-22 22:31:13.249768: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1595400 of size 256 next 644
2024-10-22 22:31:13.249783: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1595500 of size 256 next 679
2024-10-22 22:31:13.249797: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1595600 of size 256 next 580
2024-10-22 22:31:13.249812: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1595700 of size 256 next 558
2024-10-22 22:31:13.249826: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1595800 of size 256 next 641
2024-10-22 22:31:13.249841: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1595900 of size 256 next 623
2024-10-22 22:31:13.249855: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1595a00 of size 256 next 583
2024-10-22 22:31:13.249874: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1595b00 of size 256 next 492
2024-10-22 22:31:13.249890: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1595c00 of size 256 next 627
2024-10-22 22:31:13.249904: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1595d00 of size 256 next 530
2024-10-22 22:31:13.249918: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1595e00 of size 256 next 323
2024-10-22 22:31:13.249933: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1595f00 of size 256 next 624
2024-10-22 22:31:13.249947: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1596000 of size 256 next 777
2024-10-22 22:31:13.249961: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1596100 of size 256 next 556
2024-10-22 22:31:13.249975: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1596200 of size 256 next 597
2024-10-22 22:31:13.249990: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1596300 of size 256 next 663
2024-10-22 22:31:13.250014: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1596400 of size 256 next 653
2024-10-22 22:31:13.250030: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1596500 of size 256 next 592
2024-10-22 22:31:13.250043: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1596600 of size 256 next 567
2024-10-22 22:31:13.250059: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1596700 of size 256 next 667
2024-10-22 22:31:13.250073: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1596800 of size 256 next 491
2024-10-22 22:31:13.250087: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1596900 of size 256 next 668
2024-10-22 22:31:13.250101: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1596a00 of size 256 next 741
2024-10-22 22:31:13.250116: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1596b00 of size 256 next 898
2024-10-22 22:31:13.250131: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1596c00 of size 256 next 994
2024-10-22 22:31:13.250145: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1596d00 of size 256 next 1086
2024-10-22 22:31:13.250160: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1596e00 of size 256 next 1118
2024-10-22 22:31:13.250175: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1596f00 of size 256 next 1434
2024-10-22 22:31:13.250189: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1597000 of size 256 next 1512
2024-10-22 22:31:13.250203: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1597100 of size 256 next 1101
2024-10-22 22:31:13.250217: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1597200 of size 256 next 1569
2024-10-22 22:31:13.250233: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1597300 of size 256 next 1583
2024-10-22 22:31:13.250247: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1597400 of size 256 next 1613
2024-10-22 22:31:13.250263: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1597500 of size 256 next 1412
2024-10-22 22:31:13.250278: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1597600 of size 256 next 133
2024-10-22 22:31:13.250293: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1597700 of size 256 next 1643
2024-10-22 22:31:13.250308: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1597800 of size 256 next 1635
2024-10-22 22:31:13.250322: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1597900 of size 256 next 1485
2024-10-22 22:31:13.250337: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1597a00 of size 256 next 1587
2024-10-22 22:31:13.250351: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1597b00 of size 256 next 1696
2024-10-22 22:31:13.250366: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1597c00 of size 256 next 1651
2024-10-22 22:31:13.250381: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1597d00 of size 256 next 1659
2024-10-22 22:31:13.250395: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1597e00 of size 256 next 1593
2024-10-22 22:31:13.250410: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1597f00 of size 256 next 1634
2024-10-22 22:31:13.250425: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1598000 of size 256 next 107
2024-10-22 22:31:13.250439: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1598100 of size 256 next 98
2024-10-22 22:31:13.250453: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1598200 of size 256 next 1682
2024-10-22 22:31:13.250473: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1598300 of size 256 next 57
2024-10-22 22:31:13.250494: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1598400 of size 256 next 1666
2024-10-22 22:31:13.250509: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1598500 of size 256 next 1658
2024-10-22 22:31:13.250523: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1598600 of size 256 next 1532
2024-10-22 22:31:13.250538: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1598700 of size 256 next 1556
2024-10-22 22:31:13.250553: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1598800 of size 256 next 119
2024-10-22 22:31:13.250567: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1598900 of size 256 next 1642
2024-10-22 22:31:13.250581: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1598a00 of size 256 next 1547
2024-10-22 22:31:13.250600: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1598b00 of size 256 next 1598
2024-10-22 22:31:13.250615: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1598c00 of size 256 next 1576
2024-10-22 22:31:13.250630: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1598d00 of size 256 next 1677
2024-10-22 22:31:13.250645: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1598e00 of size 256 next 1409
2024-10-22 22:31:13.250660: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1598f00 of size 256 next 1456
2024-10-22 22:31:13.250674: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1599000 of size 256 next 1660
2024-10-22 22:31:13.250689: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1599100 of size 256 next 1654
2024-10-22 22:31:13.250703: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1599200 of size 256 next 1678
2024-10-22 22:31:13.250718: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1599300 of size 256 next 1663
2024-10-22 22:31:13.250732: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1599400 of size 256 next 1667
2024-10-22 22:31:13.250747: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1599500 of size 256 next 1606
2024-10-22 22:31:13.250762: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1599600 of size 256 next 1510
2024-10-22 22:31:13.250782: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1599700 of size 256 next 1520
2024-10-22 22:31:13.250797: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1599800 of size 256 next 1563
2024-10-22 22:31:13.250811: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1599900 of size 256 next 61
2024-10-22 22:31:13.250825: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1599a00 of size 256 next 80
2024-10-22 22:31:13.250840: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1599b00 of size 256 next 1636
2024-10-22 22:31:13.250855: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1599c00 of size 256 next 1656
2024-10-22 22:31:13.250869: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1599d00 of size 256 next 1562
2024-10-22 22:31:13.250884: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1599e00 of size 256 next 1579
2024-10-22 22:31:13.250898: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1599f00 of size 256 next 1597
2024-10-22 22:31:13.250913: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159a000 of size 256 next 58
2024-10-22 22:31:13.250927: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159a100 of size 256 next 1474
2024-10-22 22:31:13.250942: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159a200 of size 256 next 108
2024-10-22 22:31:13.250966: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159a300 of size 256 next 1524
2024-10-22 22:31:13.250982: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159a400 of size 256 next 1611
2024-10-22 22:31:13.250996: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159a500 of size 256 next 1614
2024-10-22 22:31:13.251009: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159a600 of size 256 next 1604
2024-10-22 22:31:13.251024: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159a700 of size 256 next 1572
2024-10-22 22:31:13.251039: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159a800 of size 256 next 1612
2024-10-22 22:31:13.251054: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159a900 of size 256 next 1581
2024-10-22 22:31:13.251068: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159aa00 of size 256 next 1473
2024-10-22 22:31:13.251083: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159ab00 of size 256 next 1454
2024-10-22 22:31:13.251098: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159ac00 of size 512 next 1645
2024-10-22 22:31:13.251113: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159ae00 of size 256 next 46
2024-10-22 22:31:13.251128: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159af00 of size 256 next 78
2024-10-22 22:31:13.251143: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159b000 of size 256 next 1591
2024-10-22 22:31:13.251159: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159b100 of size 256 next 1661
2024-10-22 22:31:13.251173: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f1dd159b200 of size 256 next 1639
2024-10-22 22:31:13.251187: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159b300 of size 256 next 1470
2024-10-22 22:31:13.251202: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159b400 of size 256 next 1620
2024-10-22 22:31:13.251217: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159b500 of size 256 next 1488
2024-10-22 22:31:13.251231: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159b600 of size 256 next 1567
2024-10-22 22:31:13.251245: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159b700 of size 256 next 1441
2024-10-22 22:31:13.251260: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159b800 of size 256 next 1625
2024-10-22 22:31:13.251275: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159b900 of size 256 next 1599
2024-10-22 22:31:13.251289: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159ba00 of size 256 next 1619
2024-10-22 22:31:13.251303: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159bb00 of size 256 next 1535
2024-10-22 22:31:13.251323: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159bc00 of size 256 next 1484
2024-10-22 22:31:13.251338: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159bd00 of size 256 next 1550
2024-10-22 22:31:13.251352: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159be00 of size 256 next 1617
2024-10-22 22:31:13.251367: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159bf00 of size 256 next 48
2024-10-22 22:31:13.251382: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159c000 of size 256 next 96
2024-10-22 22:31:13.251396: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159c100 of size 256 next 151
2024-10-22 22:31:13.251411: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159c200 of size 256 next 1451
2024-10-22 22:31:13.251426: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159c300 of size 256 next 1687
2024-10-22 22:31:13.251450: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f1dd159c400 of size 256 next 1688
2024-10-22 22:31:13.251467: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159c500 of size 768 next 1671
2024-10-22 22:31:13.251481: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f1dd159c800 of size 512 next 121
2024-10-22 22:31:13.251495: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159ca00 of size 256 next 77
2024-10-22 22:31:13.251510: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159cb00 of size 256 next 1594
2024-10-22 22:31:13.251527: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159cc00 of size 256 next 1554
2024-10-22 22:31:13.251542: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159cd00 of size 256 next 1265
2024-10-22 22:31:13.251556: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159ce00 of size 256 next 1595
2024-10-22 22:31:13.251571: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159cf00 of size 256 next 1623
2024-10-22 22:31:13.251593: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159d000 of size 256 next 1610
2024-10-22 22:31:13.251610: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159d100 of size 256 next 1519
2024-10-22 22:31:13.251624: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159d200 of size 256 next 1609
2024-10-22 22:31:13.251638: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159d300 of size 256 next 1633
2024-10-22 22:31:13.251653: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159d400 of size 256 next 1544
2024-10-22 22:31:13.251667: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159d500 of size 256 next 1505
2024-10-22 22:31:13.251681: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159d600 of size 256 next 1506
2024-10-22 22:31:13.251696: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159d700 of size 256 next 1638
2024-10-22 22:31:13.251710: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159d800 of size 256 next 1618
2024-10-22 22:31:13.251725: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159d900 of size 256 next 106
2024-10-22 22:31:13.251745: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159da00 of size 256 next 211
2024-10-22 22:31:13.251760: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159db00 of size 256 next 244
2024-10-22 22:31:13.251774: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159dc00 of size 256 next 270
2024-10-22 22:31:13.251788: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159dd00 of size 256 next 159
2024-10-22 22:31:13.251802: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159de00 of size 256 next 177
2024-10-22 22:31:13.251817: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159df00 of size 256 next 102
2024-10-22 22:31:13.251831: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159e000 of size 256 next 189
2024-10-22 22:31:13.251845: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159e100 of size 256 next 125
2024-10-22 22:31:13.251860: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159e200 of size 256 next 195
2024-10-22 22:31:13.251874: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159e300 of size 256 next 215
2024-10-22 22:31:13.251889: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159e400 of size 256 next 293
2024-10-22 22:31:13.251903: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159e500 of size 256 next 224
2024-10-22 22:31:13.251917: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159e600 of size 256 next 225
2024-10-22 22:31:13.251941: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159e700 of size 256 next 137
2024-10-22 22:31:13.251957: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159e800 of size 256 next 249
2024-10-22 22:31:13.251970: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159e900 of size 256 next 205
2024-10-22 22:31:13.251986: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159ea00 of size 256 next 226
2024-10-22 22:31:13.252000: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159eb00 of size 256 next 149
2024-10-22 22:31:13.252014: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159ec00 of size 256 next 258
2024-10-22 22:31:13.252029: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159ed00 of size 256 next 209
2024-10-22 22:31:13.252043: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159ee00 of size 256 next 280
2024-10-22 22:31:13.252058: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159ef00 of size 256 next 242
2024-10-22 22:31:13.252074: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159f000 of size 256 next 172
2024-10-22 22:31:13.252088: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159f100 of size 256 next 235
2024-10-22 22:31:13.252103: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159f200 of size 256 next 208
2024-10-22 22:31:13.252117: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159f300 of size 256 next 175
2024-10-22 22:31:13.252131: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159f400 of size 256 next 188
2024-10-22 22:31:13.252146: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159f500 of size 256 next 243
2024-10-22 22:31:13.252172: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159f600 of size 256 next 169
2024-10-22 22:31:13.252182: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159f700 of size 256 next 203
2024-10-22 22:31:13.252193: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159f800 of size 256 next 711
2024-10-22 22:31:13.252203: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159f900 of size 256 next 673
2024-10-22 22:31:13.252214: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159fa00 of size 256 next 1012
2024-10-22 22:31:13.252225: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd159fb00 of size 256 next 1070
2024-10-22 22:31:13.252235: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f1dd159fc00 of size 158464 next 1407
2024-10-22 22:31:13.252245: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c6700 of size 256 next 1476
2024-10-22 22:31:13.252256: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c6800 of size 256 next 1580
2024-10-22 22:31:13.252266: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c6900 of size 256 next 1616
2024-10-22 22:31:13.252276: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c6a00 of size 256 next 1492
2024-10-22 22:31:13.252287: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c6b00 of size 256 next 1537
2024-10-22 22:31:13.252298: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c6c00 of size 256 next 1545
2024-10-22 22:31:13.252308: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f1dd15c6d00 of size 256 next 1560
2024-10-22 22:31:13.252318: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c6e00 of size 256 next 1450
2024-10-22 22:31:13.252330: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c6f00 of size 256 next 1568
2024-10-22 22:31:13.252347: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c7000 of size 256 next 1546
2024-10-22 22:31:13.252358: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c7100 of size 256 next 1518
2024-10-22 22:31:13.252368: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c7200 of size 256 next 1655
2024-10-22 22:31:13.252378: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c7300 of size 256 next 1577
2024-10-22 22:31:13.252389: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c7400 of size 256 next 1650
2024-10-22 22:31:13.252400: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c7500 of size 256 next 1637
2024-10-22 22:31:13.252410: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c7600 of size 256 next 1674
2024-10-22 22:31:13.252420: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f1dd15c7700 of size 256 next 1533
2024-10-22 22:31:13.252431: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c7800 of size 256 next 1629
2024-10-22 22:31:13.252442: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c7900 of size 256 next 1669
2024-10-22 22:31:13.252452: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c7a00 of size 256 next 1600
2024-10-22 22:31:13.252462: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c7b00 of size 256 next 1551
2024-10-22 22:31:13.252473: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c7c00 of size 256 next 1703
2024-10-22 22:31:13.252483: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c7d00 of size 256 next 1573
2024-10-22 22:31:13.252494: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c7e00 of size 256 next 1104
2024-10-22 22:31:13.252504: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c7f00 of size 256 next 1083
2024-10-22 22:31:13.252515: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c8000 of size 256 next 823
2024-10-22 22:31:13.252526: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c8100 of size 256 next 3016
2024-10-22 22:31:13.252536: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c8200 of size 256 next 3001
2024-10-22 22:31:13.252547: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c8300 of size 256 next 2974
2024-10-22 22:31:13.252559: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f1dd15c8400 of size 2048 next 2874
2024-10-22 22:31:13.252569: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15c8c00 of size 256 next 3028
2024-10-22 22:31:13.252579: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f1dd15c8d00 of size 9472 next 2924
2024-10-22 22:31:13.252589: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15cb200 of size 256 next 2955
2024-10-22 22:31:13.252600: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15cb300 of size 256 next 3019
2024-10-22 22:31:13.252611: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f1dd15cb400 of size 256 next 2985
2024-10-22 22:31:13.252621: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15cb500 of size 256 next 2994
2024-10-22 22:31:13.252631: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15cb600 of size 256 next 3013
2024-10-22 22:31:13.252642: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15cb700 of size 256 next 2960
2024-10-22 22:31:13.252652: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f1dd15cb800 of size 74240 next 2968
2024-10-22 22:31:13.252662: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15dda00 of size 256 next 2950
2024-10-22 22:31:13.252673: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ddb00 of size 256 next 2976
2024-10-22 22:31:13.252693: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ddc00 of size 256 next 2978
2024-10-22 22:31:13.252705: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ddd00 of size 256 next 2973
2024-10-22 22:31:13.252715: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15dde00 of size 256 next 2957
2024-10-22 22:31:13.252725: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ddf00 of size 256 next 2959
2024-10-22 22:31:13.252735: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15de000 of size 256 next 2909
2024-10-22 22:31:13.252746: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f1dd15de100 of size 32768 next 2257
2024-10-22 22:31:13.252756: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e6100 of size 256 next 2266
2024-10-22 22:31:13.252766: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e6200 of size 256 next 2282
2024-10-22 22:31:13.252776: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e6300 of size 256 next 2274
2024-10-22 22:31:13.252787: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e6400 of size 256 next 2214
2024-10-22 22:31:13.252797: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e6500 of size 256 next 2273
2024-10-22 22:31:13.252807: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e6600 of size 256 next 2285
2024-10-22 22:31:13.252818: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e6700 of size 256 next 2243
2024-10-22 22:31:13.252828: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e6800 of size 256 next 2238
2024-10-22 22:31:13.252838: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e6900 of size 256 next 2216
2024-10-22 22:31:13.252848: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e6a00 of size 256 next 2231
2024-10-22 22:31:13.252859: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e6b00 of size 256 next 2234
2024-10-22 22:31:13.252869: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e6c00 of size 256 next 2309
2024-10-22 22:31:13.252879: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e6d00 of size 256 next 2263
2024-10-22 22:31:13.252889: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e6e00 of size 256 next 2268
2024-10-22 22:31:13.252899: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e6f00 of size 256 next 2269
2024-10-22 22:31:13.252910: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e7000 of size 256 next 2251
2024-10-22 22:31:13.252920: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e7100 of size 256 next 2275
2024-10-22 22:31:13.252930: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e7200 of size 256 next 2202
2024-10-22 22:31:13.252941: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e7300 of size 256 next 2260
2024-10-22 22:31:13.252951: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e7400 of size 256 next 2205
2024-10-22 22:31:13.252961: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e7500 of size 256 next 2288
2024-10-22 22:31:13.252971: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e7600 of size 256 next 2294
2024-10-22 22:31:13.252981: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e7700 of size 256 next 2289
2024-10-22 22:31:13.252992: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e7800 of size 256 next 2264
2024-10-22 22:31:13.253002: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e7900 of size 256 next 2226
2024-10-22 22:31:13.253017: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e7a00 of size 256 next 2297
2024-10-22 22:31:13.253028: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e7b00 of size 256 next 2290
2024-10-22 22:31:13.253038: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e7c00 of size 256 next 2057
2024-10-22 22:31:13.253048: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e7d00 of size 256 next 2298
2024-10-22 22:31:13.253058: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e7e00 of size 256 next 2301
2024-10-22 22:31:13.253068: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e7f00 of size 256 next 2302
2024-10-22 22:31:13.253079: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e8000 of size 256 next 2276
2024-10-22 22:31:13.253089: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e8100 of size 256 next 2278
2024-10-22 22:31:13.253099: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e8200 of size 256 next 2307
2024-10-22 22:31:13.253110: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e8300 of size 256 next 2352
2024-10-22 22:31:13.253121: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e8400 of size 256 next 1048
2024-10-22 22:31:13.253131: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e8500 of size 256 next 963
2024-10-22 22:31:13.253141: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e8600 of size 256 next 1073
2024-10-22 22:31:13.253152: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e8700 of size 256 next 1090
2024-10-22 22:31:13.253162: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e8800 of size 256 next 1142
2024-10-22 22:31:13.253172: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e8900 of size 256 next 966
2024-10-22 22:31:13.253182: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e8a00 of size 256 next 1184
2024-10-22 22:31:13.253193: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e8b00 of size 256 next 1181
2024-10-22 22:31:13.253203: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e8c00 of size 256 next 1180
2024-10-22 22:31:13.253214: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e8d00 of size 256 next 1182
2024-10-22 22:31:13.253225: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e8e00 of size 256 next 1162
2024-10-22 22:31:13.253235: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e8f00 of size 256 next 1198
2024-10-22 22:31:13.253246: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e9000 of size 256 next 1017
2024-10-22 22:31:13.253256: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e9100 of size 256 next 1151
2024-10-22 22:31:13.253266: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e9200 of size 256 next 1036
2024-10-22 22:31:13.253276: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e9300 of size 256 next 1132
2024-10-22 22:31:13.253287: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e9400 of size 256 next 1094
2024-10-22 22:31:13.253297: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e9500 of size 256 next 1013
2024-10-22 22:31:13.253307: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e9600 of size 256 next 1208
2024-10-22 22:31:13.253317: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e9700 of size 256 next 857
2024-10-22 22:31:13.253328: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e9800 of size 256 next 950
2024-10-22 22:31:13.253338: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e9900 of size 256 next 1047
2024-10-22 22:31:13.253354: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e9a00 of size 256 next 1167
2024-10-22 22:31:13.253364: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e9b00 of size 256 next 1089
2024-10-22 22:31:13.253375: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e9c00 of size 256 next 1081
2024-10-22 22:31:13.253385: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e9d00 of size 256 next 985
2024-10-22 22:31:13.253395: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e9e00 of size 256 next 1116
2024-10-22 22:31:13.253405: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15e9f00 of size 256 next 1196
2024-10-22 22:31:13.253416: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ea000 of size 256 next 1088
2024-10-22 22:31:13.253426: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ea100 of size 256 next 1166
2024-10-22 22:31:13.253436: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ea200 of size 256 next 1146
2024-10-22 22:31:13.253447: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ea300 of size 256 next 1239
2024-10-22 22:31:13.253458: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ea400 of size 256 next 1113
2024-10-22 22:31:13.253468: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ea500 of size 256 next 1233
2024-10-22 22:31:13.253478: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ea600 of size 256 next 1227
2024-10-22 22:31:13.253489: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ea700 of size 256 next 1195
2024-10-22 22:31:13.253499: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ea800 of size 256 next 895
2024-10-22 22:31:13.253510: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ea900 of size 256 next 1058
2024-10-22 22:31:13.253520: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15eaa00 of size 256 next 1164
2024-10-22 22:31:13.253530: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15eab00 of size 256 next 1216
2024-10-22 22:31:13.253541: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15eac00 of size 256 next 1150
2024-10-22 22:31:13.253551: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ead00 of size 256 next 1277
2024-10-22 22:31:13.253561: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15eae00 of size 256 next 1246
2024-10-22 22:31:13.253571: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15eaf00 of size 256 next 1237
2024-10-22 22:31:13.253582: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15eb000 of size 256 next 1241
2024-10-22 22:31:13.253592: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15eb100 of size 256 next 664
2024-10-22 22:31:13.253602: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15eb200 of size 256 next 824
2024-10-22 22:31:13.253613: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15eb300 of size 256 next 1234
2024-10-22 22:31:13.253623: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15eb400 of size 256 next 1221
2024-10-22 22:31:13.253633: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15eb500 of size 256 next 1185
2024-10-22 22:31:13.253644: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15eb600 of size 256 next 1252
2024-10-22 22:31:13.253654: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15eb700 of size 256 next 1225
2024-10-22 22:31:13.253664: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15eb800 of size 256 next 999
2024-10-22 22:31:13.253681: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15eb900 of size 256 next 1210
2024-10-22 22:31:13.253691: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15eba00 of size 256 next 1229
2024-10-22 22:31:13.253701: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ebb00 of size 256 next 1165
2024-10-22 22:31:13.253712: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ebc00 of size 256 next 1157
2024-10-22 22:31:13.253722: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ebd00 of size 256 next 1171
2024-10-22 22:31:13.253732: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ebe00 of size 256 next 1206
2024-10-22 22:31:13.253742: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ebf00 of size 256 next 1016
2024-10-22 22:31:13.253753: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ec000 of size 256 next 1144
2024-10-22 22:31:13.253763: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ec100 of size 256 next 1235
2024-10-22 22:31:13.253773: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ec200 of size 256 next 1273
2024-10-22 22:31:13.253784: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ec300 of size 256 next 1266
2024-10-22 22:31:13.253794: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ec400 of size 256 next 1268
2024-10-22 22:31:13.253804: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ec500 of size 256 next 1224
2024-10-22 22:31:13.253814: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ec600 of size 256 next 1255
2024-10-22 22:31:13.253825: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ec700 of size 256 next 957
2024-10-22 22:31:13.253835: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ec800 of size 256 next 1141
2024-10-22 22:31:13.253846: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ec900 of size 256 next 1271
2024-10-22 22:31:13.253856: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15eca00 of size 256 next 1194
2024-10-22 22:31:13.253866: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ecb00 of size 256 next 1238
2024-10-22 22:31:13.253877: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ecc00 of size 256 next 1050
2024-10-22 22:31:13.253887: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ecd00 of size 256 next 1188
2024-10-22 22:31:13.253897: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ece00 of size 256 next 943
2024-10-22 22:31:13.253908: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ecf00 of size 256 next 1209
2024-10-22 22:31:13.253918: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ed000 of size 256 next 1218
2024-10-22 22:31:13.253929: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ed100 of size 256 next 1228
2024-10-22 22:31:13.253939: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ed200 of size 256 next 1259
2024-10-22 22:31:13.253950: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ed300 of size 256 next 1261
2024-10-22 22:31:13.253960: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ed400 of size 256 next 1284
2024-10-22 22:31:13.253970: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ed500 of size 256 next 1315
2024-10-22 22:31:13.253980: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ed600 of size 256 next 1110
2024-10-22 22:31:13.253991: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ed700 of size 256 next 1274
2024-10-22 22:31:13.254001: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ed800 of size 256 next 1034
2024-10-22 22:31:13.254016: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ed900 of size 256 next 1187
2024-10-22 22:31:13.254026: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15eda00 of size 256 next 1190
2024-10-22 22:31:13.254037: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15edb00 of size 256 next 1173
2024-10-22 22:31:13.254047: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15edc00 of size 256 next 1282
2024-10-22 22:31:13.254057: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15edd00 of size 256 next 1330
2024-10-22 22:31:13.254069: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ede00 of size 256 next 1120
2024-10-22 22:31:13.254079: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15edf00 of size 256 next 1326
2024-10-22 22:31:13.254090: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ee000 of size 256 next 1215
2024-10-22 22:31:13.254100: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ee100 of size 256 next 1038
2024-10-22 22:31:13.254110: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ee200 of size 256 next 1069
2024-10-22 22:31:13.254120: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ee300 of size 256 next 1285
2024-10-22 22:31:13.254131: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ee400 of size 256 next 1286
2024-10-22 22:31:13.254141: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ee500 of size 256 next 1267
2024-10-22 22:31:13.254151: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ee600 of size 256 next 1341
2024-10-22 22:31:13.254162: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ee700 of size 256 next 1323
2024-10-22 22:31:13.254172: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ee800 of size 256 next 1350
2024-10-22 22:31:13.254182: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ee900 of size 256 next 1333
2024-10-22 22:31:13.254192: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15eea00 of size 256 next 1105
2024-10-22 22:31:13.254204: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15eeb00 of size 256 next 1122
2024-10-22 22:31:13.254214: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15eec00 of size 256 next 1220
2024-10-22 22:31:13.254225: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15eed00 of size 256 next 1097
2024-10-22 22:31:13.254235: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15eee00 of size 256 next 1103
2024-10-22 22:31:13.254246: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15eef00 of size 256 next 996
2024-10-22 22:31:13.254256: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ef000 of size 256 next 1201
2024-10-22 22:31:13.254266: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ef100 of size 256 next 1028
2024-10-22 22:31:13.254276: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ef200 of size 256 next 972
2024-10-22 22:31:13.254287: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ef300 of size 256 next 1156
2024-10-22 22:31:13.254297: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ef400 of size 256 next 1179
2024-10-22 22:31:13.254308: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ef500 of size 256 next 1145
2024-10-22 22:31:13.254318: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ef600 of size 256 next 1211
2024-10-22 22:31:13.254328: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ef700 of size 256 next 1256
2024-10-22 22:31:13.254349: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ef800 of size 256 next 1155
2024-10-22 22:31:13.254359: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ef900 of size 256 next 1279
2024-10-22 22:31:13.254370: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15efa00 of size 256 next 1170
2024-10-22 22:31:13.254380: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15efb00 of size 256 next 1037
2024-10-22 22:31:13.254391: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15efc00 of size 256 next 1245
2024-10-22 22:31:13.254401: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15efd00 of size 256 next 1293
2024-10-22 22:31:13.254411: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15efe00 of size 256 next 1289
2024-10-22 22:31:13.254422: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15eff00 of size 256 next 1186
2024-10-22 22:31:13.254432: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f0000 of size 256 next 1281
2024-10-22 22:31:13.254442: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f0100 of size 256 next 1163
2024-10-22 22:31:13.254452: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f0200 of size 256 next 1149
2024-10-22 22:31:13.254463: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f0300 of size 256 next 1292
2024-10-22 22:31:13.254473: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f0400 of size 256 next 1161
2024-10-22 22:31:13.254484: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f0500 of size 256 next 1230
2024-10-22 22:31:13.254494: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f0600 of size 256 next 1232
2024-10-22 22:31:13.254504: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f0700 of size 256 next 484
2024-10-22 22:31:13.254515: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f0800 of size 256 next 1178
2024-10-22 22:31:13.254525: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f0900 of size 256 next 1172
2024-10-22 22:31:13.254535: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f0a00 of size 256 next 1126
2024-10-22 22:31:13.254545: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f0b00 of size 256 next 1168
2024-10-22 22:31:13.254556: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f0c00 of size 256 next 1177
2024-10-22 22:31:13.254566: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f0d00 of size 256 next 1147
2024-10-22 22:31:13.254576: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f0e00 of size 256 next 920
2024-10-22 22:31:13.254587: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f0f00 of size 256 next 1136
2024-10-22 22:31:13.254597: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f1000 of size 256 next 1095
2024-10-22 22:31:13.254607: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f1100 of size 256 next 1222
2024-10-22 22:31:13.254617: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f1200 of size 256 next 1370
2024-10-22 22:31:13.254628: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f1300 of size 256 next 1361
2024-10-22 22:31:13.254638: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f1400 of size 256 next 1309
2024-10-22 22:31:13.254648: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f1500 of size 256 next 1332
2024-10-22 22:31:13.254658: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f1600 of size 256 next 1356
2024-10-22 22:31:13.254669: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f1700 of size 256 next 1106
2024-10-22 22:31:13.254689: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f1800 of size 256 next 1202
2024-10-22 22:31:13.254700: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f1900 of size 256 next 1035
2024-10-22 22:31:13.254710: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f1a00 of size 256 next 1189
2024-10-22 22:31:13.254720: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f1b00 of size 256 next 1200
2024-10-22 22:31:13.254731: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f1c00 of size 256 next 945
2024-10-22 22:31:13.254741: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f1d00 of size 256 next 1212
2024-10-22 22:31:13.254751: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f1e00 of size 256 next 1213
2024-10-22 22:31:13.254761: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f1f00 of size 256 next 1223
2024-10-22 22:31:13.254772: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f2000 of size 256 next 1368
2024-10-22 22:31:13.254782: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f2100 of size 256 next 1287
2024-10-22 22:31:13.254792: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f2200 of size 256 next 1226
2024-10-22 22:31:13.254803: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f2300 of size 256 next 1175
2024-10-22 22:31:13.254813: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f2400 of size 256 next 1154
2024-10-22 22:31:13.254823: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f2500 of size 256 next 1160
2024-10-22 22:31:13.254833: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f2600 of size 256 next 897
2024-10-22 22:31:13.254844: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f2700 of size 256 next 1253
2024-10-22 22:31:13.254855: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f2800 of size 256 next 1203
2024-10-22 22:31:13.254865: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f2900 of size 256 next 1270
2024-10-22 22:31:13.254875: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f2a00 of size 256 next 1283
2024-10-22 22:31:13.254886: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f2b00 of size 256 next 1338
2024-10-22 22:31:13.254896: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f2c00 of size 256 next 1021
2024-10-22 22:31:13.254906: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f2d00 of size 256 next 1121
2024-10-22 22:31:13.254916: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f2e00 of size 256 next 1191
2024-10-22 22:31:13.254927: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f2f00 of size 256 next 1008
2024-10-22 22:31:13.254937: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f3000 of size 256 next 1159
2024-10-22 22:31:13.254947: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f3100 of size 256 next 1127
2024-10-22 22:31:13.254958: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f3200 of size 256 next 1001
2024-10-22 22:31:13.254968: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f3300 of size 256 next 1258
2024-10-22 22:31:13.254978: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f3400 of size 256 next 1304
2024-10-22 22:31:13.254988: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f3500 of size 256 next 1262
2024-10-22 22:31:13.254998: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f3600 of size 256 next 1193
2024-10-22 22:31:13.255017: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f3700 of size 256 next 1324
2024-10-22 22:31:13.255028: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f3800 of size 256 next 1320
2024-10-22 22:31:13.255038: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f3900 of size 256 next 1217
2024-10-22 22:31:13.255048: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f3a00 of size 256 next 1339
2024-10-22 22:31:13.255059: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f3b00 of size 256 next 1143
2024-10-22 22:31:13.255069: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f3c00 of size 256 next 1062
2024-10-22 22:31:13.255079: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f3d00 of size 256 next 1205
2024-10-22 22:31:13.255089: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f3e00 of size 256 next 1288
2024-10-22 22:31:13.255100: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f3f00 of size 256 next 1240
2024-10-22 22:31:13.255110: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f4000 of size 256 next 1192
2024-10-22 22:31:13.255121: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f4100 of size 256 next 975
2024-10-22 22:31:13.255131: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f4200 of size 256 next 1254
2024-10-22 22:31:13.255141: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f4300 of size 256 next 1276
2024-10-22 22:31:13.255152: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f4400 of size 256 next 926
2024-10-22 22:31:13.255162: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f4500 of size 256 next 709
2024-10-22 22:31:13.255171: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f4600 of size 256 next 178
2024-10-22 22:31:13.255182: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f4700 of size 256 next 1000
2024-10-22 22:31:13.255193: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f4800 of size 256 next 940
2024-10-22 22:31:13.255203: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f4900 of size 256 next 929
2024-10-22 22:31:13.255213: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f4a00 of size 256 next 1004
2024-10-22 22:31:13.255224: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f4b00 of size 256 next 1078
2024-10-22 22:31:13.255234: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f4c00 of size 256 next 1066
2024-10-22 22:31:13.255244: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f4d00 of size 256 next 993
2024-10-22 22:31:13.255254: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f4e00 of size 256 next 1060
2024-10-22 22:31:13.255264: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f4f00 of size 256 next 1024
2024-10-22 22:31:13.255275: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f5000 of size 256 next 1064
2024-10-22 22:31:13.255285: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f5100 of size 256 next 1022
2024-10-22 22:31:13.255295: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f5200 of size 256 next 1057
2024-10-22 22:31:13.255306: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f5300 of size 256 next 998
2024-10-22 22:31:13.255316: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f5400 of size 256 next 1025
2024-10-22 22:31:13.255326: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f5500 of size 256 next 1045
2024-10-22 22:31:13.255336: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f5600 of size 256 next 767
2024-10-22 22:31:13.255351: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f5700 of size 256 next 1018
2024-10-22 22:31:13.255362: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f5800 of size 256 next 936
2024-10-22 22:31:13.255372: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f5900 of size 256 next 990
2024-10-22 22:31:13.255383: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f5a00 of size 256 next 1020
2024-10-22 22:31:13.255393: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f5b00 of size 256 next 1027
2024-10-22 22:31:13.255403: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f5c00 of size 256 next 1046
2024-10-22 22:31:13.255414: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f5d00 of size 256 next 1053
2024-10-22 22:31:13.255424: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f5e00 of size 256 next 956
2024-10-22 22:31:13.255434: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f5f00 of size 256 next 1091
2024-10-22 22:31:13.255445: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f6000 of size 256 next 1002
2024-10-22 22:31:13.255455: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f6100 of size 256 next 775
2024-10-22 22:31:13.255465: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f6200 of size 256 next 1023
2024-10-22 22:31:13.255475: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f6300 of size 256 next 885
2024-10-22 22:31:13.255486: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f6400 of size 256 next 935
2024-10-22 22:31:13.255496: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f6500 of size 256 next 992
2024-10-22 22:31:13.255506: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f6600 of size 256 next 1056
2024-10-22 22:31:13.255517: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f6700 of size 256 next 1030
2024-10-22 22:31:13.255527: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f6800 of size 256 next 932
2024-10-22 22:31:13.255537: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f6900 of size 256 next 1061
2024-10-22 22:31:13.255547: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f6a00 of size 256 next 962
2024-10-22 22:31:13.255558: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f6b00 of size 256 next 1009
2024-10-22 22:31:13.255568: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f6c00 of size 256 next 958
2024-10-22 22:31:13.255579: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f6d00 of size 256 next 1076
2024-10-22 22:31:13.255593: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f6e00 of size 256 next 942
2024-10-22 22:31:13.255605: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f6f00 of size 256 next 1072
2024-10-22 22:31:13.255615: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f7000 of size 256 next 793
2024-10-22 22:31:13.255625: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f7100 of size 256 next 903
2024-10-22 22:31:13.255636: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f7200 of size 256 next 1123
2024-10-22 22:31:13.255646: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f7300 of size 256 next 954
2024-10-22 22:31:13.255657: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f7400 of size 256 next 912
2024-10-22 22:31:13.255667: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f7500 of size 256 next 987
2024-10-22 22:31:13.255677: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f7600 of size 256 next 1077
2024-10-22 22:31:13.255693: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f7700 of size 256 next 1100
2024-10-22 22:31:13.255705: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f7800 of size 256 next 1128
2024-10-22 22:31:13.255715: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f7900 of size 256 next 887
2024-10-22 22:31:13.255726: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f7a00 of size 256 next 1093
2024-10-22 22:31:13.255737: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f7b00 of size 256 next 1075
2024-10-22 22:31:13.255747: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f7c00 of size 256 next 1043
2024-10-22 22:31:13.255757: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f7d00 of size 256 next 949
2024-10-22 22:31:13.255767: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f7e00 of size 256 next 1003
2024-10-22 22:31:13.255778: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f7f00 of size 256 next 1080
2024-10-22 22:31:13.255789: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f8000 of size 256 next 1067
2024-10-22 22:31:13.255799: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f8100 of size 256 next 1068
2024-10-22 22:31:13.255810: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f8200 of size 256 next 411
2024-10-22 22:31:13.255820: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f8300 of size 256 next 319
2024-10-22 22:31:13.255830: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f8400 of size 256 next 408
2024-10-22 22:31:13.255840: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f8500 of size 256 next 2228
2024-10-22 22:31:13.255851: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f8600 of size 256 next 2147
2024-10-22 22:31:13.255861: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f8700 of size 256 next 2197
2024-10-22 22:31:13.255872: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f8800 of size 256 next 2191
2024-10-22 22:31:13.255882: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f8900 of size 256 next 2240
2024-10-22 22:31:13.255893: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f8a00 of size 256 next 2123
2024-10-22 22:31:13.255904: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f8b00 of size 256 next 2184
2024-10-22 22:31:13.255914: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f8c00 of size 256 next 2312
2024-10-22 22:31:13.255924: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f8d00 of size 256 next 2284
2024-10-22 22:31:13.255935: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f8e00 of size 256 next 2247
2024-10-22 22:31:13.255945: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f8f00 of size 256 next 2261
2024-10-22 22:31:13.255956: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f9000 of size 256 next 2271
2024-10-22 22:31:13.255967: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f9100 of size 256 next 2256
2024-10-22 22:31:13.255977: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f9200 of size 256 next 2255
2024-10-22 22:31:13.255988: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f9300 of size 256 next 2223
2024-10-22 22:31:13.255998: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f9400 of size 256 next 2279
2024-10-22 22:31:13.256008: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f9500 of size 256 next 2283
2024-10-22 22:31:13.256024: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f9600 of size 256 next 2173
2024-10-22 22:31:13.256035: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f9700 of size 256 next 2249
2024-10-22 22:31:13.256045: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f9800 of size 256 next 2210
2024-10-22 22:31:13.256056: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f9900 of size 256 next 2416
2024-10-22 22:31:13.256066: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f9a00 of size 256 next 2414
2024-10-22 22:31:13.256077: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f9b00 of size 256 next 2250
2024-10-22 22:31:13.256087: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f9c00 of size 256 next 2401
2024-10-22 22:31:13.256098: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f9d00 of size 256 next 2424
2024-10-22 22:31:13.256108: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f9e00 of size 256 next 2311
2024-10-22 22:31:13.256119: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15f9f00 of size 256 next 2403
2024-10-22 22:31:13.256129: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fa000 of size 256 next 2254
2024-10-22 22:31:13.256139: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fa100 of size 256 next 2461
2024-10-22 22:31:13.256150: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fa200 of size 256 next 2473
2024-10-22 22:31:13.256160: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fa300 of size 256 next 2477
2024-10-22 22:31:13.256171: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fa400 of size 256 next 2427
2024-10-22 22:31:13.256181: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fa500 of size 256 next 2465
2024-10-22 22:31:13.256192: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fa600 of size 256 next 2702
2024-10-22 22:31:13.256202: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fa700 of size 256 next 2640
2024-10-22 22:31:13.256212: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fa800 of size 256 next 2684
2024-10-22 22:31:13.256223: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fa900 of size 256 next 2692
2024-10-22 22:31:13.256233: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15faa00 of size 256 next 2693
2024-10-22 22:31:13.256244: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fab00 of size 256 next 2680
2024-10-22 22:31:13.256254: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fac00 of size 256 next 2633
2024-10-22 22:31:13.256264: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fad00 of size 256 next 2643
2024-10-22 22:31:13.256275: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fae00 of size 256 next 2655
2024-10-22 22:31:13.256285: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15faf00 of size 256 next 2815
2024-10-22 22:31:13.256296: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fb000 of size 256 next 2628
2024-10-22 22:31:13.256306: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fb100 of size 256 next 2707
2024-10-22 22:31:13.256316: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fb200 of size 256 next 2677
2024-10-22 22:31:13.256327: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fb300 of size 256 next 2546
2024-10-22 22:31:13.256337: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fb400 of size 256 next 2700
2024-10-22 22:31:13.256348: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fb500 of size 256 next 2703
2024-10-22 22:31:13.256363: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fb600 of size 256 next 2694
2024-10-22 22:31:13.256373: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fb700 of size 256 next 2717
2024-10-22 22:31:13.256384: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fb800 of size 256 next 2730
2024-10-22 22:31:13.256394: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fb900 of size 256 next 2728
2024-10-22 22:31:13.256405: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fba00 of size 256 next 2657
2024-10-22 22:31:13.256415: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fbb00 of size 256 next 2741
2024-10-22 22:31:13.256426: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fbc00 of size 256 next 2714
2024-10-22 22:31:13.256436: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fbd00 of size 256 next 2647
2024-10-22 22:31:13.256446: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fbe00 of size 256 next 2652
2024-10-22 22:31:13.256457: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fbf00 of size 256 next 2711
2024-10-22 22:31:13.256467: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fc000 of size 256 next 2735
2024-10-22 22:31:13.256477: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fc100 of size 256 next 2681
2024-10-22 22:31:13.256488: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fc200 of size 256 next 2731
2024-10-22 22:31:13.256499: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fc300 of size 256 next 2727
2024-10-22 22:31:13.256509: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fc400 of size 256 next 2683
2024-10-22 22:31:13.256519: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fc500 of size 256 next 2724
2024-10-22 22:31:13.256530: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fc600 of size 256 next 2708
2024-10-22 22:31:13.256540: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fc700 of size 256 next 2645
2024-10-22 22:31:13.256551: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fc800 of size 256 next 2695
2024-10-22 22:31:13.256561: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fc900 of size 256 next 2624
2024-10-22 22:31:13.256571: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fca00 of size 256 next 2623
2024-10-22 22:31:13.256582: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fcb00 of size 256 next 2635
2024-10-22 22:31:13.256593: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fcc00 of size 256 next 2675
2024-10-22 22:31:13.256603: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fcd00 of size 256 next 2665
2024-10-22 22:31:13.256613: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fce00 of size 256 next 2729
2024-10-22 22:31:13.256624: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fcf00 of size 256 next 2758
2024-10-22 22:31:13.256634: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fd000 of size 256 next 2632
2024-10-22 22:31:13.256644: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fd100 of size 256 next 2719
2024-10-22 22:31:13.256655: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fd200 of size 256 next 2704
2024-10-22 22:31:13.256665: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fd300 of size 256 next 2697
2024-10-22 22:31:13.256676: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fd400 of size 256 next 2706
2024-10-22 22:31:13.256691: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fd500 of size 256 next 2769
2024-10-22 22:31:13.256702: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fd600 of size 256 next 2689
2024-10-22 22:31:13.256712: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fd700 of size 256 next 2653
2024-10-22 22:31:13.256722: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fd800 of size 256 next 2701
2024-10-22 22:31:13.256732: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fd900 of size 256 next 2778
2024-10-22 22:31:13.256743: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fda00 of size 256 next 2789
2024-10-22 22:31:13.256753: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fdb00 of size 256 next 2774
2024-10-22 22:31:13.256764: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fdc00 of size 256 next 2782
2024-10-22 22:31:13.256774: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fdd00 of size 256 next 2722
2024-10-22 22:31:13.256784: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fde00 of size 256 next 2796
2024-10-22 22:31:13.256794: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fdf00 of size 256 next 2797
2024-10-22 22:31:13.256805: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fe000 of size 256 next 2726
2024-10-22 22:31:13.256814: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fe100 of size 256 next 2752
2024-10-22 22:31:13.256825: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fe200 of size 256 next 2718
2024-10-22 22:31:13.256835: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fe300 of size 256 next 2748
2024-10-22 22:31:13.256845: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fe400 of size 256 next 2733
2024-10-22 22:31:13.256855: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fe500 of size 256 next 2685
2024-10-22 22:31:13.256866: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fe600 of size 256 next 2777
2024-10-22 22:31:13.256876: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fe700 of size 256 next 2740
2024-10-22 22:31:13.256887: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fe800 of size 256 next 2737
2024-10-22 22:31:13.256897: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fe900 of size 256 next 2785
2024-10-22 22:31:13.256907: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fea00 of size 256 next 2767
2024-10-22 22:31:13.256918: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15feb00 of size 256 next 2790
2024-10-22 22:31:13.256928: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fec00 of size 256 next 2840
2024-10-22 22:31:13.256940: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fed00 of size 256 next 2713
2024-10-22 22:31:13.256950: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fee00 of size 256 next 2696
2024-10-22 22:31:13.256961: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fef00 of size 256 next 2667
2024-10-22 22:31:13.256971: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ff000 of size 256 next 2746
2024-10-22 22:31:13.256982: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ff100 of size 256 next 2761
2024-10-22 22:31:13.256992: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ff200 of size 256 next 2793
2024-10-22 22:31:13.257003: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ff300 of size 256 next 2753
2024-10-22 22:31:13.257013: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ff400 of size 256 next 2935
2024-10-22 22:31:13.257034: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ff500 of size 256 next 2943
2024-10-22 22:31:13.257045: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ff600 of size 256 next 2933
2024-10-22 22:31:13.257055: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ff700 of size 256 next 2948
2024-10-22 22:31:13.257066: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ff800 of size 256 next 2913
2024-10-22 22:31:13.257076: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ff900 of size 256 next 2732
2024-10-22 22:31:13.257086: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ffa00 of size 256 next 2734
2024-10-22 22:31:13.257097: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ffb00 of size 256 next 2768
2024-10-22 22:31:13.257107: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ffc00 of size 256 next 2765
2024-10-22 22:31:13.257117: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ffd00 of size 256 next 2678
2024-10-22 22:31:13.257128: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15ffe00 of size 256 next 2721
2024-10-22 22:31:13.257139: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd15fff00 of size 256 next 2781
2024-10-22 22:31:13.257150: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1600000 of size 256 next 2780
2024-10-22 22:31:13.257160: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1600100 of size 256 next 2770
2024-10-22 22:31:13.257170: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1600200 of size 256 next 2776
2024-10-22 22:31:13.257181: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1600300 of size 256 next 2786
2024-10-22 22:31:13.257191: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1600400 of size 256 next 2759
2024-10-22 22:31:13.257202: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1600500 of size 256 next 2773
2024-10-22 22:31:13.257212: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1600600 of size 256 next 2792
2024-10-22 22:31:13.257223: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1600700 of size 256 next 2622
2024-10-22 22:31:13.257233: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1600800 of size 256 next 2805
2024-10-22 22:31:13.257243: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1600900 of size 256 next 2842
2024-10-22 22:31:13.257254: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1600a00 of size 256 next 2801
2024-10-22 22:31:13.257266: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1600b00 of size 256 next 2836
2024-10-22 22:31:13.257277: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1600c00 of size 256 next 2875
2024-10-22 22:31:13.257287: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1600d00 of size 256 next 2766
2024-10-22 22:31:13.257297: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1600e00 of size 256 next 2783
2024-10-22 22:31:13.257308: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1600f00 of size 256 next 2799
2024-10-22 22:31:13.257318: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1601000 of size 256 next 2712
2024-10-22 22:31:13.257329: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1601100 of size 256 next 2760
2024-10-22 22:31:13.257339: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1601200 of size 256 next 2803
2024-10-22 22:31:13.257350: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1601300 of size 256 next 2800
2024-10-22 22:31:13.257367: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1601400 of size 256 next 2747
2024-10-22 22:31:13.257378: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1601500 of size 256 next 2791
2024-10-22 22:31:13.257388: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1601600 of size 256 next 2710
2024-10-22 22:31:13.257399: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1601700 of size 256 next 2772
2024-10-22 22:31:13.257409: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1601800 of size 256 next 2806
2024-10-22 22:31:13.257419: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1601900 of size 256 next 2788
2024-10-22 22:31:13.257430: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1601a00 of size 256 next 2716
2024-10-22 22:31:13.257440: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1601b00 of size 256 next 2725
2024-10-22 22:31:13.257450: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1601c00 of size 256 next 2614
2024-10-22 22:31:13.257460: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1601d00 of size 256 next 2811
2024-10-22 22:31:13.257471: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1601e00 of size 256 next 2818
2024-10-22 22:31:13.257481: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1601f00 of size 256 next 2844
2024-10-22 22:31:13.257492: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1602000 of size 256 next 2810
2024-10-22 22:31:13.257502: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1602100 of size 256 next 2784
2024-10-22 22:31:13.257512: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1602200 of size 256 next 2832
2024-10-22 22:31:13.257523: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1602300 of size 256 next 2755
2024-10-22 22:31:13.257533: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1602400 of size 256 next 2823
2024-10-22 22:31:13.257543: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1602500 of size 256 next 2834
2024-10-22 22:31:13.257554: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1602600 of size 256 next 2816
2024-10-22 22:31:13.257564: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1602700 of size 256 next 2833
2024-10-22 22:31:13.257575: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1602800 of size 256 next 2835
2024-10-22 22:31:13.257585: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1602900 of size 256 next 2802
2024-10-22 22:31:13.257596: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1602a00 of size 256 next 2751
2024-10-22 22:31:13.257606: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1602b00 of size 256 next 2824
2024-10-22 22:31:13.257617: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1602c00 of size 256 next 2764
2024-10-22 22:31:13.257627: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1602d00 of size 256 next 2828
2024-10-22 22:31:13.257637: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1602e00 of size 256 next 2775
2024-10-22 22:31:13.257648: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1602f00 of size 256 next 2757
2024-10-22 22:31:13.257658: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1603000 of size 256 next 2808
2024-10-22 22:31:13.257668: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1603100 of size 256 next 2744
2024-10-22 22:31:13.257679: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1603200 of size 256 next 1761
2024-10-22 22:31:13.257690: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1603300 of size 256 next 1729
2024-10-22 22:31:13.257712: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1603400 of size 256 next 1740
2024-10-22 22:31:13.257723: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1603500 of size 256 next 1760
2024-10-22 22:31:13.257733: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1603600 of size 256 next 1738
2024-10-22 22:31:13.257744: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1603700 of size 256 next 1773
2024-10-22 22:31:13.257754: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1603800 of size 256 next 1711
2024-10-22 22:31:13.257764: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1603900 of size 256 next 1728
2024-10-22 22:31:13.257776: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1603a00 of size 256 next 1762
2024-10-22 22:31:13.257787: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1603b00 of size 256 next 1741
2024-10-22 22:31:13.257797: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1603c00 of size 256 next 1783
2024-10-22 22:31:13.257807: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1603d00 of size 256 next 1755
2024-10-22 22:31:13.257818: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1603e00 of size 256 next 1769
2024-10-22 22:31:13.257828: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1603f00 of size 256 next 1726
2024-10-22 22:31:13.257839: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1604000 of size 256 next 1736
2024-10-22 22:31:13.257849: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1604100 of size 256 next 1757
2024-10-22 22:31:13.257860: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1604200 of size 256 next 1767
2024-10-22 22:31:13.257870: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1604300 of size 256 next 1763
2024-10-22 22:31:13.257881: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1604400 of size 256 next 1704
2024-10-22 22:31:13.257891: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1604500 of size 256 next 1803
2024-10-22 22:31:13.257901: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1604600 of size 256 next 1780
2024-10-22 22:31:13.257912: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1604700 of size 256 next 1592
2024-10-22 22:31:13.257922: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1604800 of size 256 next 1800
2024-10-22 22:31:13.257933: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1604900 of size 256 next 1804
2024-10-22 22:31:13.257943: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1604a00 of size 256 next 1787
2024-10-22 22:31:13.257953: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1604b00 of size 256 next 1792
2024-10-22 22:31:13.257963: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1604c00 of size 256 next 1772
2024-10-22 22:31:13.257974: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1604d00 of size 256 next 1746
2024-10-22 22:31:13.257984: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1604e00 of size 256 next 1777
2024-10-22 22:31:13.257994: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1604f00 of size 256 next 1716
2024-10-22 22:31:13.258005: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1605000 of size 256 next 1754
2024-10-22 22:31:13.258015: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1605100 of size 256 next 1765
2024-10-22 22:31:13.258026: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1605200 of size 256 next 1790
2024-10-22 22:31:13.258045: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1605300 of size 256 next 1702
2024-10-22 22:31:13.258056: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1605400 of size 256 next 1786
2024-10-22 22:31:13.258066: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1605500 of size 256 next 1771
2024-10-22 22:31:13.258077: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1605600 of size 256 next 1778
2024-10-22 22:31:13.258087: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1605700 of size 256 next 1789
2024-10-22 22:31:13.258097: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1605800 of size 256 next 1720
2024-10-22 22:31:13.258107: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1605900 of size 256 next 1715
2024-10-22 22:31:13.258118: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1605a00 of size 256 next 1751
2024-10-22 22:31:13.258128: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1605b00 of size 256 next 1768
2024-10-22 22:31:13.258139: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1605c00 of size 256 next 1543
2024-10-22 22:31:13.258149: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1605d00 of size 256 next 1742
2024-10-22 22:31:13.258160: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1605e00 of size 256 next 1739
2024-10-22 22:31:13.258170: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1605f00 of size 256 next 1813
2024-10-22 22:31:13.258180: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1606000 of size 256 next 1764
2024-10-22 22:31:13.258191: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1606100 of size 256 next 1766
2024-10-22 22:31:13.258201: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1606200 of size 256 next 1311
2024-10-22 22:31:13.258212: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1606300 of size 256 next 1169
2024-10-22 22:31:13.258222: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1606400 of size 256 next 1366
2024-10-22 22:31:13.258233: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1606500 of size 256 next 1348
2024-10-22 22:31:13.258243: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1606600 of size 256 next 1460
2024-10-22 22:31:13.258254: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1606700 of size 256 next 1463
2024-10-22 22:31:13.258264: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1606800 of size 256 next 1465
2024-10-22 22:31:13.258274: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1606900 of size 256 next 907
2024-10-22 22:31:13.258285: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1606a00 of size 256 next 1383
2024-10-22 22:31:13.258296: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1606b00 of size 256 next 1248
2024-10-22 22:31:13.258306: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1606c00 of size 256 next 1243
2024-10-22 22:31:13.258316: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1606d00 of size 256 next 1296
2024-10-22 22:31:13.258326: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1606e00 of size 256 next 849
2024-10-22 22:31:13.258337: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1606f00 of size 256 next 1360
2024-10-22 22:31:13.258347: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1607000 of size 256 next 1362
2024-10-22 22:31:13.258357: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1607100 of size 256 next 1291
2024-10-22 22:31:13.258368: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1607200 of size 256 next 1355
2024-10-22 22:31:13.258385: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1607300 of size 256 next 1384
2024-10-22 22:31:13.258396: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1607400 of size 256 next 1297
2024-10-22 22:31:13.258406: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1607500 of size 256 next 1425
2024-10-22 22:31:13.258417: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1607600 of size 256 next 1414
2024-10-22 22:31:13.258427: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1607700 of size 256 next 1337
2024-10-22 22:31:13.258438: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1607800 of size 256 next 1363
2024-10-22 22:31:13.258448: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1607900 of size 256 next 1294
2024-10-22 22:31:13.258459: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1607a00 of size 256 next 1264
2024-10-22 22:31:13.258469: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1607b00 of size 256 next 1251
2024-10-22 22:31:13.258480: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1607c00 of size 256 next 1300
2024-10-22 22:31:13.258490: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1607d00 of size 256 next 1316
2024-10-22 22:31:13.258500: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1607e00 of size 256 next 1219
2024-10-22 22:31:13.258511: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1607f00 of size 256 next 1398
2024-10-22 22:31:13.258521: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1608000 of size 256 next 1084
2024-10-22 22:31:13.258532: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1608100 of size 256 next 1306
2024-10-22 22:31:13.258543: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1608200 of size 256 next 1364
2024-10-22 22:31:13.258553: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1608300 of size 256 next 1378
2024-10-22 22:31:13.258563: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1608400 of size 256 next 724
2024-10-22 22:31:13.258574: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1608500 of size 256 next 1138
2024-10-22 22:31:13.258584: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1608600 of size 256 next 1139
2024-10-22 22:31:13.258595: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1608700 of size 256 next 1079
2024-10-22 22:31:13.258605: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1608800 of size 256 next 1102
2024-10-22 22:31:13.258615: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1608900 of size 256 next 1152
2024-10-22 22:31:13.258626: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1608a00 of size 256 next 1112
2024-10-22 22:31:13.258636: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1608b00 of size 256 next 1114
2024-10-22 22:31:13.258647: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1608c00 of size 256 next 1521
2024-10-22 22:31:13.258657: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1608d00 of size 256 next 1895
2024-10-22 22:31:13.258668: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1608e00 of size 256 next 1952
2024-10-22 22:31:13.258678: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1608f00 of size 256 next 1897
2024-10-22 22:31:13.258688: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1609000 of size 256 next 1938
2024-10-22 22:31:13.258699: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1609100 of size 256 next 1925
2024-10-22 22:31:13.258716: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1609200 of size 256 next 1916
2024-10-22 22:31:13.258728: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1609300 of size 256 next 1934
2024-10-22 22:31:13.258738: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1609400 of size 256 next 1969
2024-10-22 22:31:13.258748: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1609500 of size 256 next 1880
2024-10-22 22:31:13.258759: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1609600 of size 256 next 1991
2024-10-22 22:31:13.258769: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1609700 of size 256 next 1979
2024-10-22 22:31:13.258779: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1609800 of size 256 next 1909
2024-10-22 22:31:13.258790: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1609900 of size 256 next 1941
2024-10-22 22:31:13.258800: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1609a00 of size 256 next 1974
2024-10-22 22:31:13.258811: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1609b00 of size 256 next 1957
2024-10-22 22:31:13.258821: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1609c00 of size 256 next 1944
2024-10-22 22:31:13.258831: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1609d00 of size 256 next 1950
2024-10-22 22:31:13.258842: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1609e00 of size 256 next 1959
2024-10-22 22:31:13.258852: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1609f00 of size 256 next 1971
2024-10-22 22:31:13.258863: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160a000 of size 256 next 1983
2024-10-22 22:31:13.258873: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160a100 of size 256 next 1956
2024-10-22 22:31:13.258884: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160a200 of size 256 next 1912
2024-10-22 22:31:13.258894: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160a300 of size 256 next 1958
2024-10-22 22:31:13.258904: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160a400 of size 256 next 1967
2024-10-22 22:31:13.258914: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160a500 of size 256 next 1981
2024-10-22 22:31:13.258924: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160a600 of size 256 next 1975
2024-10-22 22:31:13.258935: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160a700 of size 256 next 1923
2024-10-22 22:31:13.258945: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160a800 of size 256 next 1929
2024-10-22 22:31:13.258956: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160a900 of size 256 next 1747
2024-10-22 22:31:13.258966: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160aa00 of size 256 next 1939
2024-10-22 22:31:13.258977: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160ab00 of size 256 next 1993
2024-10-22 22:31:13.258987: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160ac00 of size 256 next 1954
2024-10-22 22:31:13.258997: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160ad00 of size 256 next 1985
2024-10-22 22:31:13.259008: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160ae00 of size 256 next 1997
2024-10-22 22:31:13.259019: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160af00 of size 256 next 1982
2024-10-22 22:31:13.259030: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160b000 of size 256 next 1968
2024-10-22 22:31:13.259040: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160b100 of size 256 next 1970
2024-10-22 22:31:13.259056: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160b200 of size 256 next 1972
2024-10-22 22:31:13.259067: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160b300 of size 256 next 1842
2024-10-22 22:31:13.259077: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160b400 of size 256 next 1990
2024-10-22 22:31:13.259088: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160b500 of size 256 next 1980
2024-10-22 22:31:13.259099: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160b600 of size 256 next 1995
2024-10-22 22:31:13.259109: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160b700 of size 256 next 2004
2024-10-22 22:31:13.259120: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160b800 of size 256 next 1955
2024-10-22 22:31:13.259130: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160b900 of size 256 next 1989
2024-10-22 22:31:13.259141: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160ba00 of size 256 next 1936
2024-10-22 22:31:13.259151: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160bb00 of size 256 next 1919
2024-10-22 22:31:13.259162: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160bc00 of size 256 next 1806
2024-10-22 22:31:13.259172: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160bd00 of size 256 next 2006
2024-10-22 22:31:13.259182: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160be00 of size 256 next 1876
2024-10-22 22:31:13.259193: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160bf00 of size 256 next 1992
2024-10-22 22:31:13.259203: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160c000 of size 256 next 1994
2024-10-22 22:31:13.259214: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160c100 of size 256 next 2003
2024-10-22 22:31:13.259224: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160c200 of size 256 next 2012
2024-10-22 22:31:13.259235: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160c300 of size 256 next 2015
2024-10-22 22:31:13.259245: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160c400 of size 256 next 1951
2024-10-22 22:31:13.259255: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160c500 of size 256 next 2016
2024-10-22 22:31:13.259266: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160c600 of size 256 next 1864
2024-10-22 22:31:13.259276: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160c700 of size 256 next 2010
2024-10-22 22:31:13.259286: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160c800 of size 256 next 1988
2024-10-22 22:31:13.259297: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160c900 of size 256 next 1828
2024-10-22 22:31:13.259307: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160ca00 of size 256 next 1940
2024-10-22 22:31:13.259318: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160cb00 of size 256 next 1966
2024-10-22 22:31:13.259328: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160cc00 of size 256 next 2024
2024-10-22 22:31:13.259338: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160cd00 of size 256 next 2002
2024-10-22 22:31:13.259349: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160ce00 of size 256 next 1904
2024-10-22 22:31:13.259359: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160cf00 of size 256 next 1927
2024-10-22 22:31:13.259370: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160d000 of size 256 next 1933
2024-10-22 22:31:13.259387: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160d100 of size 256 next 1869
2024-10-22 22:31:13.259399: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160d200 of size 256 next 1960
2024-10-22 22:31:13.259409: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160d300 of size 256 next 1893
2024-10-22 22:31:13.259419: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160d400 of size 256 next 1899
2024-10-22 22:31:13.259429: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160d500 of size 256 next 1914
2024-10-22 22:31:13.259440: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160d600 of size 256 next 1962
2024-10-22 22:31:13.259451: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160d700 of size 256 next 1845
2024-10-22 22:31:13.259461: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160d800 of size 256 next 1913
2024-10-22 22:31:13.259471: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160d900 of size 256 next 1964
2024-10-22 22:31:13.259482: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160da00 of size 256 next 2021
2024-10-22 22:31:13.259493: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160db00 of size 256 next 2034
2024-10-22 22:31:13.259503: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160dc00 of size 256 next 1953
2024-10-22 22:31:13.259513: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160dd00 of size 256 next 1965
2024-10-22 22:31:13.259524: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160de00 of size 256 next 2558
2024-10-22 22:31:13.259534: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160df00 of size 256 next 2488
2024-10-22 22:31:13.259544: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160e000 of size 256 next 2569
2024-10-22 22:31:13.259555: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160e100 of size 256 next 2535
2024-10-22 22:31:13.259565: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160e200 of size 256 next 2547
2024-10-22 22:31:13.259576: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160e300 of size 256 next 2568
2024-10-22 22:31:13.259592: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160e400 of size 256 next 2525
2024-10-22 22:31:13.259604: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160e500 of size 256 next 2580
2024-10-22 22:31:13.259614: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160e600 of size 256 next 2540
2024-10-22 22:31:13.259625: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160e700 of size 256 next 2552
2024-10-22 22:31:13.259635: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160e800 of size 256 next 2576
2024-10-22 22:31:13.259645: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160e900 of size 256 next 2691
2024-10-22 22:31:13.259656: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160ea00 of size 256 next 2699
2024-10-22 22:31:13.259666: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160eb00 of size 256 next 2715
2024-10-22 22:31:13.259677: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160ec00 of size 256 next 2518
2024-10-22 22:31:13.259687: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160ed00 of size 256 next 2561
2024-10-22 22:31:13.259698: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160ee00 of size 256 next 2583
2024-10-22 22:31:13.259708: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160ef00 of size 256 next 2574
2024-10-22 22:31:13.259718: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160f000 of size 256 next 2597
2024-10-22 22:31:13.259733: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160f100 of size 256 next 2549
2024-10-22 22:31:13.259744: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160f200 of size 256 next 2523
2024-10-22 22:31:13.259755: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160f300 of size 256 next 2584
2024-10-22 22:31:13.259765: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160f400 of size 256 next 2515
2024-10-22 22:31:13.259775: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160f500 of size 256 next 2588
2024-10-22 22:31:13.259786: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160f600 of size 256 next 2517
2024-10-22 22:31:13.259796: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160f700 of size 256 next 2619
2024-10-22 22:31:13.259807: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160f800 of size 256 next 2586
2024-10-22 22:31:13.259818: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160f900 of size 256 next 2587
2024-10-22 22:31:13.259828: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160fa00 of size 256 next 2564
2024-10-22 22:31:13.259839: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160fb00 of size 256 next 2494
2024-10-22 22:31:13.259849: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160fc00 of size 256 next 2599
2024-10-22 22:31:13.259859: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160fd00 of size 256 next 2604
2024-10-22 22:31:13.259870: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160fe00 of size 256 next 2575
2024-10-22 22:31:13.259880: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd160ff00 of size 256 next 2559
2024-10-22 22:31:13.259891: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1610000 of size 256 next 2591
2024-10-22 22:31:13.259901: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1610100 of size 256 next 2567
2024-10-22 22:31:13.259912: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1610200 of size 256 next 2593
2024-10-22 22:31:13.259922: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1610300 of size 256 next 2503
2024-10-22 22:31:13.259932: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1610400 of size 256 next 2629
2024-10-22 22:31:13.259942: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1610500 of size 256 next 2519
2024-10-22 22:31:13.259953: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1610600 of size 256 next 2579
2024-10-22 22:31:13.259964: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1610700 of size 256 next 2557
2024-10-22 22:31:13.259974: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1610800 of size 256 next 2613
2024-10-22 22:31:13.259984: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1610900 of size 256 next 2634
2024-10-22 22:31:13.259995: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1610a00 of size 256 next 2600
2024-10-22 22:31:13.260005: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1610b00 of size 256 next 2596
2024-10-22 22:31:13.260015: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1610c00 of size 256 next 2602
2024-10-22 22:31:13.260025: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1610d00 of size 256 next 2608
2024-10-22 22:31:13.260036: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1610e00 of size 256 next 2581
2024-10-22 22:31:13.260047: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1610f00 of size 256 next 2603
2024-10-22 22:31:13.260062: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1611000 of size 256 next 2609
2024-10-22 22:31:13.260073: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1611100 of size 256 next 2563
2024-10-22 22:31:13.260084: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1611200 of size 256 next 2534
2024-10-22 22:31:13.260094: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1611300 of size 256 next 2511
2024-10-22 22:31:13.260105: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1611400 of size 256 next 2607
2024-10-22 22:31:13.260115: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1611500 of size 256 next 2595
2024-10-22 22:31:13.260126: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1611600 of size 256 next 2537
2024-10-22 22:31:13.260136: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1611700 of size 256 next 2594
2024-10-22 22:31:13.260146: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1611800 of size 256 next 2570
2024-10-22 22:31:13.260157: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1611900 of size 256 next 2621
2024-10-22 22:31:13.260167: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1611a00 of size 256 next 2615
2024-10-22 22:31:13.260178: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1611b00 of size 256 next 2571
2024-10-22 22:31:13.260188: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1611c00 of size 256 next 2611
2024-10-22 22:31:13.260198: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1611d00 of size 256 next 2520
2024-10-22 22:31:13.260209: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1611e00 of size 256 next 2462
2024-10-22 22:31:13.260220: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1611f00 of size 256 next 2533
2024-10-22 22:31:13.260230: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1612000 of size 256 next 2598
2024-10-22 22:31:13.260240: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1612100 of size 256 next 2626
2024-10-22 22:31:13.260251: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1612200 of size 256 next 2631
2024-10-22 22:31:13.260261: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1612300 of size 256 next 2650
2024-10-22 22:31:13.260272: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1612400 of size 256 next 2656
2024-10-22 22:31:13.260282: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1612500 of size 256 next 2627
2024-10-22 22:31:13.260293: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1612600 of size 256 next 2592
2024-10-22 22:31:13.260304: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1612700 of size 256 next 2620
2024-10-22 22:31:13.260314: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1612800 of size 256 next 2578
2024-10-22 22:31:13.260324: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1612900 of size 256 next 2610
2024-10-22 22:31:13.260335: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1612a00 of size 256 next 2648
2024-10-22 22:31:13.260346: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1612b00 of size 256 next 2646
2024-10-22 22:31:13.260356: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1612c00 of size 256 next 2671
2024-10-22 22:31:13.260366: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1612d00 of size 256 next 2612
2024-10-22 22:31:13.260376: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1612e00 of size 256 next 2639
2024-10-22 22:31:13.260387: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1612f00 of size 256 next 2618
2024-10-22 22:31:13.260402: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1613000 of size 256 next 2658
2024-10-22 22:31:13.260413: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1613100 of size 256 next 2636
2024-10-22 22:31:13.260423: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1613200 of size 256 next 2565
2024-10-22 22:31:13.260434: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1613300 of size 256 next 2642
2024-10-22 22:31:13.260444: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1613400 of size 256 next 2553
2024-10-22 22:31:13.260454: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1613500 of size 256 next 2555
2024-10-22 22:31:13.260465: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1613600 of size 256 next 2637
2024-10-22 22:31:13.260475: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1613700 of size 256 next 2638
2024-10-22 22:31:13.260486: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1613800 of size 256 next 2649
2024-10-22 22:31:13.260496: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1613900 of size 256 next 2630
2024-10-22 22:31:13.260507: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1613a00 of size 256 next 2679
2024-10-22 22:31:13.260517: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1613b00 of size 256 next 2682
2024-10-22 22:31:13.260527: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1613c00 of size 256 next 2666
2024-10-22 22:31:13.260537: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1613d00 of size 256 next 2585
2024-10-22 22:31:13.260548: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1613e00 of size 256 next 2651
2024-10-22 22:31:13.260559: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1613f00 of size 256 next 2605
2024-10-22 22:31:13.260569: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1614000 of size 256 next 2660
2024-10-22 22:31:13.260579: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1614100 of size 256 next 2625
2024-10-22 22:31:13.260590: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1614200 of size 256 next 2573
2024-10-22 22:31:13.260601: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1614300 of size 256 next 2617
2024-10-22 22:31:13.260611: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1614400 of size 256 next 2670
2024-10-22 22:31:13.260621: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1614500 of size 256 next 2673
2024-10-22 22:31:13.260632: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1614600 of size 256 next 2659
2024-10-22 22:31:13.260643: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1614700 of size 256 next 2663
2024-10-22 22:31:13.260653: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1614800 of size 256 next 2686
2024-10-22 22:31:13.260663: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1614900 of size 256 next 2698
2024-10-22 22:31:13.260673: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1614a00 of size 256 next 2524
2024-10-22 22:31:13.260684: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1614b00 of size 256 next 2687
2024-10-22 22:31:13.260694: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1614c00 of size 256 next 2545
2024-10-22 22:31:13.260704: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1614d00 of size 256 next 2664
2024-10-22 22:31:13.260715: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1614e00 of size 256 next 2690
2024-10-22 22:31:13.260734: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1614f00 of size 256 next 2669
2024-10-22 22:31:13.260744: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1615000 of size 256 next 2736
2024-10-22 22:31:13.260754: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1615100 of size 256 next 2738
2024-10-22 22:31:13.260765: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1615200 of size 256 next 2688
2024-10-22 22:31:13.260776: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1615300 of size 256 next 2672
2024-10-22 22:31:13.260786: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1615400 of size 256 next 2644
2024-10-22 22:31:13.260796: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1615500 of size 256 next 2543
2024-10-22 22:31:13.260807: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1615600 of size 256 next 2536
2024-10-22 22:31:13.260818: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1615700 of size 256 next 2676
2024-10-22 22:31:13.260828: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1615800 of size 256 next 2654
2024-10-22 22:31:13.260838: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1615900 of size 256 next 2506
2024-10-22 22:31:13.260849: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1615a00 of size 256 next 2674
2024-10-22 22:31:13.260859: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1615b00 of size 256 next 2590
2024-10-22 22:31:13.260871: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1615c00 of size 256 next 2661
2024-10-22 22:31:13.260881: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1615d00 of size 256 next 2582
2024-10-22 22:31:13.260891: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1615e00 of size 256 next 2662
2024-10-22 22:31:13.260902: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1615f00 of size 256 next 2641
2024-10-22 22:31:13.260913: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1616000 of size 256 next 2538
2024-10-22 22:31:13.260923: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1616100 of size 256 next 2486
2024-10-22 22:31:13.260934: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1616200 of size 256 next 2548
2024-10-22 22:31:13.260945: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1616300 of size 256 next 2495
2024-10-22 22:31:13.260955: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1616400 of size 256 next 2389
2024-10-22 22:31:13.260965: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1616500 of size 256 next 2493
2024-10-22 22:31:13.260975: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1616600 of size 256 next 2542
2024-10-22 22:31:13.260986: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1616700 of size 256 next 2541
2024-10-22 22:31:13.260996: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1616800 of size 256 next 2485
2024-10-22 22:31:13.261006: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1616900 of size 256 next 2470
2024-10-22 22:31:13.261017: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1616a00 of size 256 next 2464
2024-10-22 22:31:13.261027: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1616b00 of size 256 next 2554
2024-10-22 22:31:13.261038: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1616c00 of size 256 next 2512
2024-10-22 22:31:13.261047: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1616d00 of size 256 next 2532
2024-10-22 22:31:13.261058: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1616e00 of size 256 next 2516
2024-10-22 22:31:13.261073: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1616f00 of size 256 next 2429
2024-10-22 22:31:13.261084: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1617000 of size 256 next 2556
2024-10-22 22:31:13.261094: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1617100 of size 256 next 2467
2024-10-22 22:31:13.261104: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1617200 of size 256 next 2514
2024-10-22 22:31:13.261115: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1617300 of size 256 next 2479
2024-10-22 22:31:13.261125: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1617400 of size 256 next 2560
2024-10-22 22:31:13.261135: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1617500 of size 256 next 710
2024-10-22 22:31:13.261146: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1617600 of size 256 next 704
2024-10-22 22:31:13.261156: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1617700 of size 256 next 640
2024-10-22 22:31:13.261167: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1617800 of size 256 next 630
2024-10-22 22:31:13.261177: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1617900 of size 256 next 752
2024-10-22 22:31:13.261188: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1617a00 of size 256 next 615
2024-10-22 22:31:13.261199: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1617b00 of size 256 next 662
2024-10-22 22:31:13.261209: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1617c00 of size 256 next 697
2024-10-22 22:31:13.261220: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1617d00 of size 256 next 524
2024-10-22 22:31:13.261231: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1617e00 of size 256 next 479
2024-10-22 22:31:13.261241: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1617f00 of size 256 next 716
2024-10-22 22:31:13.261252: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1618000 of size 256 next 707
2024-10-22 22:31:13.261262: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1618100 of size 256 next 774
2024-10-22 22:31:13.261273: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1618200 of size 256 next 347
2024-10-22 22:31:13.261283: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1618300 of size 256 next 561
2024-10-22 22:31:13.261294: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1618400 of size 256 next 678
2024-10-22 22:31:13.261304: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1618500 of size 256 next 670
2024-10-22 22:31:13.261314: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1618600 of size 256 next 730
2024-10-22 22:31:13.261325: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1618700 of size 256 next 795
2024-10-22 22:31:13.261335: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1618800 of size 256 next 564
2024-10-22 22:31:13.261345: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1618900 of size 256 next 518
2024-10-22 22:31:13.261356: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1618a00 of size 256 next 503
2024-10-22 22:31:13.261366: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1618b00 of size 256 next 683
2024-10-22 22:31:13.261376: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1618c00 of size 256 next 705
2024-10-22 22:31:13.261387: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1618d00 of size 256 next 654
2024-10-22 22:31:13.261399: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1618e00 of size 256 next 652
2024-10-22 22:31:13.261414: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1618f00 of size 256 next 698
2024-10-22 22:31:13.261425: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1619000 of size 256 next 905
2024-10-22 22:31:13.261434: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1619100 of size 256 next 756
2024-10-22 22:31:13.261445: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1619200 of size 256 next 749
2024-10-22 22:31:13.261456: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1619300 of size 256 next 723
2024-10-22 22:31:13.261466: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1619400 of size 256 next 761
2024-10-22 22:31:13.261476: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1619500 of size 256 next 725
2024-10-22 22:31:13.261487: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1619600 of size 256 next 766
2024-10-22 22:31:13.261497: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1619700 of size 256 next 703
2024-10-22 22:31:13.261508: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1619800 of size 256 next 713
2024-10-22 22:31:13.261518: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1619900 of size 256 next 500
2024-10-22 22:31:13.261529: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1619a00 of size 256 next 757
2024-10-22 22:31:13.261539: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1619b00 of size 256 next 631
2024-10-22 22:31:13.261549: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1619c00 of size 256 next 622
2024-10-22 22:31:13.261559: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1619d00 of size 256 next 574
2024-10-22 22:31:13.261570: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1619e00 of size 256 next 728
2024-10-22 22:31:13.261580: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd1619f00 of size 256 next 870
2024-10-22 22:31:13.261591: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd161a000 of size 256 next 810
2024-10-22 22:31:13.261601: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd161a100 of size 256 next 904
2024-10-22 22:31:13.261612: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd161a200 of size 256 next 731
2024-10-22 22:31:13.261622: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd161a300 of size 256 next 694
2024-10-22 22:31:13.261633: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd161a400 of size 256 next 649
2024-10-22 22:31:13.261643: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd161a500 of size 256 next 256
2024-10-22 22:31:13.261653: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd161a600 of size 256 next 274
2024-10-22 22:31:13.261664: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd161a700 of size 256 next 424
2024-10-22 22:31:13.261674: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd161a800 of size 256 next 425
2024-10-22 22:31:13.261684: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f1dd161a900 of size 195840 next 239
2024-10-22 22:31:13.261695: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1dd164a600 of size 256 next 168
2024-10-22 22:31:13.261705: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f1dd164a700 of size 245061888 next 18446744073709551615
2024-10-22 22:31:13.261716: I external/local_tsl/tsl/framework/bfc_allocator.cc:1075] Next region of size 134217728
2024-10-22 22:31:13.261727: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de0000000 of size 22595072 next 143
2024-10-22 22:31:13.261744: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158c600 of size 256 next 801
2024-10-22 22:31:13.261756: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158c700 of size 256 next 739
2024-10-22 22:31:13.261766: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158c800 of size 256 next 826
2024-10-22 22:31:13.261776: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158c900 of size 256 next 802
2024-10-22 22:31:13.261786: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158ca00 of size 256 next 817
2024-10-22 22:31:13.261797: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158cb00 of size 256 next 659
2024-10-22 22:31:13.261808: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158cc00 of size 256 next 794
2024-10-22 22:31:13.261818: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158cd00 of size 256 next 860
2024-10-22 22:31:13.261829: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158ce00 of size 256 next 855
2024-10-22 22:31:13.261839: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158cf00 of size 256 next 755
2024-10-22 22:31:13.261849: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158d000 of size 256 next 747
2024-10-22 22:31:13.261859: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158d100 of size 256 next 854
2024-10-22 22:31:13.261870: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158d200 of size 256 next 636
2024-10-22 22:31:13.261881: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158d300 of size 256 next 616
2024-10-22 22:31:13.261891: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158d400 of size 256 next 1005
2024-10-22 22:31:13.261901: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158d500 of size 256 next 1006
2024-10-22 22:31:13.261912: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158d600 of size 256 next 1098
2024-10-22 22:31:13.261923: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158d700 of size 256 next 1099
2024-10-22 22:31:13.261933: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158d800 of size 256 next 1109
2024-10-22 22:31:13.261943: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158d900 of size 256 next 1130
2024-10-22 22:31:13.261954: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158da00 of size 256 next 1131
2024-10-22 22:31:13.261964: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158db00 of size 256 next 1260
2024-10-22 22:31:13.261975: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158dc00 of size 256 next 1808
2024-10-22 22:31:13.261985: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158dd00 of size 256 next 1881
2024-10-22 22:31:13.261995: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158de00 of size 256 next 1888
2024-10-22 22:31:13.262006: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158df00 of size 256 next 1920
2024-10-22 22:31:13.262016: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158e000 of size 256 next 1963
2024-10-22 22:31:13.262027: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158e100 of size 256 next 2063
2024-10-22 22:31:13.262037: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158e200 of size 256 next 2070
2024-10-22 22:31:13.262048: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158e300 of size 256 next 2080
2024-10-22 22:31:13.262058: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158e400 of size 256 next 2008
2024-10-22 22:31:13.262068: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158e500 of size 256 next 2041
2024-10-22 22:31:13.262084: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158e600 of size 256 next 2082
2024-10-22 22:31:13.262095: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158e700 of size 256 next 1949
2024-10-22 22:31:13.262105: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158e800 of size 256 next 2019
2024-10-22 22:31:13.262116: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158e900 of size 256 next 2061
2024-10-22 22:31:13.262126: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158ea00 of size 256 next 2066
2024-10-22 22:31:13.262137: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158eb00 of size 256 next 2081
2024-10-22 22:31:13.262147: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158ec00 of size 256 next 2022
2024-10-22 22:31:13.262157: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158ed00 of size 256 next 1924
2024-10-22 22:31:13.262168: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158ee00 of size 256 next 2007
2024-10-22 22:31:13.262178: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158ef00 of size 256 next 2100
2024-10-22 22:31:13.262189: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158f000 of size 256 next 2106
2024-10-22 22:31:13.262199: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158f100 of size 256 next 1996
2024-10-22 22:31:13.262210: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158f200 of size 256 next 2056
2024-10-22 22:31:13.262220: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158f300 of size 256 next 1999
2024-10-22 22:31:13.262230: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158f400 of size 256 next 2090
2024-10-22 22:31:13.262240: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158f500 of size 256 next 2013
2024-10-22 22:31:13.262251: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158f600 of size 256 next 2052
2024-10-22 22:31:13.262262: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158f700 of size 256 next 2051
2024-10-22 22:31:13.262272: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158f800 of size 256 next 2069
2024-10-22 22:31:13.262282: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158f900 of size 256 next 2028
2024-10-22 22:31:13.262302: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158fa00 of size 256 next 2068
2024-10-22 22:31:13.262307: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158fb00 of size 256 next 2037
2024-10-22 22:31:13.262311: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158fc00 of size 256 next 2074
2024-10-22 22:31:13.262315: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158fd00 of size 256 next 2098
2024-10-22 22:31:13.262320: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158fe00 of size 256 next 2009
2024-10-22 22:31:13.262324: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de158ff00 of size 256 next 2023
2024-10-22 22:31:13.262328: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1590000 of size 256 next 2065
2024-10-22 22:31:13.262333: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1590100 of size 256 next 2085
2024-10-22 22:31:13.262337: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1590200 of size 256 next 2113
2024-10-22 22:31:13.262342: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1590300 of size 256 next 2059
2024-10-22 22:31:13.262346: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1590400 of size 256 next 2096
2024-10-22 22:31:13.262355: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1590500 of size 256 next 2005
2024-10-22 22:31:13.262359: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1590600 of size 256 next 2026
2024-10-22 22:31:13.262363: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1590700 of size 256 next 2084
2024-10-22 22:31:13.262367: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1590800 of size 256 next 2101
2024-10-22 22:31:13.262372: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1590900 of size 256 next 2091
2024-10-22 22:31:13.262378: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1590a00 of size 256 next 2092
2024-10-22 22:31:13.262382: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1590b00 of size 256 next 2110
2024-10-22 22:31:13.262386: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1590c00 of size 256 next 2001
2024-10-22 22:31:13.262390: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1590d00 of size 256 next 2072
2024-10-22 22:31:13.262394: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1590e00 of size 256 next 2049
2024-10-22 22:31:13.262399: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1590f00 of size 256 next 2077
2024-10-22 22:31:13.262403: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1591000 of size 256 next 2067
2024-10-22 22:31:13.262407: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1591100 of size 256 next 2152
2024-10-22 22:31:13.262411: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1591200 of size 256 next 2131
2024-10-22 22:31:13.262416: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1591300 of size 256 next 2117
2024-10-22 22:31:13.262420: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1591400 of size 256 next 2137
2024-10-22 22:31:13.262424: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1591500 of size 256 next 2089
2024-10-22 22:31:13.262428: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1591600 of size 256 next 2139
2024-10-22 22:31:13.262433: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1591700 of size 256 next 2129
2024-10-22 22:31:13.262437: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1591800 of size 256 next 2145
2024-10-22 22:31:13.262441: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1591900 of size 256 next 2127
2024-10-22 22:31:13.262446: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1591a00 of size 256 next 2122
2024-10-22 22:31:13.262450: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1591b00 of size 256 next 2087
2024-10-22 22:31:13.262455: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1591c00 of size 256 next 2102
2024-10-22 22:31:13.262459: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1591d00 of size 256 next 2141
2024-10-22 22:31:13.262464: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1591e00 of size 256 next 2130
2024-10-22 22:31:13.262468: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1591f00 of size 256 next 2104
2024-10-22 22:31:13.262472: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1592000 of size 256 next 2048
2024-10-22 22:31:13.262476: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1592100 of size 256 next 2128
2024-10-22 22:31:13.262482: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1592200 of size 256 next 2121
2024-10-22 22:31:13.262486: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1592300 of size 256 next 2073
2024-10-22 22:31:13.262490: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1592400 of size 256 next 2140
2024-10-22 22:31:13.262499: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1592500 of size 256 next 2149
2024-10-22 22:31:13.262504: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1592600 of size 256 next 2158
2024-10-22 22:31:13.262509: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1592700 of size 256 next 2094
2024-10-22 22:31:13.262513: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1592800 of size 256 next 2136
2024-10-22 22:31:13.262517: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1592900 of size 256 next 2112
2024-10-22 22:31:13.262523: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1592a00 of size 256 next 2038
2024-10-22 22:31:13.262527: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1592b00 of size 256 next 2142
2024-10-22 22:31:13.262531: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1592c00 of size 256 next 2118
2024-10-22 22:31:13.262535: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1592d00 of size 256 next 2125
2024-10-22 22:31:13.262539: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1592e00 of size 256 next 2116
2024-10-22 22:31:13.262544: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1592f00 of size 256 next 2157
2024-10-22 22:31:13.262548: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1593000 of size 256 next 2124
2024-10-22 22:31:13.262552: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1593100 of size 256 next 2188
2024-10-22 22:31:13.262556: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1593200 of size 256 next 2265
2024-10-22 22:31:13.262560: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1593300 of size 256 next 2134
2024-10-22 22:31:13.262565: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1593400 of size 256 next 2018
2024-10-22 22:31:13.262569: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1593500 of size 256 next 2095
2024-10-22 22:31:13.262573: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1593600 of size 256 next 2108
2024-10-22 22:31:13.262578: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1593700 of size 256 next 2154
2024-10-22 22:31:13.262582: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1593800 of size 256 next 2044
2024-10-22 22:31:13.262586: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1593900 of size 256 next 2078
2024-10-22 22:31:13.262591: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1593a00 of size 256 next 2111
2024-10-22 22:31:13.262595: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1593b00 of size 256 next 2183
2024-10-22 22:31:13.262599: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1593c00 of size 256 next 2177
2024-10-22 22:31:13.262603: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1593d00 of size 256 next 2187
2024-10-22 22:31:13.262608: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1593e00 of size 256 next 2166
2024-10-22 22:31:13.262612: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1593f00 of size 256 next 2155
2024-10-22 22:31:13.262616: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1594000 of size 256 next 2189
2024-10-22 22:31:13.262620: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1594100 of size 256 next 2171
2024-10-22 22:31:13.262624: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1594200 of size 256 next 2075
2024-10-22 22:31:13.262629: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1594300 of size 256 next 2172
2024-10-22 22:31:13.262646: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1594400 of size 256 next 2107
2024-10-22 22:31:13.262650: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1594500 of size 256 next 2160
2024-10-22 22:31:13.262655: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1594600 of size 256 next 690
2024-10-22 22:31:13.262659: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1594700 of size 256 next 691
2024-10-22 22:31:13.262665: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1594800 of size 256 next 2763
2024-10-22 22:31:13.262669: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1594900 of size 256 next 2771
2024-10-22 22:31:13.262673: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1594a00 of size 256 next 2779
2024-10-22 22:31:13.262678: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1594b00 of size 256 next 2787
2024-10-22 22:31:13.262682: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f1de1594c00 of size 84480 next 919
2024-10-22 22:31:13.262687: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15a9600 of size 256 next 880
2024-10-22 22:31:13.262691: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15a9700 of size 256 next 995
2024-10-22 22:31:13.262697: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15a9800 of size 256 next 827
2024-10-22 22:31:13.262702: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15a9900 of size 256 next 978
2024-10-22 22:31:13.262706: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15a9a00 of size 256 next 989
2024-10-22 22:31:13.262710: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15a9b00 of size 256 next 973
2024-10-22 22:31:13.262714: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15a9c00 of size 256 next 406
2024-10-22 22:31:13.262718: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15a9d00 of size 256 next 863
2024-10-22 22:31:13.262723: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15a9e00 of size 256 next 657
2024-10-22 22:31:13.262727: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15a9f00 of size 256 next 955
2024-10-22 22:31:13.262731: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15aa000 of size 256 next 974
2024-10-22 22:31:13.262736: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15aa100 of size 256 next 1204
2024-10-22 22:31:13.262740: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15aa200 of size 256 next 1375
2024-10-22 22:31:13.262744: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15aa300 of size 256 next 1042
2024-10-22 22:31:13.262750: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15aa400 of size 256 next 1014
2024-10-22 22:31:13.262755: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15aa500 of size 256 next 981
2024-10-22 22:31:13.262759: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15aa600 of size 256 next 719
2024-10-22 22:31:13.262763: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15aa700 of size 256 next 970
2024-10-22 22:31:13.262767: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15aa800 of size 256 next 965
2024-10-22 22:31:13.262772: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15aa900 of size 256 next 582
2024-10-22 22:31:13.262776: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15aaa00 of size 256 next 946
2024-10-22 22:31:13.262780: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15aab00 of size 256 next 924
2024-10-22 22:31:13.262785: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15aac00 of size 256 next 660
2024-10-22 22:31:13.262793: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15aad00 of size 256 next 866
2024-10-22 22:31:13.262797: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15aae00 of size 256 next 934
2024-10-22 22:31:13.262802: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15aaf00 of size 256 next 681
2024-10-22 22:31:13.262806: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ab000 of size 256 next 862
2024-10-22 22:31:13.262810: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ab100 of size 256 next 925
2024-10-22 22:31:13.262814: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ab200 of size 256 next 853
2024-10-22 22:31:13.262819: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ab300 of size 256 next 982
2024-10-22 22:31:13.262823: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ab400 of size 256 next 1029
2024-10-22 22:31:13.262828: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ab500 of size 256 next 1119
2024-10-22 22:31:13.262832: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ab600 of size 256 next 1108
2024-10-22 22:31:13.262837: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ab700 of size 256 next 918
2024-10-22 22:31:13.262841: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ab800 of size 256 next 976
2024-10-22 22:31:13.262845: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ab900 of size 256 next 913
2024-10-22 22:31:13.262850: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15aba00 of size 256 next 799
2024-10-22 22:31:13.262854: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15abb00 of size 256 next 879
2024-10-22 22:31:13.262858: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15abc00 of size 256 next 806
2024-10-22 22:31:13.262862: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15abd00 of size 256 next 829
2024-10-22 22:31:13.262866: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15abe00 of size 256 next 902
2024-10-22 22:31:13.262870: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15abf00 of size 256 next 931
2024-10-22 22:31:13.262875: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ac000 of size 256 next 1031
2024-10-22 22:31:13.262879: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ac100 of size 256 next 977
2024-10-22 22:31:13.262883: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ac200 of size 256 next 1071
2024-10-22 22:31:13.262887: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ac300 of size 256 next 964
2024-10-22 22:31:13.262891: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ac400 of size 256 next 873
2024-10-22 22:31:13.262895: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ac500 of size 256 next 768
2024-10-22 22:31:13.262900: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ac600 of size 256 next 997
2024-10-22 22:31:13.262904: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ac700 of size 256 next 841
2024-10-22 22:31:13.262909: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ac800 of size 256 next 1019
2024-10-22 22:31:13.262913: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ac900 of size 256 next 744
2024-10-22 22:31:13.262917: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15aca00 of size 256 next 294
2024-10-22 22:31:13.262921: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15acb00 of size 256 next 371
2024-10-22 22:31:13.262925: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15acc00 of size 256 next 418
2024-10-22 22:31:13.262933: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f1de15acd00 of size 65536 next 343
2024-10-22 22:31:13.262938: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bcd00 of size 256 next 672
2024-10-22 22:31:13.262942: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bce00 of size 256 next 687
2024-10-22 22:31:13.262946: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bcf00 of size 256 next 651
2024-10-22 22:31:13.262950: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bd000 of size 256 next 606
2024-10-22 22:31:13.262954: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bd100 of size 256 next 594
2024-10-22 22:31:13.262960: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bd200 of size 256 next 612
2024-10-22 22:31:13.262964: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bd300 of size 256 next 635
2024-10-22 22:31:13.262969: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bd400 of size 256 next 763
2024-10-22 22:31:13.262973: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bd500 of size 256 next 737
2024-10-22 22:31:13.262977: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bd600 of size 256 next 764
2024-10-22 22:31:13.262981: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bd700 of size 256 next 759
2024-10-22 22:31:13.262985: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bd800 of size 256 next 792
2024-10-22 22:31:13.262989: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bd900 of size 256 next 638
2024-10-22 22:31:13.262993: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bda00 of size 256 next 677
2024-10-22 22:31:13.262997: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bdb00 of size 256 next 604
2024-10-22 22:31:13.263002: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bdc00 of size 256 next 689
2024-10-22 22:31:13.263006: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bdd00 of size 256 next 390
2024-10-22 22:31:13.263010: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bde00 of size 256 next 1290
2024-10-22 22:31:13.263015: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bdf00 of size 256 next 1413
2024-10-22 22:31:13.263019: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15be000 of size 256 next 1432
2024-10-22 22:31:13.263023: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15be100 of size 256 next 1439
2024-10-22 22:31:13.263028: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15be200 of size 256 next 1449
2024-10-22 22:31:13.263032: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15be300 of size 256 next 1176
2024-10-22 22:31:13.263036: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15be400 of size 256 next 1440
2024-10-22 22:31:13.263040: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15be500 of size 256 next 1438
2024-10-22 22:31:13.263045: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15be600 of size 256 next 1462
2024-10-22 22:31:13.263049: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15be700 of size 256 next 1357
2024-10-22 22:31:13.263055: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15be800 of size 256 next 1437
2024-10-22 22:31:13.263059: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15be900 of size 256 next 1052
2024-10-22 22:31:13.263064: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bea00 of size 256 next 1305
2024-10-22 22:31:13.263071: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15beb00 of size 256 next 1422
2024-10-22 22:31:13.263076: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bec00 of size 256 next 1478
2024-10-22 22:31:13.263080: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bed00 of size 256 next 1522
2024-10-22 22:31:13.263084: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bee00 of size 256 next 1536
2024-10-22 22:31:13.263090: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bef00 of size 256 next 1564
2024-10-22 22:31:13.263095: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bf000 of size 256 next 1354
2024-10-22 22:31:13.263099: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bf100 of size 256 next 1298
2024-10-22 22:31:13.263103: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bf200 of size 256 next 1410
2024-10-22 22:31:13.263107: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bf300 of size 256 next 1494
2024-10-22 22:31:13.263113: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bf400 of size 256 next 1400
2024-10-22 22:31:13.263118: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bf500 of size 256 next 1371
2024-10-22 22:31:13.263122: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bf600 of size 256 next 1406
2024-10-22 22:31:13.263126: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bf700 of size 256 next 1391
2024-10-22 22:31:13.263130: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bf800 of size 256 next 1308
2024-10-22 22:31:13.263134: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bf900 of size 256 next 1467
2024-10-22 22:31:13.263139: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bfa00 of size 256 next 1461
2024-10-22 22:31:13.263143: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bfb00 of size 256 next 1445
2024-10-22 22:31:13.263147: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bfc00 of size 256 next 1418
2024-10-22 22:31:13.263151: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bfd00 of size 256 next 1448
2024-10-22 22:31:13.263156: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bfe00 of size 256 next 1513
2024-10-22 22:31:13.263160: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15bff00 of size 256 next 1247
2024-10-22 22:31:13.263164: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c0000 of size 256 next 1340
2024-10-22 22:31:13.263168: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c0100 of size 256 next 1436
2024-10-22 22:31:13.263172: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c0200 of size 256 next 1387
2024-10-22 22:31:13.263177: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c0300 of size 256 next 1497
2024-10-22 22:31:13.263181: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c0400 of size 256 next 1503
2024-10-22 22:31:13.263185: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c0500 of size 256 next 1504
2024-10-22 22:31:13.263189: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c0600 of size 256 next 1129
2024-10-22 22:31:13.263193: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c0700 of size 256 next 1455
2024-10-22 22:31:13.263197: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c0800 of size 256 next 1429
2024-10-22 22:31:13.263201: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c0900 of size 256 next 1313
2024-10-22 22:31:13.263205: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c0a00 of size 256 next 1500
2024-10-22 22:31:13.263213: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c0b00 of size 256 next 1431
2024-10-22 22:31:13.263218: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c0c00 of size 256 next 1499
2024-10-22 22:31:13.263222: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c0d00 of size 256 next 1552
2024-10-22 22:31:13.263226: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c0e00 of size 256 next 1566
2024-10-22 22:31:13.263230: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c0f00 of size 256 next 1420
2024-10-22 22:31:13.263234: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c1000 of size 256 next 1487
2024-10-22 22:31:13.263239: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c1100 of size 256 next 1374
2024-10-22 22:31:13.263244: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c1200 of size 256 next 1496
2024-10-22 22:31:13.263248: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c1300 of size 256 next 1514
2024-10-22 22:31:13.263252: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c1400 of size 256 next 1483
2024-10-22 22:31:13.263256: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c1500 of size 256 next 1312
2024-10-22 22:31:13.263261: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c1600 of size 256 next 1394
2024-10-22 22:31:13.263265: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c1700 of size 256 next 1367
2024-10-22 22:31:13.263269: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c1800 of size 256 next 82
2024-10-22 22:31:13.263273: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c1900 of size 256 next 1690
2024-10-22 22:31:13.263278: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c1a00 of size 256 next 1509
2024-10-22 22:31:13.263282: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c1b00 of size 256 next 1624
2024-10-22 22:31:13.263286: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c1c00 of size 256 next 1641
2024-10-22 22:31:13.263290: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c1d00 of size 256 next 1584
2024-10-22 22:31:13.263294: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c1e00 of size 256 next 1557
2024-10-22 22:31:13.263299: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c1f00 of size 256 next 1480
2024-10-22 22:31:13.263303: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c2000 of size 256 next 1336
2024-10-22 22:31:13.263307: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c2100 of size 256 next 1329
2024-10-22 22:31:13.263312: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c2200 of size 256 next 1442
2024-10-22 22:31:13.263316: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c2300 of size 256 next 1446
2024-10-22 22:31:13.263320: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c2400 of size 256 next 1561
2024-10-22 22:31:13.263324: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c2500 of size 256 next 1540
2024-10-22 22:31:13.263328: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c2600 of size 256 next 1745
2024-10-22 22:31:13.263332: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c2700 of size 256 next 1393
2024-10-22 22:31:13.263336: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c2800 of size 256 next 1328
2024-10-22 22:31:13.263341: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c2900 of size 256 next 1335
2024-10-22 22:31:13.263349: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c2a00 of size 256 next 1435
2024-10-22 22:31:13.263354: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c2b00 of size 256 next 1325
2024-10-22 22:31:13.263358: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c2c00 of size 256 next 1399
2024-10-22 22:31:13.263362: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c2d00 of size 256 next 1377
2024-10-22 22:31:13.263366: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c2e00 of size 256 next 1469
2024-10-22 22:31:13.263372: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c2f00 of size 256 next 1481
2024-10-22 22:31:13.263377: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c3000 of size 256 next 1493
2024-10-22 22:31:13.263381: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c3100 of size 256 next 1596
2024-10-22 22:31:13.263385: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c3200 of size 256 next 1486
2024-10-22 22:31:13.263389: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c3300 of size 256 next 1490
2024-10-22 22:31:13.263393: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c3400 of size 256 next 1063
2024-10-22 22:31:13.263397: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c3500 of size 256 next 1528
2024-10-22 22:31:13.263401: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c3600 of size 256 next 1526
2024-10-22 22:31:13.263406: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c3700 of size 256 next 1250
2024-10-22 22:31:13.263410: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c3800 of size 256 next 1498
2024-10-22 22:31:13.263414: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c3900 of size 256 next 1530
2024-10-22 22:31:13.263419: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c3a00 of size 256 next 1388
2024-10-22 22:31:13.263423: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c3b00 of size 256 next 1585
2024-10-22 22:31:13.263427: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c3c00 of size 256 next 1433
2024-10-22 22:31:13.263431: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c3d00 of size 256 next 1507
2024-10-22 22:31:13.263435: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c3e00 of size 256 next 1575
2024-10-22 22:31:13.263440: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c3f00 of size 256 next 1402
2024-10-22 22:31:13.263444: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c4000 of size 256 next 1588
2024-10-22 22:31:13.263448: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c4100 of size 256 next 1589
2024-10-22 22:31:13.263452: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c4200 of size 256 next 1475
2024-10-22 22:31:13.263457: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c4300 of size 256 next 1430
2024-10-22 22:31:13.263461: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c4400 of size 256 next 1343
2024-10-22 22:31:13.263465: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c4500 of size 256 next 1379
2024-10-22 22:31:13.263469: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c4600 of size 768 next 1134
2024-10-22 22:31:13.263474: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c4900 of size 256 next 1351
2024-10-22 22:31:13.263478: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c4a00 of size 256 next 1570
2024-10-22 22:31:13.263482: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c4b00 of size 256 next 1428
2024-10-22 22:31:13.263490: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c4c00 of size 256 next 1307
2024-10-22 22:31:13.263495: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c4d00 of size 256 next 1644
2024-10-22 22:31:13.263499: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c4e00 of size 256 next 1701
2024-10-22 22:31:13.263503: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c4f00 of size 256 next 1608
2024-10-22 22:31:13.263507: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c5000 of size 256 next 1495
2024-10-22 22:31:13.263513: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c5100 of size 256 next 1479
2024-10-22 22:31:13.263517: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c5200 of size 256 next 1529
2024-10-22 22:31:13.263521: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c5300 of size 256 next 1615
2024-10-22 22:31:13.263526: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c5400 of size 256 next 1453
2024-10-22 22:31:13.263530: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c5500 of size 256 next 1549
2024-10-22 22:31:13.263534: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c5600 of size 256 next 1466
2024-10-22 22:31:13.263538: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c5700 of size 256 next 1523
2024-10-22 22:31:13.263542: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c5800 of size 256 next 1468
2024-10-22 22:31:13.263547: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c5900 of size 256 next 1531
2024-10-22 22:31:13.263551: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c5a00 of size 256 next 1565
2024-10-22 22:31:13.263555: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c5b00 of size 256 next 1508
2024-10-22 22:31:13.263559: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c5c00 of size 256 next 1631
2024-10-22 22:31:13.263563: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c5d00 of size 256 next 55
2024-10-22 22:31:13.263567: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c5e00 of size 256 next 135
2024-10-22 22:31:13.263571: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c5f00 of size 256 next 69
2024-10-22 22:31:13.263576: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c6000 of size 256 next 1539
2024-10-22 22:31:13.263580: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c6100 of size 256 next 1417
2024-10-22 22:31:13.263584: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c6200 of size 256 next 1491
2024-10-22 22:31:13.263591: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c6300 of size 256 next 1457
2024-10-22 22:31:13.263596: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c6400 of size 256 next 1548
2024-10-22 22:31:13.263600: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c6500 of size 256 next 1541
2024-10-22 22:31:13.263604: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c6600 of size 256 next 1471
2024-10-22 22:31:13.263609: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c6700 of size 256 next 937
2024-10-22 22:31:13.263630: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c6800 of size 256 next 983
2024-10-22 22:31:13.263635: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c6900 of size 256 next 988
2024-10-22 22:31:13.263639: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c6a00 of size 256 next 1041
2024-10-22 22:31:13.263645: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c6b00 of size 256 next 1007
2024-10-22 22:31:13.263652: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c6c00 of size 256 next 1133
2024-10-22 22:31:13.263656: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c6d00 of size 256 next 1111
2024-10-22 22:31:13.263660: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c6e00 of size 256 next 1049
2024-10-22 22:31:13.263664: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c6f00 of size 256 next 1117
2024-10-22 22:31:13.263669: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c7000 of size 256 next 923
2024-10-22 22:31:13.263673: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15c7100 of size 256 next 980
2024-10-22 22:31:13.263677: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f1de15c7200 of size 160768 next 2942
2024-10-22 22:31:13.263682: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ee600 of size 256 next 2938
2024-10-22 22:31:13.263686: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f1de15ee700 of size 67072 next 2813
2024-10-22 22:31:13.263691: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15fed00 of size 256 next 2837
2024-10-22 22:31:13.263696: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15fee00 of size 256 next 2742
2024-10-22 22:31:13.263700: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15fef00 of size 256 next 2756
2024-10-22 22:31:13.263704: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ff000 of size 256 next 2876
2024-10-22 22:31:13.263709: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ff100 of size 256 next 2830
2024-10-22 22:31:13.263713: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ff200 of size 256 next 2825
2024-10-22 22:31:13.263717: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ff300 of size 256 next 2804
2024-10-22 22:31:13.263721: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ff400 of size 256 next 2838
2024-10-22 22:31:13.263725: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ff500 of size 256 next 2705
2024-10-22 22:31:13.263730: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ff600 of size 256 next 2709
2024-10-22 22:31:13.263734: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ff700 of size 256 next 2821
2024-10-22 22:31:13.263738: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ff800 of size 256 next 2754
2024-10-22 22:31:13.263743: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ff900 of size 256 next 2880
2024-10-22 22:31:13.263747: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ffa00 of size 256 next 2865
2024-10-22 22:31:13.263751: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ffb00 of size 256 next 2843
2024-10-22 22:31:13.263756: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ffc00 of size 256 next 2896
2024-10-22 22:31:13.263760: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ffd00 of size 256 next 2862
2024-10-22 22:31:13.263766: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15ffe00 of size 256 next 2856
2024-10-22 22:31:13.263771: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de15fff00 of size 256 next 2871
2024-10-22 22:31:13.263775: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1600000 of size 256 next 2849
2024-10-22 22:31:13.263779: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1600100 of size 256 next 2867
2024-10-22 22:31:13.263784: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1600200 of size 256 next 2910
2024-10-22 22:31:13.263792: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1600300 of size 256 next 2899
2024-10-22 22:31:13.263796: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1600400 of size 256 next 2879
2024-10-22 22:31:13.263801: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1600500 of size 256 next 2868
2024-10-22 22:31:13.263805: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1600600 of size 256 next 2905
2024-10-22 22:31:13.263810: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1600700 of size 256 next 2809
2024-10-22 22:31:13.263815: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1600800 of size 256 next 2921
2024-10-22 22:31:13.263819: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1600900 of size 256 next 2812
2024-10-22 22:31:13.263823: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1600a00 of size 256 next 2958
2024-10-22 22:31:13.263827: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1600b00 of size 256 next 2898
2024-10-22 22:31:13.263834: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1600c00 of size 256 next 2848
2024-10-22 22:31:13.263838: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1600d00 of size 256 next 2917
2024-10-22 22:31:13.263842: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1600e00 of size 256 next 2904
2024-10-22 22:31:13.263846: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1600f00 of size 256 next 2922
2024-10-22 22:31:13.263851: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1601000 of size 256 next 2926
2024-10-22 22:31:13.263855: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1601100 of size 256 next 2857
2024-10-22 22:31:13.263859: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1601200 of size 256 next 2912
2024-10-22 22:31:13.263864: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1601300 of size 256 next 2903
2024-10-22 22:31:13.263870: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1601400 of size 256 next 2846
2024-10-22 22:31:13.263874: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1601500 of size 256 next 2743
2024-10-22 22:31:13.263879: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1601600 of size 256 next 2970
2024-10-22 22:31:13.263883: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1601700 of size 256 next 2946
2024-10-22 22:31:13.263887: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1601800 of size 256 next 2988
2024-10-22 22:31:13.263891: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1601900 of size 256 next 2945
2024-10-22 22:31:13.263896: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1601a00 of size 256 next 2947
2024-10-22 22:31:13.263901: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1601b00 of size 256 next 2915
2024-10-22 22:31:13.263905: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1601c00 of size 256 next 2901
2024-10-22 22:31:13.263909: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1601d00 of size 256 next 2920
2024-10-22 22:31:13.263913: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1601e00 of size 256 next 2900
2024-10-22 22:31:13.263918: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1601f00 of size 256 next 2888
2024-10-22 22:31:13.263922: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1602000 of size 256 next 2902
2024-10-22 22:31:13.263928: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1602100 of size 256 next 2916
2024-10-22 22:31:13.263934: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1602200 of size 256 next 2925
2024-10-22 22:31:13.263939: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1602300 of size 256 next 2795
2024-10-22 22:31:13.263943: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1602400 of size 256 next 2923
2024-10-22 22:31:13.263950: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1602500 of size 256 next 2927
2024-10-22 22:31:13.263954: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1602600 of size 256 next 2889
2024-10-22 22:31:13.263958: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1602700 of size 256 next 2931
2024-10-22 22:31:13.263963: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1602800 of size 256 next 2884
2024-10-22 22:31:13.263967: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1602900 of size 256 next 2877
2024-10-22 22:31:13.263971: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1602a00 of size 256 next 2878
2024-10-22 22:31:13.263975: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1602b00 of size 256 next 2861
2024-10-22 22:31:13.263980: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1602c00 of size 256 next 2883
2024-10-22 22:31:13.263985: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1602d00 of size 256 next 2887
2024-10-22 22:31:13.263989: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1602e00 of size 256 next 2873
2024-10-22 22:31:13.263993: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1602f00 of size 256 next 2820
2024-10-22 22:31:13.263998: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f1de1603000 of size 56832 next 984
2024-10-22 22:31:13.264001: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1610e00 of size 256 next 921
2024-10-22 22:31:13.264005: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1610f00 of size 256 next 1065
2024-10-22 22:31:13.264010: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1611000 of size 256 next 1096
2024-10-22 22:31:13.264014: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1611100 of size 256 next 944
2024-10-22 22:31:13.264018: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1611200 of size 256 next 1010
2024-10-22 22:31:13.264022: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1611300 of size 256 next 941
2024-10-22 22:31:13.264026: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1611400 of size 256 next 899
2024-10-22 22:31:13.264030: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de1611500 of size 48910336 next 31
2024-10-22 22:31:13.264035: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f1de44b6500 of size 62167808 next 18446744073709551615
2024-10-22 22:31:13.264039: I external/local_tsl/tsl/framework/bfc_allocator.cc:1075] Next region of size 67108864
2024-10-22 22:31:13.264044: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de8000000 of size 22595072 next 16
2024-10-22 22:31:13.264048: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1de958c600 of size 44513792 next 18446744073709551615
2024-10-22 22:31:13.264052: I external/local_tsl/tsl/framework/bfc_allocator.cc:1075] Next region of size 33554432
2024-10-22 22:31:13.264057: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f28d6000000 of size 33554432 next 18446744073709551615
2024-10-22 22:31:13.264062: I external/local_tsl/tsl/framework/bfc_allocator.cc:1075] Next region of size 33554432
2024-10-22 22:31:13.264066: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f28d8000000 of size 16430848 next 1516
2024-10-22 22:31:13.264093: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f28d8fab700 of size 17123584 next 18446744073709551615
2024-10-22 22:31:13.264104: I external/local_tsl/tsl/framework/bfc_allocator.cc:1075] Next region of size 2097152
2024-10-22 22:31:13.264109: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73200000 of size 1280 next 1
2024-10-22 22:31:13.264113: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73200500 of size 256 next 2
2024-10-22 22:31:13.264118: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73200600 of size 256 next 206
2024-10-22 22:31:13.264122: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73200700 of size 256 next 8
2024-10-22 22:31:13.264126: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73200800 of size 256 next 7
2024-10-22 22:31:13.264130: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73200900 of size 256 next 6
2024-10-22 22:31:13.264135: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73200a00 of size 256 next 5
2024-10-22 22:31:13.264139: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73200b00 of size 768 next 10
2024-10-22 22:31:13.264144: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73200e00 of size 256 next 11
2024-10-22 22:31:13.264150: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73200f00 of size 256 next 12
2024-10-22 22:31:13.264155: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73201000 of size 256 next 4
2024-10-22 22:31:13.264159: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73201100 of size 512 next 3
2024-10-22 22:31:13.264164: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73201300 of size 512 next 14
2024-10-22 22:31:13.264169: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73201500 of size 256 next 56
2024-10-22 22:31:13.264173: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73201600 of size 256 next 49
2024-10-22 22:31:13.264177: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73201700 of size 256 next 154
2024-10-22 22:31:13.264182: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73201800 of size 256 next 18
2024-10-22 22:31:13.264186: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73201900 of size 256 next 19
2024-10-22 22:31:13.264191: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73201a00 of size 256 next 20
2024-10-22 22:31:13.264195: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73201b00 of size 256 next 21
2024-10-22 22:31:13.264199: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73201c00 of size 256 next 23
2024-10-22 22:31:13.264203: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73201d00 of size 256 next 24
2024-10-22 22:31:13.264208: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73201e00 of size 512 next 25
2024-10-22 22:31:13.264212: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73202000 of size 256 next 132
2024-10-22 22:31:13.264216: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73202100 of size 256 next 145
2024-10-22 22:31:13.264221: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73202200 of size 256 next 1679
2024-10-22 22:31:13.264225: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73202300 of size 256 next 32
2024-10-22 22:31:13.264229: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73202400 of size 256 next 33
2024-10-22 22:31:13.264234: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73202500 of size 256 next 165
2024-10-22 22:31:13.264238: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73202600 of size 256 next 34
2024-10-22 22:31:13.264242: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73202700 of size 256 next 35
2024-10-22 22:31:13.264250: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73202800 of size 256 next 194
2024-10-22 22:31:13.264254: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73202900 of size 256 next 207
2024-10-22 22:31:13.264258: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73202a00 of size 256 next 218
2024-10-22 22:31:13.264265: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73202b00 of size 256 next 37
2024-10-22 22:31:13.264269: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73202c00 of size 256 next 1578
2024-10-22 22:31:13.264273: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73202d00 of size 256 next 136
2024-10-22 22:31:13.264278: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73202e00 of size 256 next 1558
2024-10-22 22:31:13.264282: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73202f00 of size 256 next 39
2024-10-22 22:31:13.264286: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73203000 of size 256 next 191
2024-10-22 22:31:13.264291: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73203100 of size 256 next 40
2024-10-22 22:31:13.264296: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73203200 of size 256 next 2349
2024-10-22 22:31:13.264301: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73203300 of size 256 next 2105
2024-10-22 22:31:13.264305: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73203400 of size 256 next 2364
2024-10-22 22:31:13.264309: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73203500 of size 256 next 2340
2024-10-22 22:31:13.264314: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73203600 of size 256 next 2375
2024-10-22 22:31:13.264318: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73203700 of size 256 next 2369
2024-10-22 22:31:13.264322: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f2b73203800 of size 59136 next 26
2024-10-22 22:31:13.264327: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73211f00 of size 32768 next 27
2024-10-22 22:31:13.264331: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f2b73219f00 of size 350976 next 392
2024-10-22 22:31:13.264336: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7326fa00 of size 256 next 370
2024-10-22 22:31:13.264340: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7326fb00 of size 256 next 269
2024-10-22 22:31:13.264345: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7326fc00 of size 256 next 437
2024-10-22 22:31:13.264349: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7326fd00 of size 256 next 433
2024-10-22 22:31:13.264353: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7326fe00 of size 256 next 409
2024-10-22 22:31:13.264357: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7326ff00 of size 256 next 321
2024-10-22 22:31:13.264361: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73270000 of size 256 next 253
2024-10-22 22:31:13.264365: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73270100 of size 256 next 421
2024-10-22 22:31:13.264370: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73270200 of size 256 next 179
2024-10-22 22:31:13.264375: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73270300 of size 256 next 297
2024-10-22 22:31:13.264379: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73270400 of size 256 next 361
2024-10-22 22:31:13.264383: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73270500 of size 256 next 327
2024-10-22 22:31:13.264391: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73270600 of size 256 next 387
2024-10-22 22:31:13.264396: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73270700 of size 256 next 432
2024-10-22 22:31:13.264400: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73270800 of size 256 next 289
2024-10-22 22:31:13.264404: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73270900 of size 256 next 360
2024-10-22 22:31:13.264408: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73270a00 of size 256 next 460
2024-10-22 22:31:13.264413: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73270b00 of size 256 next 404
2024-10-22 22:31:13.264417: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73270c00 of size 256 next 456
2024-10-22 22:31:13.264422: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73270d00 of size 256 next 468
2024-10-22 22:31:13.264428: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73270e00 of size 256 next 446
2024-10-22 22:31:13.264433: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73270f00 of size 256 next 352
2024-10-22 22:31:13.264437: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73271000 of size 256 next 301
2024-10-22 22:31:13.264441: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73271100 of size 256 next 383
2024-10-22 22:31:13.264445: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73271200 of size 256 next 259
2024-10-22 22:31:13.264450: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73271300 of size 256 next 292
2024-10-22 22:31:13.264454: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73271400 of size 256 next 396
2024-10-22 22:31:13.264458: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73271500 of size 256 next 560
2024-10-22 22:31:13.264463: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73271600 of size 256 next 423
2024-10-22 22:31:13.264468: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73271700 of size 256 next 236
2024-10-22 22:31:13.264472: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73271800 of size 256 next 461
2024-10-22 22:31:13.264476: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73271900 of size 256 next 315
2024-10-22 22:31:13.264481: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73271a00 of size 256 next 466
2024-10-22 22:31:13.264485: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73271b00 of size 256 next 482
2024-10-22 22:31:13.264490: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73271c00 of size 256 next 388
2024-10-22 22:31:13.264494: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73271d00 of size 256 next 429
2024-10-22 22:31:13.264498: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73271e00 of size 256 next 180
2024-10-22 22:31:13.264503: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73271f00 of size 256 next 312
2024-10-22 22:31:13.264507: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73272000 of size 256 next 426
2024-10-22 22:31:13.264511: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73272100 of size 256 next 447
2024-10-22 22:31:13.264516: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73272200 of size 256 next 402
2024-10-22 22:31:13.264520: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73272300 of size 256 next 458
2024-10-22 22:31:13.264525: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73272400 of size 256 next 502
2024-10-22 22:31:13.264529: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73272500 of size 256 next 508
2024-10-22 22:31:13.264538: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73272600 of size 256 next 385
2024-10-22 22:31:13.264543: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73272700 of size 256 next 415
2024-10-22 22:31:13.264547: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73272800 of size 256 next 104
2024-10-22 22:31:13.264551: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73272900 of size 256 next 68
2024-10-22 22:31:13.264555: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73272a00 of size 256 next 428
2024-10-22 22:31:13.264560: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73272b00 of size 256 next 483
2024-10-22 22:31:13.264564: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73272c00 of size 256 next 427
2024-10-22 22:31:13.264568: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73272d00 of size 256 next 340
2024-10-22 22:31:13.264573: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73272e00 of size 256 next 519
2024-10-22 22:31:13.264577: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73272f00 of size 256 next 464
2024-10-22 22:31:13.264581: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73273000 of size 256 next 489
2024-10-22 22:31:13.264585: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73273100 of size 256 next 397
2024-10-22 22:31:13.264590: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73273200 of size 256 next 414
2024-10-22 22:31:13.264594: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73273300 of size 256 next 511
2024-10-22 22:31:13.264599: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73273400 of size 256 next 480
2024-10-22 22:31:13.264603: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73273500 of size 256 next 334
2024-10-22 22:31:13.264607: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73273600 of size 256 next 485
2024-10-22 22:31:13.264611: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73273700 of size 256 next 438
2024-10-22 22:31:13.264616: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73273800 of size 256 next 521
2024-10-22 22:31:13.264620: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73273900 of size 256 next 463
2024-10-22 22:31:13.264624: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73273a00 of size 256 next 528
2024-10-22 22:31:13.264629: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73273b00 of size 256 next 377
2024-10-22 22:31:13.264633: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73273c00 of size 256 next 473
2024-10-22 22:31:13.264637: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73273d00 of size 256 next 533
2024-10-22 22:31:13.264642: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73273e00 of size 256 next 543
2024-10-22 22:31:13.264646: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73273f00 of size 256 next 575
2024-10-22 22:31:13.264650: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73274000 of size 256 next 577
2024-10-22 22:31:13.264655: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73274100 of size 256 next 375
2024-10-22 22:31:13.264659: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73274200 of size 256 next 516
2024-10-22 22:31:13.264663: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73274300 of size 256 next 552
2024-10-22 22:31:13.264667: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73274400 of size 256 next 557
2024-10-22 22:31:13.264671: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73274500 of size 256 next 554
2024-10-22 22:31:13.264679: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73274600 of size 256 next 465
2024-10-22 22:31:13.264683: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73274700 of size 256 next 539
2024-10-22 22:31:13.264688: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73274800 of size 256 next 569
2024-10-22 22:31:13.264692: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73274900 of size 256 next 576
2024-10-22 22:31:13.264696: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73274a00 of size 256 next 586
2024-10-22 22:31:13.264700: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73274b00 of size 256 next 598
2024-10-22 22:31:13.264705: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73274c00 of size 256 next 517
2024-10-22 22:31:13.264709: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73274d00 of size 256 next 602
2024-10-22 22:31:13.264713: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73274e00 of size 256 next 699
2024-10-22 22:31:13.264718: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73274f00 of size 256 next 329
2024-10-22 22:31:13.264722: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73275000 of size 256 next 546
2024-10-22 22:31:13.264727: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73275100 of size 256 next 551
2024-10-22 22:31:13.264731: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73275200 of size 256 next 632
2024-10-22 22:31:13.264735: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73275300 of size 256 next 565
2024-10-22 22:31:13.264740: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73275400 of size 256 next 495
2024-10-22 22:31:13.264744: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73275500 of size 256 next 643
2024-10-22 22:31:13.264749: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73275600 of size 256 next 571
2024-10-22 22:31:13.264753: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73275700 of size 256 next 637
2024-10-22 22:31:13.264757: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73275800 of size 256 next 595
2024-10-22 22:31:13.264761: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73275900 of size 256 next 588
2024-10-22 22:31:13.264766: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73275a00 of size 256 next 284
2024-10-22 22:31:13.264770: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73275b00 of size 256 next 625
2024-10-22 22:31:13.264774: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73275c00 of size 256 next 512
2024-10-22 22:31:13.264778: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73275d00 of size 256 next 590
2024-10-22 22:31:13.264783: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73275e00 of size 256 next 435
2024-10-22 22:31:13.264787: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73275f00 of size 256 next 559
2024-10-22 22:31:13.264792: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73276000 of size 256 next 570
2024-10-22 22:31:13.264796: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73276100 of size 256 next 550
2024-10-22 22:31:13.264801: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73276200 of size 256 next 496
2024-10-22 22:31:13.264805: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73276300 of size 256 next 537
2024-10-22 22:31:13.264809: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73276400 of size 256 next 536
2024-10-22 22:31:13.264813: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73276500 of size 256 next 470
2024-10-22 22:31:13.264820: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73276600 of size 256 next 553
2024-10-22 22:31:13.264825: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73276700 of size 256 next 451
2024-10-22 22:31:13.264829: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73276800 of size 256 next 241
2024-10-22 22:31:13.264833: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73276900 of size 256 next 374
2024-10-22 22:31:13.264838: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73276a00 of size 256 next 526
2024-10-22 22:31:13.264842: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73276b00 of size 256 next 527
2024-10-22 22:31:13.264846: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73276c00 of size 256 next 563
2024-10-22 22:31:13.264850: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73276d00 of size 256 next 506
2024-10-22 22:31:13.264855: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73276e00 of size 256 next 804
2024-10-22 22:31:13.264859: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73276f00 of size 256 next 813
2024-10-22 22:31:13.264863: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73277000 of size 256 next 655
2024-10-22 22:31:13.264867: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73277100 of size 256 next 610
2024-10-22 22:31:13.264872: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73277200 of size 256 next 646
2024-10-22 22:31:13.264876: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73277300 of size 256 next 440
2024-10-22 22:31:13.264880: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73277400 of size 256 next 389
2024-10-22 22:31:13.264885: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73277500 of size 256 next 255
2024-10-22 22:31:13.264890: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73277600 of size 256 next 381
2024-10-22 22:31:13.264894: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73277700 of size 256 next 520
2024-10-22 22:31:13.264899: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73277800 of size 256 next 514
2024-10-22 22:31:13.264905: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73277900 of size 256 next 525
2024-10-22 22:31:13.264909: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73277a00 of size 256 next 579
2024-10-22 22:31:13.264914: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73277b00 of size 256 next 686
2024-10-22 22:31:13.264918: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73277c00 of size 256 next 282
2024-10-22 22:31:13.264922: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73277d00 of size 256 next 382
2024-10-22 22:31:13.264926: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73277e00 of size 256 next 648
2024-10-22 22:31:13.264930: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73277f00 of size 256 next 656
2024-10-22 22:31:13.264935: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73278000 of size 256 next 523
2024-10-22 22:31:13.264939: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73278100 of size 256 next 412
2024-10-22 22:31:13.264943: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73278200 of size 256 next 645
2024-10-22 22:31:13.264947: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73278300 of size 256 next 626
2024-10-22 22:31:13.264951: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73278400 of size 256 next 692
2024-10-22 22:31:13.264959: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73278500 of size 256 next 647
2024-10-22 22:31:13.264964: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73278600 of size 256 next 498
2024-10-22 22:31:13.264968: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73278700 of size 256 next 379
2024-10-22 22:31:13.264973: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73278800 of size 256 next 642
2024-10-22 22:31:13.264977: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73278900 of size 256 next 715
2024-10-22 22:31:13.264981: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73278a00 of size 256 next 472
2024-10-22 22:31:13.264985: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73278b00 of size 256 next 573
2024-10-22 22:31:13.264990: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73278c00 of size 256 next 497
2024-10-22 22:31:13.264994: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73278d00 of size 256 next 769
2024-10-22 22:31:13.264998: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73278e00 of size 256 next 701
2024-10-22 22:31:13.265002: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73278f00 of size 256 next 680
2024-10-22 22:31:13.265007: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73279000 of size 256 next 700
2024-10-22 22:31:13.265011: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73279100 of size 256 next 753
2024-10-22 22:31:13.265015: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73279200 of size 256 next 568
2024-10-22 22:31:13.265019: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73279300 of size 256 next 633
2024-10-22 22:31:13.265024: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73279400 of size 256 next 634
2024-10-22 22:31:13.265028: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73279500 of size 256 next 515
2024-10-22 22:31:13.265032: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73279600 of size 256 next 591
2024-10-22 22:31:13.265036: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73279700 of size 256 next 214
2024-10-22 22:31:13.265041: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73279800 of size 256 next 619
2024-10-22 22:31:13.265045: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73279900 of size 256 next 665
2024-10-22 22:31:13.265049: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73279a00 of size 256 next 547
2024-10-22 22:31:13.265054: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73279b00 of size 256 next 702
2024-10-22 22:31:13.265058: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73279c00 of size 256 next 696
2024-10-22 22:31:13.265063: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73279d00 of size 256 next 834
2024-10-22 22:31:13.265067: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73279e00 of size 256 next 599
2024-10-22 22:31:13.265071: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73279f00 of size 256 next 587
2024-10-22 22:31:13.265076: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327a000 of size 256 next 549
2024-10-22 22:31:13.265080: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327a100 of size 256 next 439
2024-10-22 22:31:13.265085: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327a200 of size 256 next 54
2024-10-22 22:31:13.265089: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327a300 of size 256 next 1987
2024-10-22 22:31:13.265093: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327a400 of size 256 next 2040
2024-10-22 22:31:13.265100: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327a500 of size 256 next 2043
2024-10-22 22:31:13.265105: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327a600 of size 256 next 1976
2024-10-22 22:31:13.265110: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327a700 of size 256 next 2055
2024-10-22 22:31:13.265114: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327a800 of size 256 next 2045
2024-10-22 22:31:13.265118: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327a900 of size 256 next 2046
2024-10-22 22:31:13.265123: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327aa00 of size 256 next 2047
2024-10-22 22:31:13.265128: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327ab00 of size 256 next 1977
2024-10-22 22:31:13.265132: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327ac00 of size 256 next 2031
2024-10-22 22:31:13.265136: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327ad00 of size 256 next 2054
2024-10-22 22:31:13.265143: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327ae00 of size 256 next 2036
2024-10-22 22:31:13.265147: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327af00 of size 256 next 1935
2024-10-22 22:31:13.265151: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327b000 of size 256 next 2000
2024-10-22 22:31:13.265155: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327b100 of size 256 next 2032
2024-10-22 22:31:13.265160: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327b200 of size 256 next 1848
2024-10-22 22:31:13.265165: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327b300 of size 256 next 1998
2024-10-22 22:31:13.265171: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327b400 of size 256 next 2029
2024-10-22 22:31:13.265175: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327b500 of size 256 next 1930
2024-10-22 22:31:13.265179: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327b600 of size 256 next 2035
2024-10-22 22:31:13.265184: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327b700 of size 256 next 2168
2024-10-22 22:31:13.265188: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327b800 of size 256 next 2099
2024-10-22 22:31:13.265192: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327b900 of size 256 next 2246
2024-10-22 22:31:13.265196: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327ba00 of size 256 next 2258
2024-10-22 22:31:13.265201: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327bb00 of size 256 next 2277
2024-10-22 22:31:13.265206: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327bc00 of size 256 next 2286
2024-10-22 22:31:13.265210: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327bd00 of size 256 next 2150
2024-10-22 22:31:13.265214: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327be00 of size 256 next 2198
2024-10-22 22:31:13.265218: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327bf00 of size 256 next 2201
2024-10-22 22:31:13.265222: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327c000 of size 256 next 2138
2024-10-22 22:31:13.265226: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327c100 of size 256 next 2178
2024-10-22 22:31:13.265231: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327c200 of size 256 next 2156
2024-10-22 22:31:13.265235: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327c300 of size 256 next 2165
2024-10-22 22:31:13.265240: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327c400 of size 256 next 2159
2024-10-22 22:31:13.265247: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327c500 of size 256 next 2164
2024-10-22 22:31:13.265252: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327c600 of size 256 next 2169
2024-10-22 22:31:13.265256: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327c700 of size 256 next 2153
2024-10-22 22:31:13.265260: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327c800 of size 256 next 2221
2024-10-22 22:31:13.265264: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327c900 of size 256 next 2176
2024-10-22 22:31:13.265269: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327ca00 of size 256 next 2222
2024-10-22 22:31:13.265273: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327cb00 of size 256 next 2181
2024-10-22 22:31:13.265277: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327cc00 of size 256 next 2135
2024-10-22 22:31:13.265281: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327cd00 of size 256 next 2079
2024-10-22 22:31:13.265286: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327ce00 of size 256 next 2162
2024-10-22 22:31:13.265290: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327cf00 of size 256 next 2186
2024-10-22 22:31:13.265294: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327d000 of size 256 next 2179
2024-10-22 22:31:13.265299: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327d100 of size 256 next 2327
2024-10-22 22:31:13.265303: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327d200 of size 256 next 2252
2024-10-22 22:31:13.265308: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327d300 of size 256 next 2300
2024-10-22 22:31:13.265313: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327d400 of size 256 next 2360
2024-10-22 22:31:13.265318: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327d500 of size 256 next 2267
2024-10-22 22:31:13.265322: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327d600 of size 256 next 2242
2024-10-22 22:31:13.265326: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327d700 of size 256 next 2219
2024-10-22 22:31:13.265330: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327d800 of size 256 next 2208
2024-10-22 22:31:13.265335: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327d900 of size 256 next 2143
2024-10-22 22:31:13.265339: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327da00 of size 256 next 2207
2024-10-22 22:31:13.265343: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327db00 of size 256 next 2119
2024-10-22 22:31:13.265347: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327dc00 of size 256 next 2192
2024-10-22 22:31:13.265352: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327dd00 of size 256 next 2174
2024-10-22 22:31:13.265356: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327de00 of size 256 next 2182
2024-10-22 22:31:13.265361: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327df00 of size 256 next 2167
2024-10-22 22:31:13.265365: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327e000 of size 256 next 2163
2024-10-22 22:31:13.265369: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327e100 of size 256 next 2027
2024-10-22 22:31:13.265373: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327e200 of size 256 next 2151
2024-10-22 22:31:13.265378: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327e300 of size 256 next 2212
2024-10-22 22:31:13.265386: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327e400 of size 256 next 2175
2024-10-22 22:31:13.265390: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327e500 of size 256 next 2196
2024-10-22 22:31:13.265395: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327e600 of size 256 next 2194
2024-10-22 22:31:13.265399: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327e700 of size 256 next 2253
2024-10-22 22:31:13.265403: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327e800 of size 256 next 2232
2024-10-22 22:31:13.265408: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327e900 of size 256 next 2259
2024-10-22 22:31:13.265413: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327ea00 of size 256 next 2239
2024-10-22 22:31:13.265417: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327eb00 of size 256 next 2195
2024-10-22 22:31:13.265423: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327ec00 of size 256 next 2248
2024-10-22 22:31:13.265428: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327ed00 of size 256 next 2204
2024-10-22 22:31:13.265432: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327ee00 of size 256 next 2225
2024-10-22 22:31:13.265437: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327ef00 of size 256 next 2236
2024-10-22 22:31:13.265441: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327f000 of size 256 next 2200
2024-10-22 22:31:13.265445: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327f100 of size 256 next 2230
2024-10-22 22:31:13.265449: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327f200 of size 256 next 2206
2024-10-22 22:31:13.265454: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327f300 of size 256 next 2209
2024-10-22 22:31:13.265458: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327f400 of size 256 next 1984
2024-10-22 22:31:13.265462: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327f500 of size 256 next 2220
2024-10-22 22:31:13.265466: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327f600 of size 256 next 2190
2024-10-22 22:31:13.265471: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327f700 of size 256 next 2237
2024-10-22 22:31:13.265475: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327f800 of size 256 next 2218
2024-10-22 22:31:13.265479: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327f900 of size 256 next 2235
2024-10-22 22:31:13.265484: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327fa00 of size 256 next 2241
2024-10-22 22:31:13.265489: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327fb00 of size 256 next 2227
2024-10-22 22:31:13.265493: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327fc00 of size 256 next 2115
2024-10-22 22:31:13.265497: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327fd00 of size 256 next 2262
2024-10-22 22:31:13.265502: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327fe00 of size 256 next 2185
2024-10-22 22:31:13.265506: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7327ff00 of size 256 next 2224
2024-10-22 22:31:13.265511: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73280000 of size 256 next 2245
2024-10-22 22:31:13.265515: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73280100 of size 256 next 2170
2024-10-22 22:31:13.265519: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73280200 of size 256 next 2217
2024-10-22 22:31:13.265523: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73280300 of size 256 next 2233
2024-10-22 22:31:13.265533: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73280400 of size 256 next 2215
2024-10-22 22:31:13.265537: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73280500 of size 256 next 2229
2024-10-22 22:31:13.265542: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73280600 of size 256 next 1942
2024-10-22 22:31:13.265546: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73280700 of size 256 next 1978
2024-10-22 22:31:13.265550: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73280800 of size 256 next 2060
2024-10-22 22:31:13.265555: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73280900 of size 256 next 2053
2024-10-22 22:31:13.265559: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73280a00 of size 256 next 2039
2024-10-22 22:31:13.265563: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73280b00 of size 256 next 2161
2024-10-22 22:31:13.265567: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73280c00 of size 256 next 2148
2024-10-22 22:31:13.265572: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73280d00 of size 256 next 2097
2024-10-22 22:31:13.265577: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73280e00 of size 256 next 2126
2024-10-22 22:31:13.265581: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73280f00 of size 256 next 2146
2024-10-22 22:31:13.265585: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73281000 of size 256 next 2114
2024-10-22 22:31:13.265589: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73281100 of size 256 next 2088
2024-10-22 22:31:13.265594: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73281200 of size 256 next 2071
2024-10-22 22:31:13.265599: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73281300 of size 256 next 2014
2024-10-22 22:31:13.265603: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73281400 of size 256 next 2083
2024-10-22 22:31:13.265607: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73281500 of size 256 next 2093
2024-10-22 22:31:13.265613: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73281600 of size 256 next 2086
2024-10-22 22:31:13.265617: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73281700 of size 256 next 2103
2024-10-22 22:31:13.265621: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73281800 of size 256 next 2109
2024-10-22 22:31:13.265625: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73281900 of size 256 next 2133
2024-10-22 22:31:13.265630: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73281a00 of size 256 next 2180
2024-10-22 22:31:13.265634: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73281b00 of size 256 next 2058
2024-10-22 22:31:13.265639: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73281c00 of size 256 next 2132
2024-10-22 22:31:13.265644: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73281d00 of size 256 next 2050
2024-10-22 22:31:13.265648: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73281e00 of size 256 next 2020
2024-10-22 22:31:13.265652: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73281f00 of size 256 next 2011
2024-10-22 22:31:13.265657: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73282000 of size 256 next 1961
2024-10-22 22:31:13.265661: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73282100 of size 256 next 2120
2024-10-22 22:31:13.265666: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73282200 of size 256 next 2076
2024-10-22 22:31:13.265674: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73282300 of size 256 next 2064
2024-10-22 22:31:13.265678: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73282400 of size 256 next 2033
2024-10-22 22:31:13.265682: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73282500 of size 256 next 2042
2024-10-22 22:31:13.265687: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73282600 of size 256 next 59
2024-10-22 22:31:13.265691: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73282700 of size 512 next 62
2024-10-22 22:31:13.265696: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73282900 of size 512 next 63
2024-10-22 22:31:13.265700: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73282b00 of size 256 next 83
2024-10-22 22:31:13.265704: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73282c00 of size 256 next 84
2024-10-22 22:31:13.265708: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73282d00 of size 256 next 85
2024-10-22 22:31:13.265713: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73282e00 of size 256 next 66
2024-10-22 22:31:13.265717: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73282f00 of size 256 next 1680
2024-10-22 22:31:13.265721: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73283000 of size 256 next 1653
2024-10-22 22:31:13.265726: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73283100 of size 256 next 1627
2024-10-22 22:31:13.265730: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73283200 of size 256 next 1652
2024-10-22 22:31:13.265734: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73283300 of size 256 next 1692
2024-10-22 22:31:13.265738: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73283400 of size 256 next 1542
2024-10-22 22:31:13.265743: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73283500 of size 256 next 1665
2024-10-22 22:31:13.265747: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73283600 of size 256 next 72
2024-10-22 22:31:13.265751: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73283700 of size 256 next 109
2024-10-22 22:31:13.265756: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73283800 of size 256 next 73
2024-10-22 22:31:13.265760: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73283900 of size 256 next 1699
2024-10-22 22:31:13.265764: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73283a00 of size 256 next 140
2024-10-22 22:31:13.265769: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73283b00 of size 256 next 146
2024-10-22 22:31:13.265773: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73283c00 of size 256 next 75
2024-10-22 22:31:13.265778: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73283d00 of size 256 next 223
2024-10-22 22:31:13.265783: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73283e00 of size 256 next 76
2024-10-22 22:31:13.265787: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73283f00 of size 256 next 1622
2024-10-22 22:31:13.265791: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73284000 of size 256 next 134
2024-10-22 22:31:13.265796: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73284100 of size 256 next 1689
2024-10-22 22:31:13.265800: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73284200 of size 256 next 1685
2024-10-22 22:31:13.265805: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73284300 of size 256 next 1683
2024-10-22 22:31:13.265809: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73284400 of size 256 next 1278
2024-10-22 22:31:13.265817: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73284500 of size 256 next 28
2024-10-22 22:31:13.265822: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73284600 of size 256 next 81
2024-10-22 22:31:13.265826: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73284700 of size 256 next 93
2024-10-22 22:31:13.265830: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73284800 of size 256 next 197
2024-10-22 22:31:13.265835: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73284900 of size 256 next 264
2024-10-22 22:31:13.265839: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73284a00 of size 256 next 87
2024-10-22 22:31:13.265844: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73284b00 of size 256 next 184
2024-10-22 22:31:13.265849: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73284c00 of size 256 next 286
2024-10-22 22:31:13.265853: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73284d00 of size 256 next 276
2024-10-22 22:31:13.265858: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73284e00 of size 256 next 290
2024-10-22 22:31:13.265862: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73284f00 of size 256 next 229
2024-10-22 22:31:13.265866: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73285000 of size 256 next 298
2024-10-22 22:31:13.265871: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73285100 of size 256 next 326
2024-10-22 22:31:13.265875: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73285200 of size 256 next 246
2024-10-22 22:31:13.265879: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73285300 of size 256 next 110
2024-10-22 22:31:13.265883: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73285400 of size 256 next 115
2024-10-22 22:31:13.265888: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73285500 of size 256 next 271
2024-10-22 22:31:13.265892: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73285600 of size 256 next 118
2024-10-22 22:31:13.265897: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73285700 of size 256 next 120
2024-10-22 22:31:13.265901: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73285800 of size 256 next 1722
2024-10-22 22:31:13.265906: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73285900 of size 256 next 1725
2024-10-22 22:31:13.265912: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73285a00 of size 256 next 1603
2024-10-22 22:31:13.265917: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73285b00 of size 256 next 127
2024-10-22 22:31:13.265921: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73285c00 of size 256 next 285
2024-10-22 22:31:13.265925: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73285d00 of size 256 next 314
2024-10-22 22:31:13.265930: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73285e00 of size 256 next 198
2024-10-22 22:31:13.265934: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73285f00 of size 256 next 131
2024-10-22 22:31:13.265938: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73286000 of size 256 next 1590
2024-10-22 22:31:13.265943: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73286100 of size 256 next 95
2024-10-22 22:31:13.265947: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73286200 of size 256 next 1242
2024-10-22 22:31:13.265951: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73286300 of size 256 next 1602
2024-10-22 22:31:13.265955: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73286400 of size 256 next 1664
2024-10-22 22:31:13.265963: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73286500 of size 256 next 1737
2024-10-22 22:31:13.265967: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73286600 of size 256 next 1770
2024-10-22 22:31:13.265971: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73286700 of size 256 next 1706
2024-10-22 22:31:13.265976: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73286800 of size 256 next 1621
2024-10-22 22:31:13.265980: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73286900 of size 256 next 150
2024-10-22 22:31:13.265984: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73286a00 of size 256 next 152
2024-10-22 22:31:13.265989: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73286b00 of size 256 next 153
2024-10-22 22:31:13.265994: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73286c00 of size 256 next 251
2024-10-22 22:31:13.265999: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73286d00 of size 256 next 155
2024-10-22 22:31:13.266003: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73286e00 of size 256 next 156
2024-10-22 22:31:13.266007: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73286f00 of size 256 next 157
2024-10-22 22:31:13.266013: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73287000 of size 256 next 158
2024-10-22 22:31:13.266017: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73287100 of size 256 next 232
2024-10-22 22:31:13.266021: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73287200 of size 256 next 97
2024-10-22 22:31:13.266025: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73287300 of size 13312 next 60
2024-10-22 22:31:13.266030: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f2b7328a700 of size 83968 next 2497
2024-10-22 22:31:13.266034: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7329ef00 of size 256 next 2438
2024-10-22 22:31:13.266039: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7329f000 of size 256 next 2443
2024-10-22 22:31:13.266045: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7329f100 of size 256 next 2478
2024-10-22 22:31:13.266050: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7329f200 of size 256 next 2483
2024-10-22 22:31:13.266054: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7329f300 of size 256 next 2413
2024-10-22 22:31:13.266058: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7329f400 of size 256 next 2471
2024-10-22 22:31:13.266062: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7329f500 of size 256 next 2498
2024-10-22 22:31:13.266066: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7329f600 of size 256 next 2562
2024-10-22 22:31:13.266071: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7329f700 of size 256 next 2606
2024-10-22 22:31:13.266075: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7329f800 of size 256 next 2601
2024-10-22 22:31:13.266079: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7329f900 of size 256 next 2566
2024-10-22 22:31:13.266084: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7329fa00 of size 256 next 2499
2024-10-22 22:31:13.266088: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7329fb00 of size 256 next 2476
2024-10-22 22:31:13.266092: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7329fc00 of size 256 next 2509
2024-10-22 22:31:13.266097: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7329fd00 of size 256 next 2463
2024-10-22 22:31:13.266105: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7329fe00 of size 256 next 2505
2024-10-22 22:31:13.266110: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7329ff00 of size 256 next 2440
2024-10-22 22:31:13.266114: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a0000 of size 256 next 2359
2024-10-22 22:31:13.266118: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a0100 of size 256 next 2468
2024-10-22 22:31:13.266123: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a0200 of size 256 next 2437
2024-10-22 22:31:13.266127: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a0300 of size 256 next 2510
2024-10-22 22:31:13.266132: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a0400 of size 256 next 2452
2024-10-22 22:31:13.266136: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a0500 of size 256 next 2496
2024-10-22 22:31:13.266141: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a0600 of size 256 next 2504
2024-10-22 22:31:13.266145: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a0700 of size 256 next 2500
2024-10-22 22:31:13.266150: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a0800 of size 256 next 2501
2024-10-22 22:31:13.266154: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a0900 of size 256 next 2469
2024-10-22 22:31:13.266158: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a0a00 of size 256 next 2513
2024-10-22 22:31:13.266163: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a0b00 of size 256 next 2306
2024-10-22 22:31:13.266168: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a0c00 of size 256 next 2480
2024-10-22 22:31:13.266172: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a0d00 of size 256 next 2406
2024-10-22 22:31:13.266176: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a0e00 of size 256 next 2450
2024-10-22 22:31:13.266181: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a0f00 of size 256 next 2317
2024-10-22 22:31:13.266185: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a1000 of size 256 next 2529
2024-10-22 22:31:13.266189: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a1100 of size 256 next 2490
2024-10-22 22:31:13.266194: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a1200 of size 256 next 2489
2024-10-22 22:31:13.266198: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a1300 of size 256 next 2502
2024-10-22 22:31:13.266202: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a1400 of size 256 next 2492
2024-10-22 22:31:13.266207: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a1500 of size 256 next 2491
2024-10-22 22:31:13.266211: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a1600 of size 256 next 2435
2024-10-22 22:31:13.266215: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a1700 of size 256 next 2391
2024-10-22 22:31:13.266220: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a1800 of size 256 next 2551
2024-10-22 22:31:13.266224: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a1900 of size 256 next 2531
2024-10-22 22:31:13.266230: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a1a00 of size 256 next 2550
2024-10-22 22:31:13.266234: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a1b00 of size 256 next 2720
2024-10-22 22:31:13.266239: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a1c00 of size 256 next 2750
2024-10-22 22:31:13.266243: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a1d00 of size 256 next 2739
2024-10-22 22:31:13.266252: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a1e00 of size 256 next 2723
2024-10-22 22:31:13.266257: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a1f00 of size 256 next 2472
2024-10-22 22:31:13.266261: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a2000 of size 256 next 2521
2024-10-22 22:31:13.266265: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a2100 of size 256 next 2530
2024-10-22 22:31:13.266269: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a2200 of size 256 next 2522
2024-10-22 22:31:13.266273: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a2300 of size 256 next 2457
2024-10-22 22:31:13.266278: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a2400 of size 256 next 2430
2024-10-22 22:31:13.266282: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a2500 of size 256 next 2481
2024-10-22 22:31:13.266286: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a2600 of size 256 next 65
2024-10-22 22:31:13.266291: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732a2700 of size 32768 next 64
2024-10-22 22:31:13.266295: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f2b732aa700 of size 65536 next 67
2024-10-22 22:31:13.266299: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732ba700 of size 256 next 279
2024-10-22 22:31:13.266303: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732ba800 of size 256 next 105
2024-10-22 22:31:13.266307: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732ba900 of size 256 next 51
2024-10-22 22:31:13.266312: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732baa00 of size 256 next 201
2024-10-22 22:31:13.266316: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bab00 of size 256 next 199
2024-10-22 22:31:13.266320: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bac00 of size 256 next 192
2024-10-22 22:31:13.266325: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bad00 of size 256 next 130
2024-10-22 22:31:13.266329: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bae00 of size 256 next 266
2024-10-22 22:31:13.266333: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732baf00 of size 256 next 335
2024-10-22 22:31:13.266338: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bb000 of size 256 next 182
2024-10-22 22:31:13.266342: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bb100 of size 256 next 324
2024-10-22 22:31:13.266347: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bb200 of size 256 next 302
2024-10-22 22:31:13.266352: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bb300 of size 256 next 313
2024-10-22 22:31:13.266356: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bb400 of size 256 next 234
2024-10-22 22:31:13.266360: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bb500 of size 256 next 296
2024-10-22 22:31:13.266364: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bb600 of size 256 next 308
2024-10-22 22:31:13.266368: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bb700 of size 256 next 332
2024-10-22 22:31:13.266373: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bb800 of size 256 next 300
2024-10-22 22:31:13.266377: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bb900 of size 256 next 200
2024-10-22 22:31:13.266382: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bba00 of size 256 next 187
2024-10-22 22:31:13.266390: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bbb00 of size 256 next 202
2024-10-22 22:31:13.266394: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bbc00 of size 256 next 204
2024-10-22 22:31:13.266399: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bbd00 of size 256 next 112
2024-10-22 22:31:13.266403: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bbe00 of size 256 next 317
2024-10-22 22:31:13.266407: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bbf00 of size 256 next 250
2024-10-22 22:31:13.266411: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bc000 of size 256 next 338
2024-10-22 22:31:13.266416: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bc100 of size 256 next 247
2024-10-22 22:31:13.266420: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bc200 of size 256 next 267
2024-10-22 22:31:13.266424: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bc300 of size 256 next 306
2024-10-22 22:31:13.266428: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bc400 of size 256 next 305
2024-10-22 22:31:13.266432: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bc500 of size 256 next 318
2024-10-22 22:31:13.266436: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bc600 of size 256 next 378
2024-10-22 22:31:13.266440: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bc700 of size 256 next 366
2024-10-22 22:31:13.266444: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bc800 of size 256 next 262
2024-10-22 22:31:13.266449: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bc900 of size 256 next 217
2024-10-22 22:31:13.266453: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bca00 of size 256 next 240
2024-10-22 22:31:13.266457: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bcb00 of size 256 next 291
2024-10-22 22:31:13.266461: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bcc00 of size 256 next 228
2024-10-22 22:31:13.266466: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bcd00 of size 256 next 181
2024-10-22 22:31:13.266470: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bce00 of size 256 next 230
2024-10-22 22:31:13.266474: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bcf00 of size 256 next 310
2024-10-22 22:31:13.266478: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bd000 of size 256 next 339
2024-10-22 22:31:13.266483: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bd100 of size 256 next 260
2024-10-22 22:31:13.266487: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bd200 of size 256 next 351
2024-10-22 22:31:13.266491: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bd300 of size 256 next 365
2024-10-22 22:31:13.266496: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bd400 of size 256 next 17
2024-10-22 22:31:13.266500: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bd500 of size 256 next 245
2024-10-22 22:31:13.266504: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bd600 of size 256 next 288
2024-10-22 22:31:13.266509: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bd700 of size 256 next 345
2024-10-22 22:31:13.266513: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bd800 of size 256 next 376
2024-10-22 22:31:13.266520: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bd900 of size 256 next 101
2024-10-22 22:31:13.266524: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bda00 of size 256 next 275
2024-10-22 22:31:13.266530: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bdb00 of size 256 next 237
2024-10-22 22:31:13.266535: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bdc00 of size 256 next 277
2024-10-22 22:31:13.266540: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bdd00 of size 256 next 238
2024-10-22 22:31:13.266545: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bde00 of size 256 next 342
2024-10-22 22:31:13.266549: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bdf00 of size 256 next 99
2024-10-22 22:31:13.266553: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732be000 of size 256 next 173
2024-10-22 22:31:13.266557: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732be100 of size 256 next 337
2024-10-22 22:31:13.266561: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732be200 of size 256 next 221
2024-10-22 22:31:13.266565: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732be300 of size 256 next 307
2024-10-22 22:31:13.266570: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732be400 of size 256 next 248
2024-10-22 22:31:13.266574: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732be500 of size 256 next 216
2024-10-22 22:31:13.266579: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732be600 of size 256 next 70
2024-10-22 22:31:13.266583: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732be700 of size 256 next 1673
2024-10-22 22:31:13.266587: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732be800 of size 256 next 1797
2024-10-22 22:31:13.266592: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732be900 of size 256 next 1820
2024-10-22 22:31:13.266596: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bea00 of size 256 next 1553
2024-10-22 22:31:13.266600: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732beb00 of size 256 next 1626
2024-10-22 22:31:13.266604: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bec00 of size 256 next 1668
2024-10-22 22:31:13.266609: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bed00 of size 256 next 1709
2024-10-22 22:31:13.266613: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bee00 of size 256 next 1517
2024-10-22 22:31:13.266617: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bef00 of size 256 next 1860
2024-10-22 22:31:13.266621: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bf000 of size 256 next 1846
2024-10-22 22:31:13.266626: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bf100 of size 256 next 1865
2024-10-22 22:31:13.266630: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bf200 of size 256 next 1850
2024-10-22 22:31:13.266634: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bf300 of size 256 next 1717
2024-10-22 22:31:13.266638: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bf400 of size 256 next 1684
2024-10-22 22:31:13.266643: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bf500 of size 256 next 1719
2024-10-22 22:31:13.266647: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bf600 of size 256 next 1648
2024-10-22 22:31:13.266651: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bf700 of size 256 next 1632
2024-10-22 22:31:13.266655: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bf800 of size 256 next 1571
2024-10-22 22:31:13.266659: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bf900 of size 256 next 1607
2024-10-22 22:31:13.266663: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bfa00 of size 256 next 1712
2024-10-22 22:31:13.266673: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bfb00 of size 256 next 1693
2024-10-22 22:31:13.266677: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bfc00 of size 256 next 1724
2024-10-22 22:31:13.266682: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bfd00 of size 256 next 1630
2024-10-22 22:31:13.266686: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bfe00 of size 256 next 1723
2024-10-22 22:31:13.266690: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732bff00 of size 256 next 1730
2024-10-22 22:31:13.266694: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c0000 of size 256 next 1718
2024-10-22 22:31:13.266699: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c0100 of size 256 next 1734
2024-10-22 22:31:13.266703: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c0200 of size 256 next 1676
2024-10-22 22:31:13.266707: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c0300 of size 256 next 1713
2024-10-22 22:31:13.266711: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c0400 of size 256 next 1538
2024-10-22 22:31:13.266716: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c0500 of size 256 next 1691
2024-10-22 22:31:13.266720: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c0600 of size 256 next 1750
2024-10-22 22:31:13.266724: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c0700 of size 256 next 1735
2024-10-22 22:31:13.266729: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c0800 of size 256 next 1721
2024-10-22 22:31:13.266733: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c0900 of size 256 next 1743
2024-10-22 22:31:13.266737: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c0a00 of size 256 next 1758
2024-10-22 22:31:13.266741: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c0b00 of size 256 next 1662
2024-10-22 22:31:13.266746: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c0c00 of size 256 next 124
2024-10-22 22:31:13.266750: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c0d00 of size 256 next 1705
2024-10-22 22:31:13.266754: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c0e00 of size 256 next 1733
2024-10-22 22:31:13.266758: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c0f00 of size 256 next 1698
2024-10-22 22:31:13.266763: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c1000 of size 256 next 1756
2024-10-22 22:31:13.266767: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c1100 of size 256 next 1732
2024-10-22 22:31:13.266771: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c1200 of size 256 next 1744
2024-10-22 22:31:13.266775: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c1300 of size 256 next 147
2024-10-22 22:31:13.266780: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c1400 of size 256 next 71
2024-10-22 22:31:13.266784: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c1500 of size 256 next 38
2024-10-22 22:31:13.266789: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c1600 of size 256 next 103
2024-10-22 22:31:13.266793: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c1700 of size 256 next 50
2024-10-22 22:31:13.266799: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c1800 of size 256 next 52
2024-10-22 22:31:13.266804: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c1900 of size 256 next 100
2024-10-22 22:31:13.266813: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c1a00 of size 256 next 160
2024-10-22 22:31:13.266817: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c1b00 of size 256 next 126
2024-10-22 22:31:13.266822: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c1c00 of size 256 next 129
2024-10-22 22:31:13.266827: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c1d00 of size 256 next 162
2024-10-22 22:31:13.266831: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c1e00 of size 256 next 161
2024-10-22 22:31:13.266835: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c1f00 of size 256 next 219
2024-10-22 22:31:13.266839: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c2000 of size 256 next 163
2024-10-22 22:31:13.266844: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c2100 of size 256 next 164
2024-10-22 22:31:13.266848: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c2200 of size 256 next 53
2024-10-22 22:31:13.266852: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c2300 of size 256 next 166
2024-10-22 22:31:13.266856: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c2400 of size 256 next 167
2024-10-22 22:31:13.266861: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c2500 of size 256 next 1841
2024-10-22 22:31:13.266865: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c2600 of size 256 next 1796
2024-10-22 22:31:13.266869: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c2700 of size 256 next 1788
2024-10-22 22:31:13.266874: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c2800 of size 256 next 1776
2024-10-22 22:31:13.266880: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c2900 of size 256 next 1779
2024-10-22 22:31:13.266884: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c2a00 of size 256 next 1672
2024-10-22 22:31:13.266888: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c2b00 of size 256 next 1798
2024-10-22 22:31:13.266893: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c2c00 of size 256 next 1801
2024-10-22 22:31:13.266897: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c2d00 of size 256 next 1810
2024-10-22 22:31:13.266901: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c2e00 of size 256 next 1816
2024-10-22 22:31:13.266905: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c2f00 of size 256 next 1785
2024-10-22 22:31:13.266910: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c3000 of size 256 next 1749
2024-10-22 22:31:13.266914: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c3100 of size 256 next 1847
2024-10-22 22:31:13.266918: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c3200 of size 256 next 1805
2024-10-22 22:31:13.266922: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c3300 of size 256 next 1753
2024-10-22 22:31:13.266927: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c3400 of size 256 next 1794
2024-10-22 22:31:13.266931: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c3500 of size 256 next 1727
2024-10-22 22:31:13.266935: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c3600 of size 256 next 1793
2024-10-22 22:31:13.266939: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c3700 of size 256 next 1852
2024-10-22 22:31:13.266944: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c3800 of size 256 next 1795
2024-10-22 22:31:13.266948: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c3900 of size 256 next 1775
2024-10-22 22:31:13.266954: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c3a00 of size 256 next 1657
2024-10-22 22:31:13.266958: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c3b00 of size 256 next 1759
2024-10-22 22:31:13.266962: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c3c00 of size 256 next 1815
2024-10-22 22:31:13.266966: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c3d00 of size 256 next 1832
2024-10-22 22:31:13.266970: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c3e00 of size 256 next 1821
2024-10-22 22:31:13.266974: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c3f00 of size 256 next 1477
2024-10-22 22:31:13.266978: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c4000 of size 256 next 1748
2024-10-22 22:31:13.266982: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c4100 of size 256 next 1829
2024-10-22 22:31:13.266986: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c4200 of size 256 next 1791
2024-10-22 22:31:13.266990: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c4300 of size 256 next 1826
2024-10-22 22:31:13.266995: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c4400 of size 256 next 1752
2024-10-22 22:31:13.266999: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c4500 of size 256 next 1839
2024-10-22 22:31:13.267003: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c4600 of size 256 next 1781
2024-10-22 22:31:13.267007: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c4700 of size 256 next 1774
2024-10-22 22:31:13.267012: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c4800 of size 256 next 1824
2024-10-22 22:31:13.267016: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c4900 of size 256 next 1827
2024-10-22 22:31:13.267020: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c4a00 of size 256 next 1867
2024-10-22 22:31:13.267024: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c4b00 of size 256 next 1908
2024-10-22 22:31:13.267029: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c4c00 of size 256 next 1886
2024-10-22 22:31:13.267033: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c4d00 of size 256 next 1817
2024-10-22 22:31:13.267037: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c4e00 of size 256 next 1937
2024-10-22 22:31:13.267041: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c4f00 of size 256 next 1840
2024-10-22 22:31:13.267045: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c5000 of size 256 next 1892
2024-10-22 22:31:13.267049: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c5100 of size 256 next 1811
2024-10-22 22:31:13.267053: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c5200 of size 256 next 1889
2024-10-22 22:31:13.267057: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c5300 of size 256 next 1854
2024-10-22 22:31:13.267061: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c5400 of size 256 next 1871
2024-10-22 22:31:13.267065: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c5500 of size 256 next 1831
2024-10-22 22:31:13.267069: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c5600 of size 256 next 1866
2024-10-22 22:31:13.267073: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c5700 of size 256 next 1884
2024-10-22 22:31:13.267077: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c5800 of size 256 next 1890
2024-10-22 22:31:13.267084: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c5900 of size 256 next 1885
2024-10-22 22:31:13.267089: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c5a00 of size 256 next 1782
2024-10-22 22:31:13.267092: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c5b00 of size 256 next 1911
2024-10-22 22:31:13.267097: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c5c00 of size 256 next 1874
2024-10-22 22:31:13.267101: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c5d00 of size 256 next 1879
2024-10-22 22:31:13.267105: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c5e00 of size 256 next 1922
2024-10-22 22:31:13.267109: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c5f00 of size 256 next 1873
2024-10-22 22:31:13.267113: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c6000 of size 256 next 1898
2024-10-22 22:31:13.267117: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c6100 of size 256 next 1870
2024-10-22 22:31:13.267121: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c6200 of size 256 next 1838
2024-10-22 22:31:13.267125: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c6300 of size 256 next 1833
2024-10-22 22:31:13.267129: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c6400 of size 256 next 1835
2024-10-22 22:31:13.267133: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c6500 of size 256 next 1907
2024-10-22 22:31:13.267137: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c6600 of size 256 next 1822
2024-10-22 22:31:13.267141: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c6700 of size 256 next 1877
2024-10-22 22:31:13.267145: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c6800 of size 256 next 1917
2024-10-22 22:31:13.267150: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c6900 of size 256 next 1883
2024-10-22 22:31:13.267154: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c6a00 of size 256 next 1906
2024-10-22 22:31:13.267158: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c6b00 of size 256 next 1896
2024-10-22 22:31:13.267162: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c6c00 of size 256 next 1910
2024-10-22 22:31:13.267166: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c6d00 of size 256 next 1799
2024-10-22 22:31:13.267170: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c6e00 of size 256 next 1891
2024-10-22 22:31:13.267174: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c6f00 of size 256 next 1818
2024-10-22 22:31:13.267178: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c7000 of size 256 next 2017
2024-10-22 22:31:13.267182: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c7100 of size 256 next 1986
2024-10-22 22:31:13.267187: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c7200 of size 256 next 2025
2024-10-22 22:31:13.267190: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c7300 of size 256 next 2030
2024-10-22 22:31:13.267195: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c7400 of size 256 next 1947
2024-10-22 22:31:13.267199: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c7500 of size 256 next 1807
2024-10-22 22:31:13.267203: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c7600 of size 256 next 1918
2024-10-22 22:31:13.267206: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c7700 of size 256 next 1878
2024-10-22 22:31:13.267211: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c7800 of size 256 next 1905
2024-10-22 22:31:13.267217: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c7900 of size 256 next 1809
2024-10-22 22:31:13.267221: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c7a00 of size 256 next 1948
2024-10-22 22:31:13.267225: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c7b00 of size 256 next 1946
2024-10-22 22:31:13.267229: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c7c00 of size 256 next 1945
2024-10-22 22:31:13.267234: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c7d00 of size 256 next 1903
2024-10-22 22:31:13.267238: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c7e00 of size 256 next 1973
2024-10-22 22:31:13.267242: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c7f00 of size 256 next 1915
2024-10-22 22:31:13.267246: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c8000 of size 256 next 1921
2024-10-22 22:31:13.267250: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c8100 of size 256 next 1851
2024-10-22 22:31:13.267254: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c8200 of size 256 next 1857
2024-10-22 22:31:13.267259: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c8300 of size 256 next 1837
2024-10-22 22:31:13.267263: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c8400 of size 256 next 1926
2024-10-22 22:31:13.267267: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c8500 of size 256 next 1901
2024-10-22 22:31:13.267271: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c8600 of size 256 next 1932
2024-10-22 22:31:13.267275: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c8700 of size 256 next 1872
2024-10-22 22:31:13.267279: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c8800 of size 256 next 1931
2024-10-22 22:31:13.267283: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c8900 of size 256 next 1843
2024-10-22 22:31:13.267287: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c8a00 of size 256 next 1856
2024-10-22 22:31:13.267291: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c8b00 of size 256 next 1784
2024-10-22 22:31:13.267295: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c8c00 of size 256 next 1830
2024-10-22 22:31:13.267299: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c8d00 of size 256 next 1875
2024-10-22 22:31:13.267303: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c8e00 of size 256 next 1853
2024-10-22 22:31:13.267307: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c8f00 of size 256 next 1819
2024-10-22 22:31:13.267311: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c9000 of size 256 next 1823
2024-10-22 22:31:13.267315: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c9100 of size 256 next 1868
2024-10-22 22:31:13.267319: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c9200 of size 256 next 1707
2024-10-22 22:31:13.267323: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c9300 of size 256 next 1814
2024-10-22 22:31:13.267327: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c9400 of size 256 next 1863
2024-10-22 22:31:13.267332: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c9500 of size 256 next 1862
2024-10-22 22:31:13.267336: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c9600 of size 256 next 1844
2024-10-22 22:31:13.267339: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c9700 of size 256 next 1802
2024-10-22 22:31:13.267346: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c9800 of size 256 next 1836
2024-10-22 22:31:13.267350: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c9900 of size 256 next 1861
2024-10-22 22:31:13.267354: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c9a00 of size 256 next 1825
2024-10-22 22:31:13.267358: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c9b00 of size 256 next 1887
2024-10-22 22:31:13.267362: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c9c00 of size 256 next 1849
2024-10-22 22:31:13.267366: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c9d00 of size 256 next 1859
2024-10-22 22:31:13.267371: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c9e00 of size 256 next 1812
2024-10-22 22:31:13.267374: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732c9f00 of size 256 next 1882
2024-10-22 22:31:13.267379: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732ca000 of size 256 next 1900
2024-10-22 22:31:13.267383: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732ca100 of size 256 next 1928
2024-10-22 22:31:13.267387: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732ca200 of size 256 next 1902
2024-10-22 22:31:13.267391: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732ca300 of size 256 next 1943
2024-10-22 22:31:13.267395: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732ca400 of size 256 next 1055
2024-10-22 22:31:13.267399: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732ca500 of size 256 next 1174
2024-10-22 22:31:13.267404: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f2b732ca600 of size 54016 next 2324
2024-10-22 22:31:13.267408: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d7900 of size 256 next 2331
2024-10-22 22:31:13.267412: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d7a00 of size 256 next 2144
2024-10-22 22:31:13.267416: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d7b00 of size 256 next 2299
2024-10-22 22:31:13.267420: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d7c00 of size 256 next 2213
2024-10-22 22:31:13.267424: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d7d00 of size 256 next 2334
2024-10-22 22:31:13.267428: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d7e00 of size 256 next 2310
2024-10-22 22:31:13.267432: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d7f00 of size 256 next 2308
2024-10-22 22:31:13.267436: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d8000 of size 256 next 2339
2024-10-22 22:31:13.267440: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d8100 of size 256 next 2351
2024-10-22 22:31:13.267445: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d8200 of size 256 next 2342
2024-10-22 22:31:13.267448: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d8300 of size 256 next 2341
2024-10-22 22:31:13.267453: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d8400 of size 256 next 2348
2024-10-22 22:31:13.267456: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d8500 of size 256 next 2372
2024-10-22 22:31:13.267461: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d8600 of size 256 next 2313
2024-10-22 22:31:13.267465: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d8700 of size 256 next 2335
2024-10-22 22:31:13.267469: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d8800 of size 256 next 2270
2024-10-22 22:31:13.267473: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d8900 of size 256 next 2244
2024-10-22 22:31:13.267480: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d8a00 of size 256 next 2293
2024-10-22 22:31:13.267484: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d8b00 of size 256 next 2318
2024-10-22 22:31:13.267488: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d8c00 of size 256 next 2315
2024-10-22 22:31:13.267492: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d8d00 of size 256 next 2305
2024-10-22 22:31:13.267496: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d8e00 of size 256 next 2272
2024-10-22 22:31:13.267501: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d8f00 of size 256 next 2281
2024-10-22 22:31:13.267505: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d9000 of size 256 next 2347
2024-10-22 22:31:13.267509: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d9100 of size 256 next 2314
2024-10-22 22:31:13.267513: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d9200 of size 256 next 2328
2024-10-22 22:31:13.267517: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d9300 of size 256 next 2332
2024-10-22 22:31:13.267521: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d9400 of size 256 next 2303
2024-10-22 22:31:13.267525: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d9500 of size 256 next 2322
2024-10-22 22:31:13.267529: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d9600 of size 256 next 2367
2024-10-22 22:31:13.267533: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d9700 of size 256 next 2358
2024-10-22 22:31:13.267537: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d9800 of size 256 next 2395
2024-10-22 22:31:13.267541: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d9900 of size 256 next 2321
2024-10-22 22:31:13.267545: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d9a00 of size 256 next 2338
2024-10-22 22:31:13.267550: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d9b00 of size 256 next 2357
2024-10-22 22:31:13.267554: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d9c00 of size 256 next 2363
2024-10-22 22:31:13.267557: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d9d00 of size 256 next 2344
2024-10-22 22:31:13.267562: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d9e00 of size 256 next 2325
2024-10-22 22:31:13.267566: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732d9f00 of size 256 next 2304
2024-10-22 22:31:13.267570: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732da000 of size 256 next 2356
2024-10-22 22:31:13.267574: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732da100 of size 256 next 2336
2024-10-22 22:31:13.267578: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732da200 of size 256 next 2295
2024-10-22 22:31:13.267582: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732da300 of size 256 next 2287
2024-10-22 22:31:13.267588: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732da400 of size 256 next 2316
2024-10-22 22:31:13.267594: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732da500 of size 256 next 2296
2024-10-22 22:31:13.267598: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732da600 of size 256 next 79
2024-10-22 22:31:13.267602: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732da700 of size 256 next 86
2024-10-22 22:31:13.267607: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732da800 of size 256 next 114
2024-10-22 22:31:13.267613: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732da900 of size 256 next 90
2024-10-22 22:31:13.267617: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732daa00 of size 256 next 91
2024-10-22 22:31:13.267622: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dab00 of size 256 next 92
2024-10-22 22:31:13.267626: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dac00 of size 256 next 265
2024-10-22 22:31:13.267630: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dad00 of size 256 next 257
2024-10-22 22:31:13.267634: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dae00 of size 256 next 311
2024-10-22 22:31:13.267638: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732daf00 of size 256 next 303
2024-10-22 22:31:13.267642: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732db000 of size 256 next 356
2024-10-22 22:31:13.267646: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732db100 of size 256 next 186
2024-10-22 22:31:13.267650: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732db200 of size 256 next 331
2024-10-22 22:31:13.267654: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732db300 of size 256 next 171
2024-10-22 22:31:13.267659: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732db400 of size 256 next 273
2024-10-22 22:31:13.267663: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732db500 of size 256 next 174
2024-10-22 22:31:13.267667: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732db600 of size 256 next 309
2024-10-22 22:31:13.267671: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732db700 of size 256 next 213
2024-10-22 22:31:13.267675: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732db800 of size 256 next 231
2024-10-22 22:31:13.267679: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732db900 of size 256 next 325
2024-10-22 22:31:13.267683: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dba00 of size 256 next 252
2024-10-22 22:31:13.267688: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dbb00 of size 256 next 349
2024-10-22 22:31:13.267692: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dbc00 of size 256 next 272
2024-10-22 22:31:13.267696: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dbd00 of size 256 next 295
2024-10-22 22:31:13.267700: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dbe00 of size 256 next 322
2024-10-22 22:31:13.267704: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dbf00 of size 256 next 185
2024-10-22 22:31:13.267708: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dc000 of size 256 next 393
2024-10-22 22:31:13.267712: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dc100 of size 256 next 341
2024-10-22 22:31:13.267716: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dc200 of size 256 next 369
2024-10-22 22:31:13.267721: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dc300 of size 256 next 399
2024-10-22 22:31:13.267725: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dc400 of size 256 next 417
2024-10-22 22:31:13.267729: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dc500 of size 256 next 403
2024-10-22 22:31:13.267733: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dc600 of size 256 next 220
2024-10-22 22:31:13.267737: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dc700 of size 256 next 346
2024-10-22 22:31:13.267741: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dc800 of size 256 next 210
2024-10-22 22:31:13.267748: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dc900 of size 256 next 422
2024-10-22 22:31:13.267752: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dca00 of size 256 next 128
2024-10-22 22:31:13.267756: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dcb00 of size 256 next 212
2024-10-22 22:31:13.267760: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dcc00 of size 256 next 254
2024-10-22 22:31:13.267764: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dcd00 of size 256 next 287
2024-10-22 22:31:13.267769: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dce00 of size 256 next 170
2024-10-22 22:31:13.267773: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dcf00 of size 256 next 354
2024-10-22 22:31:13.267777: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dd000 of size 256 next 336
2024-10-22 22:31:13.267781: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dd100 of size 256 next 359
2024-10-22 22:31:13.267785: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dd200 of size 256 next 419
2024-10-22 22:31:13.267789: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dd300 of size 256 next 416
2024-10-22 22:31:13.267793: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dd400 of size 256 next 364
2024-10-22 22:31:13.267798: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dd500 of size 256 next 368
2024-10-22 22:31:13.267802: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dd600 of size 256 next 400
2024-10-22 22:31:13.267806: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dd700 of size 256 next 261
2024-10-22 22:31:13.267810: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dd800 of size 256 next 304
2024-10-22 22:31:13.267815: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dd900 of size 256 next 505
2024-10-22 22:31:13.267819: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dda00 of size 256 next 476
2024-10-22 22:31:13.267823: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732ddb00 of size 256 next 471
2024-10-22 22:31:13.267828: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732ddc00 of size 256 next 544
2024-10-22 22:31:13.267832: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732ddd00 of size 256 next 545
2024-10-22 22:31:13.267836: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732dde00 of size 256 next 450
2024-10-22 22:31:13.267840: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732ddf00 of size 256 next 442
2024-10-22 22:31:13.267844: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732de000 of size 256 next 453
2024-10-22 22:31:13.267848: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732de100 of size 256 next 394
2024-10-22 22:31:13.267852: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732de200 of size 256 next 36
2024-10-22 22:31:13.267856: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732de300 of size 256 next 281
2024-10-22 22:31:13.267860: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732de400 of size 256 next 384
2024-10-22 22:31:13.267864: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732de500 of size 256 next 386
2024-10-22 22:31:13.267869: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732de600 of size 256 next 449
2024-10-22 22:31:13.267873: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732de700 of size 256 next 88
2024-10-22 22:31:13.267877: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b732de800 of size 16384 next 89
2024-10-22 22:31:13.267883: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f2b732e2800 of size 147456 next 2869
2024-10-22 22:31:13.267887: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73306800 of size 256 next 2908
2024-10-22 22:31:13.267892: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73306900 of size 256 next 2975
2024-10-22 22:31:13.267896: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73306a00 of size 256 next 2919
2024-10-22 22:31:13.267900: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73306b00 of size 256 next 2897
2024-10-22 22:31:13.267904: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f2b73306c00 of size 1536 next 2965
2024-10-22 22:31:13.267908: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73307200 of size 256 next 2964
2024-10-22 22:31:13.267912: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73307300 of size 256 next 2893
2024-10-22 22:31:13.267916: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73307400 of size 256 next 2984
2024-10-22 22:31:13.267920: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73307500 of size 256 next 3009
2024-10-22 22:31:13.267924: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73307600 of size 256 next 2979
2024-10-22 22:31:13.267929: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73307700 of size 256 next 2986
2024-10-22 22:31:13.267933: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73307800 of size 256 next 2956
2024-10-22 22:31:13.267937: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73307900 of size 256 next 2999
2024-10-22 22:31:13.267941: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73307a00 of size 256 next 2928
2024-10-22 22:31:13.267946: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73307b00 of size 256 next 2934
2024-10-22 22:31:13.267950: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73307c00 of size 256 next 3008
2024-10-22 22:31:13.267954: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73307d00 of size 256 next 3003
2024-10-22 22:31:13.267958: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f2b73307e00 of size 256 next 3027
2024-10-22 22:31:13.267963: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73307f00 of size 256 next 2977
2024-10-22 22:31:13.267967: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73308000 of size 256 next 3017
2024-10-22 22:31:13.267971: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f2b73308100 of size 256 next 2981
2024-10-22 22:31:13.267975: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73308200 of size 256 next 3010
2024-10-22 22:31:13.267979: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f2b73308300 of size 256 next 3007
2024-10-22 22:31:13.267983: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73308400 of size 256 next 2980
2024-10-22 22:31:13.267987: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73308500 of size 256 next 2983
2024-10-22 22:31:13.267991: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73308600 of size 256 next 3011
2024-10-22 22:31:13.267995: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73308700 of size 256 next 2854
2024-10-22 22:31:13.268000: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73308800 of size 256 next 2969
2024-10-22 22:31:13.268004: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73308900 of size 256 next 2949
2024-10-22 22:31:13.268008: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73308a00 of size 256 next 2895
2024-10-22 22:31:13.268014: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73308b00 of size 256 next 3040
2024-10-22 22:31:13.268018: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73308c00 of size 256 next 3024
2024-10-22 22:31:13.268023: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73308d00 of size 256 next 2798
2024-10-22 22:31:13.268027: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f2b73308e00 of size 1280 next 2807
2024-10-22 22:31:13.268031: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73309300 of size 256 next 2847
2024-10-22 22:31:13.268035: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73309400 of size 256 next 2839
2024-10-22 22:31:13.268040: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73309500 of size 256 next 2817
2024-10-22 22:31:13.268044: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73309600 of size 256 next 2881
2024-10-22 22:31:13.268048: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73309700 of size 256 next 2855
2024-10-22 22:31:13.268052: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73309800 of size 256 next 2860
2024-10-22 22:31:13.268057: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73309900 of size 256 next 2885
2024-10-22 22:31:13.268061: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73309a00 of size 256 next 2870
2024-10-22 22:31:13.268065: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73309b00 of size 256 next 2914
2024-10-22 22:31:13.268069: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73309c00 of size 256 next 2891
2024-10-22 22:31:13.268073: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73309d00 of size 256 next 2907
2024-10-22 22:31:13.268077: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73309e00 of size 256 next 2892
2024-10-22 22:31:13.268081: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73309f00 of size 256 next 2852
2024-10-22 22:31:13.268085: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7330a000 of size 256 next 2845
2024-10-22 22:31:13.268089: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7330a100 of size 256 next 2866
2024-10-22 22:31:13.268093: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7330a200 of size 256 next 2851
2024-10-22 22:31:13.268098: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7330a300 of size 256 next 2827
2024-10-22 22:31:13.268102: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7330a400 of size 256 next 2882
2024-10-22 22:31:13.268106: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7330a500 of size 256 next 2863
2024-10-22 22:31:13.268110: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7330a600 of size 256 next 2841
2024-10-22 22:31:13.268114: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7330a700 of size 256 next 2749
2024-10-22 22:31:13.268118: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7330a800 of size 256 next 2826
2024-10-22 22:31:13.268122: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f2b7330a900 of size 68608 next 2936
2024-10-22 22:31:13.268126: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7331b500 of size 256 next 2932
2024-10-22 22:31:13.268130: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7331b600 of size 256 next 2939
2024-10-22 22:31:13.268135: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7331b700 of size 256 next 2906
2024-10-22 22:31:13.268139: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7331b800 of size 256 next 2963
2024-10-22 22:31:13.268143: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7331b900 of size 256 next 2972
2024-10-22 22:31:13.268149: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7331ba00 of size 256 next 2971
2024-10-22 22:31:13.268153: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7331bb00 of size 256 next 3002
2024-10-22 22:31:13.268158: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7331bc00 of size 256 next 3015
2024-10-22 22:31:13.268162: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7331bd00 of size 256 next 2982
2024-10-22 22:31:13.268165: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7331be00 of size 256 next 2961
2024-10-22 22:31:13.268170: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f2b7331bf00 of size 1792 next 3025
2024-10-22 22:31:13.268174: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7331c600 of size 256 next 3000
2024-10-22 22:31:13.268178: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7331c700 of size 256 next 3034
2024-10-22 22:31:13.268183: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7331c800 of size 256 next 2966
2024-10-22 22:31:13.268187: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7331c900 of size 256 next 3020
2024-10-22 22:31:13.268191: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f2b7331ca00 of size 13824 next 2930
2024-10-22 22:31:13.268196: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73320000 of size 256 next 2962
2024-10-22 22:31:13.268199: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73320100 of size 256 next 2911
2024-10-22 22:31:13.268204: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73320200 of size 256 next 2872
2024-10-22 22:31:13.268208: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73320300 of size 256 next 2940
2024-10-22 22:31:13.268212: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73320400 of size 256 next 2937
2024-10-22 22:31:13.268217: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f2b73320500 of size 58880 next 877
2024-10-22 22:31:13.268221: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7332eb00 of size 256 next 808
2024-10-22 22:31:13.268225: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7332ec00 of size 256 next 951
2024-10-22 22:31:13.268230: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7332ed00 of size 256 next 969
2024-10-22 22:31:13.268233: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7332ee00 of size 256 next 909
2024-10-22 22:31:13.268238: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7332ef00 of size 256 next 967
2024-10-22 22:31:13.268242: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7332f000 of size 256 next 1082
2024-10-22 22:31:13.268246: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7332f100 of size 256 next 738
2024-10-22 22:31:13.268250: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7332f200 of size 256 next 837
2024-10-22 22:31:13.268254: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7332f300 of size 256 next 830
2024-10-22 22:31:13.268258: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7332f400 of size 256 next 765
2024-10-22 22:31:13.268262: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7332f500 of size 256 next 856
2024-10-22 22:31:13.268266: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7332f600 of size 256 next 991
2024-10-22 22:31:13.268271: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7332f700 of size 256 next 871
2024-10-22 22:31:13.268275: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7332f800 of size 256 next 629
2024-10-22 22:31:13.268279: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7332f900 of size 256 next 892
2024-10-22 22:31:13.268286: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7332fa00 of size 256 next 869
2024-10-22 22:31:13.268290: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7332fb00 of size 256 next 922
2024-10-22 22:31:13.268295: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7332fc00 of size 256 next 840
2024-10-22 22:31:13.268299: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7332fd00 of size 256 next 807
2024-10-22 22:31:13.268303: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7332fe00 of size 256 next 1011
2024-10-22 22:31:13.268307: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7332ff00 of size 256 next 889
2024-10-22 22:31:13.268312: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73330000 of size 256 next 1087
2024-10-22 22:31:13.268316: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73330100 of size 256 next 930
2024-10-22 22:31:13.268320: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73330200 of size 256 next 875
2024-10-22 22:31:13.268325: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73330300 of size 256 next 928
2024-10-22 22:31:13.268329: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73330400 of size 256 next 848
2024-10-22 22:31:13.268333: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73330500 of size 256 next 959
2024-10-22 22:31:13.268337: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73330600 of size 256 next 896
2024-10-22 22:31:13.268341: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73330700 of size 256 next 494
2024-10-22 22:31:13.268345: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73330800 of size 256 next 833
2024-10-22 22:31:13.268350: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73330900 of size 256 next 911
2024-10-22 22:31:13.268354: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73330a00 of size 256 next 883
2024-10-22 22:31:13.268358: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73330b00 of size 256 next 884
2024-10-22 22:31:13.268362: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73330c00 of size 256 next 858
2024-10-22 22:31:13.268366: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73330d00 of size 256 next 933
2024-10-22 22:31:13.268370: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73330e00 of size 256 next 947
2024-10-22 22:31:13.268374: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73330f00 of size 256 next 979
2024-10-22 22:31:13.268378: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73331000 of size 256 next 1032
2024-10-22 22:31:13.268382: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73331100 of size 256 next 968
2024-10-22 22:31:13.268386: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73331200 of size 256 next 1054
2024-10-22 22:31:13.268391: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73331300 of size 256 next 1026
2024-10-22 22:31:13.268395: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73331400 of size 256 next 852
2024-10-22 22:31:13.268399: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73331500 of size 256 next 838
2024-10-22 22:31:13.268403: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73331600 of size 256 next 864
2024-10-22 22:31:13.268407: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73331700 of size 256 next 1015
2024-10-22 22:31:13.268412: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73331800 of size 256 next 916
2024-10-22 22:31:13.268418: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73331900 of size 256 next 1115
2024-10-22 22:31:13.268422: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73331a00 of size 256 next 1085
2024-10-22 22:31:13.268427: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73331b00 of size 256 next 1051
2024-10-22 22:31:13.268431: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73331c00 of size 256 next 865
2024-10-22 22:31:13.268435: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73331d00 of size 256 next 720
2024-10-22 22:31:13.268439: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73331e00 of size 256 next 733
2024-10-22 22:31:13.268443: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73331f00 of size 256 next 881
2024-10-22 22:31:13.268447: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73332000 of size 256 next 971
2024-10-22 22:31:13.268451: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73332100 of size 256 next 783
2024-10-22 22:31:13.268455: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73332200 of size 256 next 927
2024-10-22 22:31:13.268459: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73332300 of size 256 next 742
2024-10-22 22:31:13.268463: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73332400 of size 256 next 846
2024-10-22 22:31:13.268467: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73332500 of size 256 next 839
2024-10-22 22:31:13.268472: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73332600 of size 256 next 986
2024-10-22 22:31:13.268476: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73332700 of size 256 next 123
2024-10-22 22:31:13.268480: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73332800 of size 21760 next 117
2024-10-22 22:31:13.268484: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73337d00 of size 16384 next 116
2024-10-22 22:31:13.268488: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7333bd00 of size 256 next 2941
2024-10-22 22:31:13.268492: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7333be00 of size 256 next 2918
2024-10-22 22:31:13.268496: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7333bf00 of size 256 next 2929
2024-10-22 22:31:13.268500: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7333c000 of size 256 next 2951
2024-10-22 22:31:13.268505: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7333c100 of size 256 next 2894
2024-10-22 22:31:13.268509: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7333c200 of size 256 next 2953
2024-10-22 22:31:13.268513: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7333c300 of size 256 next 2850
2024-10-22 22:31:13.268517: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f2b7333c400 of size 256 next 2858
2024-10-22 22:31:13.268521: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7333c500 of size 256 next 2853
2024-10-22 22:31:13.268525: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7333c600 of size 256 next 2831
2024-10-22 22:31:13.268529: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7333c700 of size 256 next 2829
2024-10-22 22:31:13.268534: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7333c800 of size 256 next 2745
2024-10-22 22:31:13.268538: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7333c900 of size 256 next 2822
2024-10-22 22:31:13.268542: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7333ca00 of size 256 next 2819
2024-10-22 22:31:13.268546: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7333cb00 of size 256 next 2794
2024-10-22 22:31:13.268553: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7333cc00 of size 256 next 2864
2024-10-22 22:31:13.268557: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7333cd00 of size 256 next 2814
2024-10-22 22:31:13.268561: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f2b7333ce00 of size 163584 next 2343
2024-10-22 22:31:13.268565: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73364d00 of size 256 next 2333
2024-10-22 22:31:13.268569: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73364e00 of size 256 next 2323
2024-10-22 22:31:13.268573: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73364f00 of size 256 next 2280
2024-10-22 22:31:13.268577: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73365000 of size 256 next 2432
2024-10-22 22:31:13.268582: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73365100 of size 256 next 2412
2024-10-22 22:31:13.268586: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73365200 of size 256 next 2407
2024-10-22 22:31:13.268590: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73365300 of size 256 next 2444
2024-10-22 22:31:13.268594: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73365400 of size 256 next 2381
2024-10-22 22:31:13.268598: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73365500 of size 256 next 2353
2024-10-22 22:31:13.268602: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73365600 of size 256 next 2421
2024-10-22 22:31:13.268606: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73365700 of size 256 next 2402
2024-10-22 22:31:13.268610: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73365800 of size 256 next 2365
2024-10-22 22:31:13.268615: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73365900 of size 256 next 2459
2024-10-22 22:31:13.268619: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73365a00 of size 256 next 2408
2024-10-22 22:31:13.268623: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73365b00 of size 256 next 2378
2024-10-22 22:31:13.268627: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73365c00 of size 256 next 2458
2024-10-22 22:31:13.268632: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73365d00 of size 256 next 2319
2024-10-22 22:31:13.268636: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73365e00 of size 256 next 2374
2024-10-22 22:31:13.268640: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73365f00 of size 256 next 2434
2024-10-22 22:31:13.268644: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73366000 of size 256 next 2398
2024-10-22 22:31:13.268648: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73366100 of size 256 next 2423
2024-10-22 22:31:13.268652: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73366200 of size 256 next 2577
2024-10-22 22:31:13.268656: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73366300 of size 256 next 2405
2024-10-22 22:31:13.268661: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73366400 of size 256 next 2616
2024-10-22 22:31:13.268665: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73366500 of size 256 next 2330
2024-10-22 22:31:13.268669: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73366600 of size 256 next 2508
2024-10-22 22:31:13.268673: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73366700 of size 256 next 2474
2024-10-22 22:31:13.268677: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73366800 of size 256 next 2383
2024-10-22 22:31:13.268685: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73366900 of size 256 next 2426
2024-10-22 22:31:13.268689: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73366a00 of size 256 next 2415
2024-10-22 22:31:13.268693: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73366b00 of size 256 next 2460
2024-10-22 22:31:13.268697: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73366c00 of size 256 next 2384
2024-10-22 22:31:13.268701: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73366d00 of size 256 next 2445
2024-10-22 22:31:13.268705: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73366e00 of size 256 next 2392
2024-10-22 22:31:13.268710: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73366f00 of size 256 next 2377
2024-10-22 22:31:13.268714: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73367000 of size 256 next 2447
2024-10-22 22:31:13.268719: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73367100 of size 256 next 2361
2024-10-22 22:31:13.268723: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73367200 of size 256 next 2428
2024-10-22 22:31:13.268727: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73367300 of size 256 next 2448
2024-10-22 22:31:13.268731: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73367400 of size 256 next 2455
2024-10-22 22:31:13.268736: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73367500 of size 256 next 2439
2024-10-22 22:31:13.268740: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73367600 of size 256 next 2373
2024-10-22 22:31:13.268744: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73367700 of size 256 next 2442
2024-10-22 22:31:13.268749: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73367800 of size 256 next 2466
2024-10-22 22:31:13.268753: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73367900 of size 256 next 2409
2024-10-22 22:31:13.268757: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73367a00 of size 256 next 2441
2024-10-22 22:31:13.268761: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73367b00 of size 256 next 2454
2024-10-22 22:31:13.268765: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73367c00 of size 256 next 2482
2024-10-22 22:31:13.268769: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73367d00 of size 256 next 2397
2024-10-22 22:31:13.268773: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73367e00 of size 256 next 2346
2024-10-22 22:31:13.268777: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73367f00 of size 256 next 2385
2024-10-22 22:31:13.268781: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73368000 of size 256 next 2453
2024-10-22 22:31:13.268786: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73368100 of size 256 next 2425
2024-10-22 22:31:13.268790: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73368200 of size 256 next 2475
2024-10-22 22:31:13.268794: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73368300 of size 256 next 2487
2024-10-22 22:31:13.268798: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73368400 of size 256 next 2484
2024-10-22 22:31:13.268802: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73368500 of size 256 next 2449
2024-10-22 22:31:13.268806: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73368600 of size 256 next 2436
2024-10-22 22:31:13.268810: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73368700 of size 256 next 2337
2024-10-22 22:31:13.268814: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73368800 of size 256 next 2410
2024-10-22 22:31:13.268821: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73368900 of size 256 next 2393
2024-10-22 22:31:13.268826: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73368a00 of size 256 next 2396
2024-10-22 22:31:13.268830: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73368b00 of size 256 next 2371
2024-10-22 22:31:13.268834: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73368c00 of size 256 next 2291
2024-10-22 22:31:13.268838: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73368d00 of size 256 next 2390
2024-10-22 22:31:13.268843: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73368e00 of size 256 next 2418
2024-10-22 22:31:13.268847: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73368f00 of size 256 next 2350
2024-10-22 22:31:13.268851: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73369000 of size 256 next 2394
2024-10-22 22:31:13.268856: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73369100 of size 256 next 2419
2024-10-22 22:31:13.268860: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73369200 of size 256 next 2399
2024-10-22 22:31:13.268864: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73369300 of size 256 next 2329
2024-10-22 22:31:13.268868: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73369400 of size 256 next 2400
2024-10-22 22:31:13.268873: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73369500 of size 256 next 2380
2024-10-22 22:31:13.268877: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73369600 of size 256 next 2388
2024-10-22 22:31:13.268880: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73369700 of size 256 next 2326
2024-10-22 22:31:13.268885: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73369800 of size 256 next 2362
2024-10-22 22:31:13.268889: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73369900 of size 256 next 2354
2024-10-22 22:31:13.268893: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73369a00 of size 256 next 2345
2024-10-22 22:31:13.268897: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73369b00 of size 256 next 2417
2024-10-22 22:31:13.268901: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73369c00 of size 256 next 2404
2024-10-22 22:31:13.268905: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73369d00 of size 256 next 2292
2024-10-22 22:31:13.268909: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73369e00 of size 256 next 2376
2024-10-22 22:31:13.268913: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73369f00 of size 256 next 2320
2024-10-22 22:31:13.268917: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336a000 of size 256 next 2203
2024-10-22 22:31:13.268922: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336a100 of size 256 next 2366
2024-10-22 22:31:13.268926: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336a200 of size 256 next 2379
2024-10-22 22:31:13.268930: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336a300 of size 256 next 2411
2024-10-22 22:31:13.268934: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336a400 of size 256 next 2382
2024-10-22 22:31:13.268938: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336a500 of size 256 next 2370
2024-10-22 22:31:13.268942: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336a600 of size 256 next 2431
2024-10-22 22:31:13.268946: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336a700 of size 256 next 2193
2024-10-22 22:31:13.268953: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336a800 of size 256 next 2528
2024-10-22 22:31:13.268957: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336a900 of size 256 next 2539
2024-10-22 22:31:13.268961: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336aa00 of size 256 next 2420
2024-10-22 22:31:13.268965: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336ab00 of size 256 next 2572
2024-10-22 22:31:13.268969: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336ac00 of size 256 next 2387
2024-10-22 22:31:13.268973: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336ad00 of size 256 next 2422
2024-10-22 22:31:13.268977: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336ae00 of size 256 next 2446
2024-10-22 22:31:13.268981: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336af00 of size 256 next 2456
2024-10-22 22:31:13.268986: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336b000 of size 256 next 2527
2024-10-22 22:31:13.268990: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336b100 of size 256 next 2544
2024-10-22 22:31:13.268994: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336b200 of size 256 next 2451
2024-10-22 22:31:13.268998: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336b300 of size 256 next 2433
2024-10-22 22:31:13.269003: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336b400 of size 256 next 2507
2024-10-22 22:31:13.269007: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336b500 of size 256 next 2589
2024-10-22 22:31:13.269012: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336b600 of size 256 next 2762
2024-10-22 22:31:13.269016: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336b700 of size 256 next 2355
2024-10-22 22:31:13.269020: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336b800 of size 256 next 2368
2024-10-22 22:31:13.269024: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336b900 of size 256 next 2386
2024-10-22 22:31:13.269029: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336ba00 of size 256 next 2668
2024-10-22 22:31:13.269033: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336bb00 of size 256 next 2526
2024-10-22 22:31:13.269037: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336bc00 of size 256 next 113
2024-10-22 22:31:13.269041: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336bd00 of size 256 next 1249
2024-10-22 22:31:13.269045: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336be00 of size 256 next 1214
2024-10-22 22:31:13.269049: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336bf00 of size 256 next 1318
2024-10-22 22:31:13.269053: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336c000 of size 256 next 1197
2024-10-22 22:31:13.269057: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336c100 of size 256 next 1299
2024-10-22 22:31:13.269061: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336c200 of size 256 next 1586
2024-10-22 22:31:13.269065: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336c300 of size 256 next 1458
2024-10-22 22:31:13.269069: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336c400 of size 256 next 1392
2024-10-22 22:31:13.269073: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336c500 of size 256 next 1319
2024-10-22 22:31:13.269078: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336c600 of size 256 next 1236
2024-10-22 22:31:13.269082: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336c700 of size 256 next 1344
2024-10-22 22:31:13.269088: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336c800 of size 256 next 1349
2024-10-22 22:31:13.269093: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336c900 of size 256 next 1331
2024-10-22 22:31:13.269097: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336ca00 of size 256 next 1280
2024-10-22 22:31:13.269101: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336cb00 of size 256 next 1372
2024-10-22 22:31:13.269105: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336cc00 of size 256 next 1401
2024-10-22 22:31:13.269109: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336cd00 of size 256 next 1148
2024-10-22 22:31:13.269114: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336ce00 of size 256 next 1426
2024-10-22 22:31:13.269118: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336cf00 of size 256 next 1275
2024-10-22 22:31:13.269122: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336d000 of size 256 next 1381
2024-10-22 22:31:13.269127: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336d100 of size 256 next 1310
2024-10-22 22:31:13.269131: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336d200 of size 256 next 1411
2024-10-22 22:31:13.269135: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336d300 of size 256 next 1301
2024-10-22 22:31:13.269139: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336d400 of size 256 next 1443
2024-10-22 22:31:13.269143: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336d500 of size 256 next 1257
2024-10-22 22:31:13.269147: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336d600 of size 256 next 1385
2024-10-22 22:31:13.269151: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336d700 of size 256 next 1404
2024-10-22 22:31:13.269155: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336d800 of size 256 next 1472
2024-10-22 22:31:13.269160: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336d900 of size 256 next 1342
2024-10-22 22:31:13.269164: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336da00 of size 256 next 1353
2024-10-22 22:31:13.269168: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336db00 of size 256 next 1482
2024-10-22 22:31:13.269172: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336dc00 of size 256 next 1359
2024-10-22 22:31:13.269178: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336dd00 of size 256 next 1408
2024-10-22 22:31:13.269182: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336de00 of size 256 next 1346
2024-10-22 22:31:13.269186: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336df00 of size 256 next 1345
2024-10-22 22:31:13.269190: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336e000 of size 256 next 1415
2024-10-22 22:31:13.269194: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336e100 of size 256 next 1302
2024-10-22 22:31:13.269199: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336e200 of size 256 next 1153
2024-10-22 22:31:13.269202: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336e300 of size 256 next 1380
2024-10-22 22:31:13.269207: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336e400 of size 256 next 1390
2024-10-22 22:31:13.269211: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336e500 of size 256 next 1327
2024-10-22 22:31:13.269215: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336e600 of size 256 next 1369
2024-10-22 22:31:13.269221: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336e700 of size 256 next 1423
2024-10-22 22:31:13.269225: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336e800 of size 256 next 1489
2024-10-22 22:31:13.269229: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336e900 of size 256 next 1365
2024-10-22 22:31:13.269234: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336ea00 of size 256 next 1424
2024-10-22 22:31:13.269238: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336eb00 of size 256 next 1376
2024-10-22 22:31:13.269242: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336ec00 of size 256 next 1269
2024-10-22 22:31:13.269246: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336ed00 of size 256 next 1419
2024-10-22 22:31:13.269250: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336ee00 of size 256 next 1244
2024-10-22 22:31:13.269254: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336ef00 of size 256 next 1389
2024-10-22 22:31:13.269258: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336f000 of size 256 next 1358
2024-10-22 22:31:13.269262: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336f100 of size 256 next 1447
2024-10-22 22:31:13.269267: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336f200 of size 256 next 1403
2024-10-22 22:31:13.269271: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336f300 of size 256 next 1314
2024-10-22 22:31:13.269275: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336f400 of size 256 next 1502
2024-10-22 22:31:13.269280: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336f500 of size 256 next 1263
2024-10-22 22:31:13.269284: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336f600 of size 256 next 1405
2024-10-22 22:31:13.269288: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336f700 of size 256 next 1386
2024-10-22 22:31:13.269292: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336f800 of size 256 next 1555
2024-10-22 22:31:13.269296: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336f900 of size 256 next 1695
2024-10-22 22:31:13.269300: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336fa00 of size 256 next 1605
2024-10-22 22:31:13.269304: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336fb00 of size 256 next 1382
2024-10-22 22:31:13.269308: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336fc00 of size 256 next 1511
2024-10-22 22:31:13.269312: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336fd00 of size 256 next 1464
2024-10-22 22:31:13.269317: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336fe00 of size 256 next 1515
2024-10-22 22:31:13.269321: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b7336ff00 of size 256 next 1416
2024-10-22 22:31:13.269325: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73370000 of size 256 next 1295
2024-10-22 22:31:13.269329: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73370100 of size 256 next 1395
2024-10-22 22:31:13.269333: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73370200 of size 256 next 1427
2024-10-22 22:31:13.269337: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73370300 of size 256 next 1334
2024-10-22 22:31:13.269341: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73370400 of size 256 next 1321
2024-10-22 22:31:13.269345: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73370500 of size 256 next 960
2024-10-22 22:31:13.269349: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f2b73370600 of size 512 next 1646
2024-10-22 22:31:13.269356: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73370800 of size 256 next 1183
2024-10-22 22:31:13.269360: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73370900 of size 256 next 1397
2024-10-22 22:31:13.269364: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73370a00 of size 256 next 1501
2024-10-22 22:31:13.269369: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73370b00 of size 256 next 1452
2024-10-22 22:31:13.269373: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73370c00 of size 256 next 1534
2024-10-22 22:31:13.269377: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73370d00 of size 256 next 43
2024-10-22 22:31:13.269381: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73370e00 of size 256 next 44
2024-10-22 22:31:13.269385: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73370f00 of size 256 next 1628
2024-10-22 22:31:13.269389: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73371000 of size 256 next 1347
2024-10-22 22:31:13.269394: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73371100 of size 256 next 1272
2024-10-22 22:31:13.269398: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73371200 of size 256 next 1525
2024-10-22 22:31:13.269403: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73371300 of size 256 next 1559
2024-10-22 22:31:13.269407: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73371400 of size 256 next 1681
2024-10-22 22:31:13.269411: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73371500 of size 256 next 1647
2024-10-22 22:31:13.269415: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73371600 of size 256 next 1697
2024-10-22 22:31:13.269420: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73371700 of size 256 next 1700
2024-10-22 22:31:13.269424: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73371800 of size 256 next 47
2024-10-22 22:31:13.269428: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73371900 of size 256 next 1708
2024-10-22 22:31:13.269432: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73371a00 of size 256 next 1710
2024-10-22 22:31:13.269436: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73371b00 of size 256 next 1714
2024-10-22 22:31:13.269440: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73371c00 of size 256 next 1694
2024-10-22 22:31:13.269444: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73371d00 of size 256 next 1444
2024-10-22 22:31:13.269448: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73371e00 of size 256 next 1396
2024-10-22 22:31:13.269452: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73371f00 of size 256 next 1574
2024-10-22 22:31:13.269456: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73372000 of size 256 next 122
2024-10-22 22:31:13.269460: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73372100 of size 256 next 1527
2024-10-22 22:31:13.269464: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73372200 of size 256 next 1107
2024-10-22 22:31:13.269468: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73372300 of size 256 next 1601
2024-10-22 22:31:13.269472: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73372400 of size 256 next 1582
2024-10-22 22:31:13.269477: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73372500 of size 256 next 1421
2024-10-22 22:31:13.269481: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73372600 of size 256 next 1459
2024-10-22 22:31:13.269487: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73372700 of size 256 next 410
2024-10-22 22:31:13.269491: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73372800 of size 256 next 355
2024-10-22 22:31:13.269496: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73372900 of size 256 next 367
2024-10-22 22:31:13.269500: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73372a00 of size 256 next 222
2024-10-22 22:31:13.269504: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73372b00 of size 256 next 350
2024-10-22 22:31:13.269508: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73372c00 of size 256 next 233
2024-10-22 22:31:13.269512: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73372d00 of size 256 next 263
2024-10-22 22:31:13.269517: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73372e00 of size 256 next 509
2024-10-22 22:31:13.269521: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73372f00 of size 256 next 462
2024-10-22 22:31:13.269525: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73373000 of size 256 next 278
2024-10-22 22:31:13.269529: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73373100 of size 256 next 481
2024-10-22 22:31:13.269533: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73373200 of size 256 next 299
2024-10-22 22:31:13.269537: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73373300 of size 256 next 196
2024-10-22 22:31:13.269541: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73373400 of size 256 next 190
2024-10-22 22:31:13.269545: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73373500 of size 256 next 430
2024-10-22 22:31:13.269549: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73373600 of size 256 next 441
2024-10-22 22:31:13.269553: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73373700 of size 256 next 448
2024-10-22 22:31:13.269557: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73373800 of size 256 next 522
2024-10-22 22:31:13.269562: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73373900 of size 256 next 529
2024-10-22 22:31:13.269566: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73373a00 of size 256 next 474
2024-10-22 22:31:13.269570: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73373b00 of size 256 next 457
2024-10-22 22:31:13.269574: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73373c00 of size 256 next 330
2024-10-22 22:31:13.269579: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73373d00 of size 256 next 348
2024-10-22 22:31:13.269583: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73373e00 of size 256 next 493
2024-10-22 22:31:13.269587: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73373f00 of size 256 next 405
2024-10-22 22:31:13.269591: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73374000 of size 256 next 401
2024-10-22 22:31:13.269595: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73374100 of size 256 next 455
2024-10-22 22:31:13.269599: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73374200 of size 256 next 445
2024-10-22 22:31:13.269604: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73374300 of size 256 next 144
2024-10-22 22:31:13.269608: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73374400 of size 256 next 373
2024-10-22 22:31:13.269612: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73374500 of size 256 next 333
2024-10-22 22:31:13.269616: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73374600 of size 256 next 344
2024-10-22 22:31:13.269623: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73374700 of size 256 next 362
2024-10-22 22:31:13.269627: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73374800 of size 256 next 513
2024-10-22 22:31:13.269631: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73374900 of size 256 next 555
2024-10-22 22:31:13.269635: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73374a00 of size 256 next 620
2024-10-22 22:31:13.269639: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73374b00 of size 256 next 617
2024-10-22 22:31:13.269643: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73374c00 of size 256 next 431
2024-10-22 22:31:13.269648: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73374d00 of size 256 next 603
2024-10-22 22:31:13.269652: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73374e00 of size 256 next 486
2024-10-22 22:31:13.269656: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73374f00 of size 256 next 420
2024-10-22 22:31:13.269660: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73375000 of size 256 next 395
2024-10-22 22:31:13.269664: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73375100 of size 256 next 363
2024-10-22 22:31:13.269668: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73375200 of size 256 next 407
2024-10-22 22:31:13.269672: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73375300 of size 256 next 490
2024-10-22 22:31:13.269676: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73375400 of size 256 next 372
2024-10-22 22:31:13.269680: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73375500 of size 256 next 320
2024-10-22 22:31:13.269684: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73375600 of size 256 next 357
2024-10-22 22:31:13.269688: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73375700 of size 256 next 477
2024-10-22 22:31:13.269692: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73375800 of size 256 next 572
2024-10-22 22:31:13.269697: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73375900 of size 256 next 380
2024-10-22 22:31:13.269701: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73375a00 of size 256 next 459
2024-10-22 22:31:13.269705: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73375b00 of size 256 next 469
2024-10-22 22:31:13.269709: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73375c00 of size 256 next 531
2024-10-22 22:31:13.269713: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73375d00 of size 256 next 671
2024-10-22 22:31:13.269718: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73375e00 of size 256 next 596
2024-10-22 22:31:13.269722: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73375f00 of size 256 next 613
2024-10-22 22:31:13.269726: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73376000 of size 256 next 578
2024-10-22 22:31:13.269731: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73376100 of size 256 next 566
2024-10-22 22:31:13.269735: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73376200 of size 256 next 532
2024-10-22 22:31:13.269739: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73376300 of size 256 next 540
2024-10-22 22:31:13.269743: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73376400 of size 256 next 650
2024-10-22 22:31:13.269747: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73376500 of size 256 next 316
2024-10-22 22:31:13.269751: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73376600 of size 256 next 535
2024-10-22 22:31:13.269758: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73376700 of size 256 next 475
2024-10-22 22:31:13.269762: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73376800 of size 256 next 139
2024-10-22 22:31:13.269766: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b73376900 of size 176640 next 141
2024-10-22 22:31:13.269777: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a1b00 of size 256 next 413
2024-10-22 22:31:13.269782: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a1c00 of size 256 next 1207
2024-10-22 22:31:13.269786: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a1d00 of size 256 next 1040
2024-10-22 22:31:13.269790: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a1e00 of size 256 next 1137
2024-10-22 22:31:13.269794: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a1f00 of size 256 next 1140
2024-10-22 22:31:13.269798: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a2000 of size 256 next 1352
2024-10-22 22:31:13.269802: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a2100 of size 256 next 444
2024-10-22 22:31:13.269806: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a2200 of size 256 next 111
2024-10-22 22:31:13.269810: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a2300 of size 256 next 328
2024-10-22 22:31:13.269814: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a2400 of size 256 next 138
2024-10-22 22:31:13.269819: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a2500 of size 256 next 467
2024-10-22 22:31:13.269823: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a2600 of size 256 next 478
2024-10-22 22:31:13.269827: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a2700 of size 256 next 600
2024-10-22 22:31:13.269831: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a2800 of size 256 next 676
2024-10-22 22:31:13.269835: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a2900 of size 256 next 510
2024-10-22 22:31:13.269840: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a2a00 of size 256 next 434
2024-10-22 22:31:13.269844: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a2b00 of size 256 next 488
2024-10-22 22:31:13.269848: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a2c00 of size 256 next 507
2024-10-22 22:31:13.269853: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a2d00 of size 256 next 1059
2024-10-22 22:31:13.269857: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a2e00 of size 256 next 499
2024-10-22 22:31:13.269861: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a2f00 of size 256 next 548
2024-10-22 22:31:13.269865: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a3000 of size 256 next 688
2024-10-22 22:31:13.269869: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a3100 of size 256 next 1231
2024-10-22 22:31:13.269873: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a3200 of size 256 next 41
2024-10-22 22:31:13.269877: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a3300 of size 256 next 29
2024-10-22 22:31:13.269881: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a3400 of size 256 next 1074
2024-10-22 22:31:13.269885: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a3500 of size 256 next 1033
2024-10-22 22:31:13.269889: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a3600 of size 256 next 948
2024-10-22 22:31:13.269893: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a3700 of size 256 next 541
2024-10-22 22:31:13.269900: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a3800 of size 256 next 608
2024-10-22 22:31:13.269904: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a3900 of size 256 next 176
2024-10-22 22:31:13.269908: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a3a00 of size 256 next 601
2024-10-22 22:31:13.269912: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a3b00 of size 256 next 609
2024-10-22 22:31:13.269916: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a3c00 of size 256 next 1125
2024-10-22 22:31:13.269920: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a3d00 of size 256 next 1124
2024-10-22 22:31:13.269925: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a3e00 of size 256 next 398
2024-10-22 22:31:13.269929: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a3f00 of size 256 next 227
2024-10-22 22:31:13.269933: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a4000 of size 256 next 1092
2024-10-22 22:31:13.269937: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a4100 of size 256 next 1731
2024-10-22 22:31:13.269941: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a4200 of size 256 next 1640
2024-10-22 22:31:13.269945: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a4300 of size 512 next 1322
2024-10-22 22:31:13.269949: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a4500 of size 256 next 30
2024-10-22 22:31:13.269954: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a4600 of size 256 next 1373
2024-10-22 22:31:13.269958: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a4700 of size 256 next 1199
2024-10-22 22:31:13.269962: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a4800 of size 256 next 831
2024-10-22 22:31:13.269966: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a4900 of size 256 next 771
2024-10-22 22:31:13.269970: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a4a00 of size 256 next 1039
2024-10-22 22:31:13.269975: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a4b00 of size 256 next 1044
2024-10-22 22:31:13.269978: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a4c00 of size 256 next 1135
2024-10-22 22:31:13.269983: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a4d00 of size 256 next 1670
2024-10-22 22:31:13.269987: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a4e00 of size 256 next 1317
2024-10-22 22:31:13.269991: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a4f00 of size 256 next 1158
2024-10-22 22:31:13.269995: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a5000 of size 256 next 790
2024-10-22 22:31:13.269999: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a5100 of size 256 next 1303
2024-10-22 22:31:13.270003: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a5200 of size 256 next 1855
2024-10-22 22:31:13.270007: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a5300 of size 256 next 1834
2024-10-22 22:31:13.270011: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a5400 of size 256 next 1894
2024-10-22 22:31:13.270015: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a5500 of size 256 next 2062
2024-10-22 22:31:13.270019: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a5600 of size 256 next 2211
2024-10-22 22:31:13.270023: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a5700 of size 256 next 2199
2024-10-22 22:31:13.270029: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a5800 of size 256 next 1858
2024-10-22 22:31:13.270034: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a5900 of size 256 next 443
2024-10-22 22:31:13.270038: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2b733a5a00 of size 256 next 94
2024-10-22 22:31:13.270042: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7f2b733a5b00 of size 369920 next 18446744073709551615
2024-10-22 22:31:13.270046: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: 
2024-10-22 22:31:13.270054: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 2934 Chunks of size 256 totalling 733.5KiB
2024-10-22 22:31:13.270059: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 7 Chunks of size 512 totalling 3.5KiB
2024-10-22 22:31:13.270064: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 768 totalling 2.2KiB
2024-10-22 22:31:13.270068: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1280 totalling 1.2KiB
2024-10-22 22:31:13.270073: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 13312 totalling 13.0KiB
2024-10-22 22:31:13.270078: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 16384 totalling 32.0KiB
2024-10-22 22:31:13.270083: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 21760 totalling 21.2KiB
2024-10-22 22:31:13.270088: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 32768 totalling 64.0KiB
2024-10-22 22:31:13.270092: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 176640 totalling 172.5KiB
2024-10-22 22:31:13.270097: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 382208 totalling 373.2KiB
2024-10-22 22:31:13.270102: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 16430848 totalling 15.67MiB
2024-10-22 22:31:13.270107: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 17123584 totalling 16.33MiB
2024-10-22 22:31:13.270111: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 22212864 totalling 21.18MiB
2024-10-22 22:31:13.270116: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 22595072 totalling 43.10MiB
2024-10-22 22:31:13.270121: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 33554432 totalling 32.00MiB
2024-10-22 22:31:13.270125: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 44513792 totalling 42.45MiB
2024-10-22 22:31:13.270130: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 48910336 totalling 46.64MiB
2024-10-22 22:31:13.270135: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 16862984704 totalling 15.70GiB
2024-10-22 22:31:13.270140: I external/local_tsl/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 15.92GiB
2024-10-22 22:31:13.270145: I external/local_tsl/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 17718837248 memory_limit_: 28969861120 available bytes: 11251023872 curr_region_allocation_bytes_: 17179869184
2024-10-22 22:31:13.270153: I external/local_tsl/tsl/framework/bfc_allocator.cc:1114] Stats: 
Limit:                     28969861120
InUse:                     17092371200
MaxInUse:                  17154539520
NumAllocs:                      404928
MaxAllocSize:              16862984704
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2024-10-22 22:31:13.270284: W external/local_tsl/tsl/framework/bfc_allocator.cc:494] ****************************************************************************************************
2024-10-22 22:31:13.270332: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at matmul_op_impl.h:960 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[95528,44131] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
2024-10-22 22:31:13.270369: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[95528,44131] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Unknown
['alveolar capillary type 2 endothelial cell', 'Unknown', 'capillary endothelial cell', 'endothelial cell of artery', 'vein endothelial cell', ..., 'muscle cell', 'ionocyte', 'plasmablast', 'erythrocyte', 'T follicular helper cell']
Length: 44
Categories (44, object): ['CD16-negative, CD56-bright natural killer cel..., 'CD16-positive, CD56-dim natural killer cell, ...,
                          'T cell', 'T follicular helper cell', ..., 'type I pneumocyte',
                          'type II pneumocyte', 'vein endothelial cell', 'Unknown']
19105
adam
Training scheme : training_scheme_13, warmup 5
adam
Epoch 1/6, Current strat Epoch 1/3
use_perm = True
switching perm
switching perm
13263320
15313/15284 - total loss: 0.5727 - classification loss: 1.4097 - dann loss: 4.1564 - reconstruction loss: 0.0201
Epoch 2/6, Current strat Epoch 2/3
use_perm = True
switching perm
switching perm
72776412
15313/15284 - total loss: 0.4389 - classification loss: 0.4938 - dann loss: 3.7423 - reconstruction loss: 0.0191
Epoch 3/6, Current strat Epoch 3/3
use_perm = True
switching perm
switching perm
36198043
15313/15284 - total loss: 0.4006 - classification loss: 0.2308 - dann loss: 3.6225 - reconstruction loss: 0.0191
adam
Epoch 4/6, Current strat Epoch 1/3
use_perm = False
switching perm
switching perm
7418823
15313/15284 - total loss: 0.0093 - classification loss: 0.0930 - dann loss: 3.5971 - reconstruction loss: 0.0193
Epoch 5/6, Current strat Epoch 2/3
use_perm = False
switching perm
switching perm
75303268
15313/15284 - total loss: 0.0047 - classification loss: 0.0467 - dann loss: 3.5951 - reconstruction loss: 0.0193
Epoch 6/6, Current strat Epoch 3/3
use_perm = False
switching perm
switching perm
98495864
15313/15284 - total loss: 0.0032 - classification loss: 0.0319 - dann loss: 3.5960 - reconstruction loss: 0.0193
Traceback (most recent call last):
  File "/home/becavin/scMusketeers/scmusketeers/__main__.py", line 112, in <module>
    run_sc_musketeers()
  File "/home/becavin/scMusketeers/scmusketeers/__main__.py", line 52, in run_sc_musketeers
    workflow.make_experiment()
  File "/home/becavin/scMusketeers/scmusketeers/transfer/optimize_model.py", line 551, in make_experiment
    enc, clas, dann, rec = self.dann_ae(
                           ^^^^^^^^^^^^^
  File "/data/analysis/ML_models/conda_env/cb_scmusketeers/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/becavin/scMusketeers/scmusketeers/tools/models.py", line 459, in call
    mean = self.ae_output_layer(dec_layer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Exception encountered when calling Dense.call().

[1m{{function_node __wrapped__MatMul_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[95528,44131] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:MatMul] name: [0m

Arguments received by Dense.call():
  • inputs=tf.Tensor(shape=(95528, 128), dtype=float32)
  • training=False
[neptune] [info   ] Shutting down background jobs, please wait a moment...
[neptune] [info   ] Done!
[neptune] [info   ] Waiting for the remaining 134 operations to synchronize with Neptune. Do not kill this process.
[neptune] [info   ] All 134 operations synced, thanks for waiting!
[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/becavin-lab/sc-musketeers/e/SCMUS-5/metadata
2024-11-20 12:19:46.245760: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-11-20 12:19:46.259498: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-11-20 12:19:46.275279: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-11-20 12:19:46.280199: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-20 12:19:46.293209: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-11-20 12:19:49.072498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14815 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:37:00.0, compute capability: 8.0
/home/becavin/scMusketeers/scmusketeers/transfer/dataset_tf.py:238: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.
  self.adata_train_extended.obs["train_split"] = spl.values
2024-11-20 12:20:25.809834: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-11-20 12:20:25.823791: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-11-20 12:20:25.839315: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-11-20 12:20:25.844265: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-20 12:20:25.858482: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-11-20 12:20:28.760443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14126 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:37:00.0, compute capability: 8.0
/home/becavin/scMusketeers/scmusketeers/transfer/dataset_tf.py:98: FutureWarning: Use anndata.concat instead of AnnData.concatenate, AnnData.concatenate is deprecated and will be removed in the future. See the tutorial for concat at: https://anndata.readthedocs.io/en/latest/concatenation.html
  adata = ref.concatenate(query, join="inner")
/home/becavin/scMusketeers/scmusketeers/transfer/dataset_tf.py:238: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.
  self.adata_train_extended.obs["train_split"] = spl.values
/data/analysis/ML_models/conda_env/cb_scmusketeers/bin
Use Neptune.ai log : False
Unknown
['Secretory N', 'Suprabasal N', 'Unknown', 'Basal', 'Multiciliated', ..., 'Ionocyte', 'Precursor', 'AT1', 'Brush cells', 'AT2']
Length: 29
Categories (29, object): ['AT1', 'AT2', 'B cells', 'Basal', ..., 'Smooth muscle', 'Suprabasal',
                          'Suprabasal N', 'Unknown']
4678
adam
Training scheme : training_scheme_13, warmup 100
adam
Epoch 1/150, Current strat Epoch 1/100
use_perm = True
switching perm
128/3742 - total loss: 0.6879 - classification loss: 3.4633 - dann loss: 2.6859 - reconstruction loss: 0.0912128/3742 - total loss: 0.6601 - classification loss: 3.0807 - dann loss: 2.7917 - reconstruction loss: 0.0910128/3742 - total loss: 0.6363 - classification loss: 2.8155 - dann loss: 2.8396 - reconstruction loss: 0.0885128/3742 - total loss: 0.6300 - classification loss: 2.6947 - dann loss: 2.9028 - reconstruction loss: 0.0878128/3742 - total loss: 0.6266 - classification loss: 2.5940 - dann loss: 2.9806 - reconstruction loss: 0.0865128/3742 - total loss: 0.6159 - classification loss: 2.4551 - dann loss: 3.0244 - reconstruction loss: 0.0849128/3742 - total loss: 0.6107 - classification loss: 2.3737 - dann loss: 3.0633 - reconstruction loss: 0.0837128/3742 - total loss: 0.6032 - classification loss: 2.2725 - dann loss: 3.0984 - reconstruction loss: 0.0826128/3742 - total loss: 0.5969 - classification loss: 2.1917 - dann loss: 3.1274 - reconstruction loss: 0.0812128/3742 - total loss: 0.5906 - classification loss: 2.1168 - dann loss: 3.1504 - reconstruction loss: 0.0799128/3742 - total loss: 0.5866 - classification loss: 2.0563 - dann loss: 3.1796 - reconstruction loss: 0.0788128/3742 - total loss: 0.5810 - classification loss: 1.9975 - dann loss: 3.1905 - reconstruction loss: 0.0777128/3742 - total loss: 0.5781 - classification loss: 1.9483 - dann loss: 3.2204 - reconstruction loss: 0.0765128/3742 - total loss: 0.5741 - classification loss: 1.9008 - dann loss: 3.2374 - reconstruction loss: 0.0753128/3742 - total loss: 0.5707 - classification loss: 1.8562 - dann loss: 3.2567 - reconstruction loss: 0.0743128/3742 - total loss: 0.5675 - classification loss: 1.8138 - dann loss: 3.2745 - reconstruction loss: 0.0733128/3742 - total loss: 0.5636 - classification loss: 1.7775 - dann loss: 3.2786 - reconstruction loss: 0.0725128/3742 - total loss: 0.5606 - classification loss: 1.7456 - dann loss: 3.2871 - reconstruction loss: 0.0716128/3742 - total loss: 0.5571 - classification loss: 1.7104 - dann loss: 3.2945 - reconstruction loss: 0.0707128/3742 - total loss: 0.5538 - classification loss: 1.6727 - dann loss: 3.3059 - reconstruction loss: 0.0700128/3742 - total loss: 0.5505 - classification loss: 1.6421 - dann loss: 3.3090 - reconstruction loss: 0.0692128/3742 - total loss: 0.5472 - classification loss: 1.6132 - dann loss: 3.3106 - reconstruction loss: 0.0685128/3742 - total loss: 0.5441 - classification loss: 1.5849 - dann loss: 3.3133 - reconstruction loss: 0.0678128/3742 - total loss: 0.5416 - classification loss: 1.5596 - dann loss: 3.3200 - reconstruction loss: 0.0671128/3742 - total loss: 0.5389 - classification loss: 1.5342 - dann loss: 3.3247 - reconstruction loss: 0.0663128/3742 - total loss: 0.5366 - classification loss: 1.5121 - dann loss: 3.3284 - reconstruction loss: 0.0656128/3742 - total loss: 0.5331 - classification loss: 1.4853 - dann loss: 3.3267 - reconstruction loss: 0.0649128/3742 - total loss: 0.5306 - classification loss: 1.4633 - dann loss: 3.3286 - reconstruction loss: 0.0642128/3742 - total loss: 0.5280 - classification loss: 1.4439 - dann loss: 3.3276 - reconstruction loss: 0.063529/3742 - total loss: 0.5257 - classification loss: 1.4205 - dann loss: 3.3331 - reconstruction loss: 0.06290/3742 - total loss: 0.5257 - classification loss: 1.4205 - dann loss: 3.3331 - reconstruction loss: 0.0629Epoch 2/150, Current strat Epoch 2/100
use_perm = True
switching perm
128/3742 - total loss: 0.4667 - classification loss: 0.7341 - dann loss: 3.5664 - reconstruction loss: 0.0458128/3742 - total loss: 0.4647 - classification loss: 0.7431 - dann loss: 3.5352 - reconstruction loss: 0.0460128/3742 - total loss: 0.4671 - classification loss: 0.7518 - dann loss: 3.5561 - reconstruction loss: 0.0454128/3742 - total loss: 0.4694 - classification loss: 0.7711 - dann loss: 3.5618 - reconstruction loss: 0.0451128/3742 - total loss: 0.4681 - classification loss: 0.7533 - dann loss: 3.5692 - reconstruction loss: 0.0448128/3742 - total loss: 0.4658 - classification loss: 0.7543 - dann loss: 3.5474 - reconstruction loss: 0.0445128/3742 - total loss: 0.4635 - classification loss: 0.7409 - dann loss: 3.5417 - reconstruction loss: 0.0440128/3742 - total loss: 0.4628 - classification loss: 0.7341 - dann loss: 3.5433 - reconstruction loss: 0.0439128/3742 - total loss: 0.4607 - classification loss: 0.7271 - dann loss: 3.5317 - reconstruction loss: 0.0436128/3742 - total loss: 0.4578 - classification loss: 0.7152 - dann loss: 3.5156 - reconstruction loss: 0.0435128/3742 - total loss: 0.4562 - classification loss: 0.7077 - dann loss: 3.5078 - reconstruction loss: 0.0433128/3742 - total loss: 0.4548 - classification loss: 0.6996 - dann loss: 3.5032 - reconstruction loss: 0.0432128/3742 - total loss: 0.4533 - classification loss: 0.6962 - dann loss: 3.4930 - reconstruction loss: 0.0429128/3742 - total loss: 0.4508 - classification loss: 0.6866 - dann loss: 3.4801 - reconstruction loss: 0.0426128/3742 - total loss: 0.4491 - classification loss: 0.6801 - dann loss: 3.4718 - reconstruction loss: 0.0424128/3742 - total loss: 0.4463 - classification loss: 0.6704 - dann loss: 3.4547 - reconstruction loss: 0.0422128/3742 - total loss: 0.4450 - classification loss: 0.6704 - dann loss: 3.4434 - reconstruction loss: 0.0421128/3742 - total loss: 0.4432 - classification loss: 0.6655 - dann loss: 3.4302 - reconstruction loss: 0.0420128/3742 - total loss: 0.4419 - classification loss: 0.6617 - dann loss: 3.4225 - reconstruction loss: 0.0419128/3742 - total loss: 0.4412 - classification loss: 0.6615 - dann loss: 3.4158 - reconstruction loss: 0.0418128/3742 - total loss: 0.4390 - classification loss: 0.6561 - dann loss: 3.4011 - reconstruction loss: 0.0416128/3742 - total loss: 0.4373 - classification loss: 0.6487 - dann loss: 3.3932 - reconstruction loss: 0.0414128/3742 - total loss: 0.4357 - classification loss: 0.6472 - dann loss: 3.3797 - reconstruction loss: 0.0413128/3742 - total loss: 0.4335 - classification loss: 0.6406 - dann loss: 3.3657 - reconstruction loss: 0.0411128/3742 - total loss: 0.4314 - classification loss: 0.6356 - dann loss: 3.3505 - reconstruction loss: 0.0409128/3742 - total loss: 0.4296 - classification loss: 0.6313 - dann loss: 3.3381 - reconstruction loss: 0.0409128/3742 - total loss: 0.4281 - classification loss: 0.6279 - dann loss: 3.3277 - reconstruction loss: 0.0407128/3742 - total loss: 0.4261 - classification loss: 0.6211 - dann loss: 3.3149 - reconstruction loss: 0.0406128/3742 - total loss: 0.4245 - classification loss: 0.6179 - dann loss: 3.3035 - reconstruction loss: 0.040529/3742 - total loss: 0.4231 - classification loss: 0.6156 - dann loss: 3.2917 - reconstruction loss: 0.04040/3742 - total loss: 0.4231 - classification loss: 0.6156 - dann loss: 3.2917 - reconstruction loss: 0.0404Epoch 3/150, Current strat Epoch 3/100
use_perm = True
switching perm
128/3742 - total loss: 0.3806 - classification loss: 0.4103 - dann loss: 3.1117 - reconstruction loss: 0.0355128/3742 - total loss: 0.3857 - classification loss: 0.4230 - dann loss: 3.1389 - reconstruction loss: 0.0369128/3742 - total loss: 0.3873 - classification loss: 0.4256 - dann loss: 3.1476 - reconstruction loss: 0.0374128/3742 - total loss: 0.3866 - classification loss: 0.4423 - dann loss: 3.1239 - reconstruction loss: 0.0375128/3742 - total loss: 0.3860 - classification loss: 0.4404 - dann loss: 3.1210 - reconstruction loss: 0.0373128/3742 - total loss: 0.3850 - classification loss: 0.4387 - dann loss: 3.1131 - reconstruction loss: 0.0372128/3742 - total loss: 0.3820 - classification loss: 0.4323 - dann loss: 3.0899 - reconstruction loss: 0.0372/data/analysis/ML_models/conda_env/cb_scmusketeers/bin
Use Neptune.ai log : False
Unknown
['Secretory N' 'Suprabasal N' 'Secretory' 'Basal' 'Multiciliated' 'Serous'
 'Endothelial' 'Multiciliated N' 'Suprabasal' 'Macrophage' 'Monocyte'
 'Fibroblast' 'SMG Goblet' 'Cycling Basal' 'LT/NK' 'Pericyte' 'B cells'
 'Mast cells' 'Deuterosomal' 'Dendritic' 'Smooth muscle' 'Plasma cells'
 'PNEC' 'Ionocyte' 'Precursor' 'Brush cells' 'AT1' 'AT2' 'Unknown']
4336
adam
Training scheme : training_scheme_13, warmup 100
adam
Epoch 1/150, Current strat Epoch 1/100
use_perm = True
switching perm
128/3811 - total loss: 0.6772 - classification loss: 3.4446 - dann loss: 2.5959 - reconstruction loss: 0.0914128/3811 - total loss: 0.6650 - classification loss: 3.1840 - dann loss: 2.7440 - reconstruction loss: 0.0902128/3811 - total loss: 0.6528 - classification loss: 3.0139 - dann loss: 2.7990 - reconstruction loss: 0.0894128/3811 - total loss: 0.6447 - classification loss: 2.8734 - dann loss: 2.8680 - reconstruction loss: 0.0882128/3811 - total loss: 0.6368 - classification loss: 2.7511 - dann loss: 2.9198 - reconstruction loss: 0.0871128/3811 - total loss: 0.6292 - classification loss: 2.6368 - dann loss: 2.9711 - reconstruction loss: 0.0855128/3811 - total loss: 0.6234 - classification loss: 2.5392 - dann loss: 3.0238 - reconstruction loss: 0.0839128/3811 - total loss: 0.6180 - classification loss: 2.4523 - dann loss: 3.0671 - reconstruction loss: 0.0826128/3811 - total loss: 0.6136 - classification loss: 2.3784 - dann loss: 3.1062 - reconstruction loss: 0.0814128/3811 - total loss: 0.6086 - classification loss: 2.3152 - dann loss: 3.1290 - reconstruction loss: 0.0803128/3811 - total loss: 0.6059 - classification loss: 2.2557 - dann loss: 3.1705 - reconstruction loss: 0.0790128/3811 - total loss: 0.6030 - classification loss: 2.2035 - dann loss: 3.2021 - reconstruction loss: 0.0780128/3811 - total loss: 0.5990 - classification loss: 2.1475 - dann loss: 3.2270 - reconstruction loss: 0.0769128/3811 - total loss: 0.5955 - classification loss: 2.0997 - dann loss: 3.2479 - reconstruction loss: 0.0759128/3811 - total loss: 0.5925 - classification loss: 2.0615 - dann loss: 3.2635 - reconstruction loss: 0.0750128/3811 - total loss: 0.5891 - classification loss: 2.0196 - dann loss: 3.2781 - reconstruction loss: 0.0742128/3811 - total loss: 0.5853 - classification loss: 1.9814 - dann loss: 3.2860 - reconstruction loss: 0.0732128/3811 - total loss: 0.5817 - classification loss: 1.9429 - dann loss: 3.2969 - reconstruction loss: 0.0722128/3811 - total loss: 0.5782 - classification loss: 1.9054 - dann loss: 3.3051 - reconstruction loss: 0.0714128/3811 - total loss: 0.5748 - classification loss: 1.8760 - dann loss: 3.3071 - reconstruction loss: 0.0706128/3811 - total loss: 0.5714 - classification loss: 1.8382 - dann loss: 3.3166 - reconstruction loss: 0.0698128/3811 - total loss: 0.5682 - classification loss: 1.8063 - dann loss: 3.3234 - reconstruction loss: 0.0690128/3811 - total loss: 0.5650 - classification loss: 1.7748 - dann loss: 3.3282 - reconstruction loss: 0.0684128/3811 - total loss: 0.5621 - classification loss: 1.7442 - dann loss: 3.3359 - reconstruction loss: 0.0676128/3811 - total loss: 0.5589 - classification loss: 1.7143 - dann loss: 3.3390 - reconstruction loss: 0.0669128/3811 - total loss: 0.5562 - classification loss: 1.6904 - dann loss: 3.3424 - reconstruction loss: 0.0661128/3811 - total loss: 0.5532 - classification loss: 1.6620 - dann loss: 3.3461 - reconstruction loss: 0.0654128/3811 - total loss: 0.5499 - classification loss: 1.6357 - dann loss: 3.3453 - reconstruction loss: 0.0648128/3811 - total loss: 0.5472 - classification loss: 1.6120 - dann loss: 3.3470 - reconstruction loss: 0.064298/3811 - total loss: 0.5448 - classification loss: 1.5905 - dann loss: 3.3486 - reconstruction loss: 0.06360/3811 - total loss: 0.5448 - classification loss: 1.5905 - dann loss: 3.3486 - reconstruction loss: 0.0636Epoch 2/150, Current strat Epoch 2/100
use_perm = True
switching perm
128/3811 - total loss: 0.4636 - classification loss: 0.7855 - dann loss: 3.4798 - reconstruction loss: 0.0463128/3811 - total loss: 0.4655 - classification loss: 0.7737 - dann loss: 3.5117 - reconstruction loss: 0.0462128/3811 - total loss: 0.4624 - classification loss: 0.7605 - dann loss: 3.4972 - reconstruction loss: 0.0458128/3811 - total loss: 0.4603 - classification loss: 0.7874 - dann loss: 3.4475 - reconstruction loss: 0.0460128/3811 - total loss: 0.4576 - classification loss: 0.7841 - dann loss: 3.4285 - reconstruction loss: 0.0455128/3811 - total loss: 0.4535 - classification loss: 0.7686 - dann loss: 3.4047 - reconstruction loss: 0.0453128/3811 - total loss: 0.4541 - classification loss: 0.7747 - dann loss: 3.4076 - reconstruction loss: 0.0449128/3811 - total loss: 0.4513 - classification loss: 0.7616 - dann loss: 3.3947 - reconstruction loss: 0.0446128/3811 - total loss: 0.4482 - classification loss: 0.7533 - dann loss: 3.3742 - reconstruction loss: 0.0444128/3811 - total loss: 0.4448 - classification loss: 0.7487 - dann loss: 3.3461 - reconstruction loss: 0.0441128/3811 - total loss: 0.4412 - classification loss: 0.7366 - dann loss: 3.3242 - reconstruction loss: 0.0439128/3811 - total loss: 0.4381 - classification loss: 0.7322 - dann loss: 3.3002 - reconstruction loss: 0.0436128/3811 - total loss: 0.4347 - classification loss: 0.7238 - dann loss: 3.2758 - reconstruction loss: 0.0434128/3811 - total loss: 0.4312 - classification loss: 0.7217 - dann loss: 3.2449 - reconstruction loss: 0.0432128/3811 - total loss: 0.4291 - classification loss: 0.7160 - dann loss: 3.2320 - reconstruction loss: 0.0429128/3811 - total loss: 0.4265 - classification loss: 0.7079 - dann loss: 3.2156 - reconstruction loss: 0.0427128/3811 - total loss: 0.4239 - classification loss: 0.6969 - dann loss: 3.2018 - reconstruction loss: 0.0425128/3811 - total loss: 0.4215 - classification loss: 0.6890 - dann loss: 3.1871 - reconstruction loss: 0.0424128/3811 - total loss: 0.4184 - classification loss: 0.6790 - dann loss: 3.1677 - reconstruction loss: 0.0421128/3811 - total loss: 0.4154 - classification loss: 0.6727 - dann loss: 3.1456 - reconstruction loss: 0.0419128/3811 - total loss: 0.4131 - classification loss: 0.6667 - dann loss: 3.1309 - reconstruction loss: 0.0417128/3811 - total loss: 0.4107 - classification loss: 0.6621 - dann loss: 3.1123 - reconstruction loss: 0.0416128/3811 - total loss: 0.4081 - classification loss: 0.6567 - dann loss: 3.0930 - reconstruction loss: 0.0414128/3811 - total loss: 0.4059 - classification loss: 0.6527 - dann loss: 3.0759 - reconstruction loss: 0.0413128/3811 - total loss: 0.4036 - classification loss: 0.6491 - dann loss: 3.0585 - reconstruction loss: 0.0411128/3811 - total loss: 0.4015 - classification loss: 0.6414 - dann loss: 3.0461 - reconstruction loss: 0.0410128/3811 - total loss: 0.4000 - classification loss: 0.6410 - dann loss: 3.0313 - reconstruction loss: 0.0409128/3811 - total loss: 0.3976 - classification loss: 0.6338 - dann loss: 3.0156 - reconstruction loss: 0.0408128/3811 - total loss: 0.3955 - classification loss: 0.6285 - dann loss: 3.0000 - reconstruction loss: 0.040898/3811 - total loss: 0.3938 - classification loss: 0.6253 - dann loss: 2.9875 - reconstruction loss: 0.04060/3811 - total loss: 0.3938 - classification loss: 0.6253 - dann loss: 2.9875 - reconstruction loss: 0.0406Epoch 3/150, Current strat Epoch 3/100
use_perm = True
switching perm
128/3811 - total loss: 0.3392 - classification loss: 0.4023 - dann loss: 2.6984 - reconstruction loss: 0.0364128/3811 - total loss: 0.3387 - classification loss: 0.4181 - dann loss: 2.6741 - reconstruction loss: 0.0368128/3811 - total loss: 0.3393 - classification loss: 0.4310 - dann loss: 2.6654 - reconstruction loss: 0.0371128/3811 - total loss: 0.3377 - classification loss: 0.4166 - dann loss: 2.6661 - reconstruction loss: 0.0368128/3811 - total loss: 0.3342 - classification loss: 0.3994 - dann loss: 2.6473 - reconstruction loss: 0.0369128/3811 - total loss: 0.3315 - classification loss: 0.3949 - dann loss: 2.6266 - reconstruction loss: 0.0367128/3811 - total loss: 0.3318 - classification loss: 0.3953 - dann loss: 2.6267 - reconstruction loss: 0.0370128/3811 - total loss: 0.3316 - classification loss: 0.3910 - dann loss: 2.6310 - reconstruction loss: 0.0367128/3811 - total loss: 0.3307 - classification loss: 0.3897 - dann loss: 2.6240 - reconstruction loss: 0.0367128/3811 - total loss: 0.3299 - classification loss: 0.3921 - dann loss: 2.6130 - reconstruction loss: 0.0367128/3811 - total loss: 0.3289 - classification loss: 0.3941 - dann loss: 2.6008 - reconstruction loss: 0.0367128/3811 - total loss: 0.3272 - classification loss: 0.3927 - dann loss: 2.5869 - reconstruction loss: 0.0366128/3811 - total loss: 0.3264 - classification loss: 0.3970 - dann loss: 2.5734 - reconstruction loss: 0.0367128/3811 - total loss: 0.3258 - classification loss: 0.3986 - dann loss: 2.5659 - reconstruction loss: 0.0367128/3811 - total loss: 0.3255 - classification loss: 0.3978 - dann loss: 2.5634 - reconstruction loss: 0.0367128/3811 - total loss: 0.3244 - classification loss: 0.3928 - dann loss: 2.5573 - reconstruction loss: 0.0367128/3811 - total loss: 0.3235 - classification loss: 0.3959 - dann loss: 2.5453 - reconstruction loss: 0.0367128/3811 - total loss: 0.3221 - classification loss: 0.3944 - dann loss: 2.5331 - reconstruction loss: 0.0367128/3811 - total loss: 0.3216 - classification loss: 0.3935 - dann loss: 2.5284 - reconstruction loss: 0.0367128/3811 - total loss: 0.3204 - classification loss: 0.3922 - dann loss: 2.5182 - reconstruction loss: 0.0366128/3811 - total loss: 0.3194 - classification loss: 0.3882 - dann loss: 2.5128 - reconstruction loss: 0.0367128/3811 - total loss: 0.3177 - classification loss: 0.3839 - dann loss: 2.4999 - reconstruction loss: 0.0367128/3811 - total loss: 0.3171 - classification loss: 0.3835 - dann loss: 2.4943 - reconstruction loss: 0.0367128/3811 - total loss: 0.3166 - classification loss: 0.3820 - dann loss: 2.4898 - reconstruction loss: 0.0367128/3811 - total loss: 0.3154 - classification loss: 0.3799 - dann loss: 2.4802 - reconstruction loss: 0.0367128/3811 - total loss: 0.3148 - classification loss: 0.3803 - dann loss: 2.4743 - reconstruction loss: 0.0367128/3811 - total loss: 0.3144 - classification loss: 0.3778 - dann loss: 2.4724 - reconstruction loss: 0.0367128/3811 - total loss: 0.3139 - classification loss: 0.3771 - dann loss: 2.4690 - reconstruction loss: 0.0366128/3811 - total loss: 0.3130 - classification loss: 0.3729 - dann loss: 2.4635 - reconstruction loss: 0.036798/3811 - total loss: 0.3128 - classification loss: 0.3757 - dann loss: 2.4591 - reconstruction loss: 0.03670/3811 - total loss: 0.3128 - classification loss: 0.3757 - dann loss: 2.4591 - reconstruction loss: 0.0367Epoch 4/150, Current strat Epoch 4/100
use_perm = True
switching perm
128/3811 - total loss: 0.3036 - classification loss: 0.3121 - dann loss: 2.4265 - reconstruction loss: 0.0372128/3811 - total loss: 0.3013 - classification loss: 0.3057 - dann loss: 2.4128 - reconstruction loss: 0.0368128/3811 - total loss: 0.3002 - classification loss: 0.3044 - dann loss: 2.4032 - reconstruction loss: 0.0368128/3811 - total loss: 0.2986 - classification loss: 0.2987 - dann loss: 2.3943 - reconstruction loss: 0.0367128/3811 - total loss: 0.2994 - classification loss: 0.3101 - dann loss: 2.3930 - reconstruction loss: 0.0364128/3811 - total loss: 0.3002 - classification loss: 0.3119 - dann loss: 2.4000 - reconstruction loss: 0.0362128/3811 - total loss: 0.2983 - classification loss: 0.3008 - dann loss: 2.3943 - reconstruction loss: 0.0360128/3811 - total loss: 0.2987 - classification loss: 0.2966 - dann loss: 2.4021 - reconstruction loss: 0.0360128/3811 - total loss: 0.2983 - classification loss: 0.2909 - dann loss: 2.4033 - reconstruction loss: 0.0360128/3811 - total loss: 0.2986 - classification loss: 0.2973 - dann loss: 2.4002 - reconstruction loss: 0.0360128/3811 - total loss: 0.2983 - classification loss: 0.2946 - dann loss: 2.4008 - reconstruction loss: 0.0360128/3811 - total loss: 0.2984 - classification loss: 0.2901 - dann loss: 2.4065 - reconstruction loss: 0.0359128/3811 - total loss: 0.2992 - classification loss: 0.2875 - dann loss: 2.4176 - reconstruction loss: 0.0359128/3811 - total loss: 0.2979 - classification loss: 0.2826 - dann loss: 2.4101 - reconstruction loss: 0.0358128/3811 - total loss: 0.2986 - classification loss: 0.2828 - dann loss: 2.4166 - reconstruction loss: 0.0358128/3811 - total loss: 0.2979 - classification loss: 0.2811 - dann loss: 2.4121 - reconstruction loss: 0.0357128/3811 - total loss: 0.2975 - classification loss: 0.2847 - dann loss: 2.4045 - reconstruction loss: 0.0358128/3811 - total loss: 0.2974 - classification loss: 0.2859 - dann loss: 2.4025 - reconstruction loss: 0.0357128/3811 - total loss: 0.2976 - classification loss: 0.2846 - dann loss: 2.4048 - reconstruction loss: 0.0358128/3811 - total loss: 0.2970 - classification loss: 0.2803 - dann loss: 2.4022 - reconstruction loss: 0.0359128/3811 - total loss: 0.2963 - classification loss: 0.2779 - dann loss: 2.3982 - reconstruction loss: 0.0359128/3811 - total loss: 0.2959 - classification loss: 0.2758 - dann loss: 2.3955 - reconstruction loss: 0.0360128/3811 - total loss: 0.2959 - classification loss: 0.2723 - dann loss: 2.3986 - reconstruction loss: 0.0360128/3811 - total loss: 0.2954 - classification loss: 0.2700 - dann loss: 2.3961 - reconstruction loss: 0.0359128/3811 - total loss: 0.2950 - classification loss: 0.2695 - dann loss: 2.3919 - reconstruction loss: 0.0360128/3811 - total loss: 0.2950 - classification loss: 0.2716 - dann loss: 2.3900 - reconstruction loss: 0.0361128/3811 - total loss: 0.2944 - classification loss: 0.2677 - dann loss: 2.3876 - reconstruction loss: 0.0360128/3811 - total loss: 0.2940 - classification loss: 0.2658 - dann loss: 2.3863 - reconstruction loss: 0.0360128/3811 - total loss: 0.2940 - classification loss: 0.2652 - dann loss: 2.3865 - reconstruction loss: 0.036098/3811 - total loss: 0.2934 - classification loss: 0.2638 - dann loss: 2.3821 - reconstruction loss: 0.03610/3811 - total loss: 0.2934 - classification loss: 0.2638 - dann loss: 2.3821 - reconstruction loss: 0.0361Epoch 5/150, Current strat Epoch 5/100
use_perm = True
switching perm
128/3811 - total loss: 0.2947 - classification loss: 0.1367 - dann loss: 2.5158 - reconstruction loss: 0.0368128/3811 - total loss: 0.2869 - classification loss: 0.1658 - dann loss: 2.4072 - reconstruction loss: 0.0370128/3811 - total loss: 0.2950 - classification loss: 0.2038 - dann loss: 2.4523 - reconstruction loss: 0.0367128/3811 - total loss: 0.2914 - classification loss: 0.2011 - dann loss: 2.4208 - reconstruction loss: 0.0365128/3811 - total loss: 0.2914 - classification loss: 0.1986 - dann loss: 2.4238 - reconstruction loss: 0.0365128/3811 - total loss: 0.2906 - classification loss: 0.1953 - dann loss: 2.4203 - reconstruction loss: 0.0363128/3811 - total loss: 0.2909 - classification loss: 0.1919 - dann loss: 2.4285 - reconstruction loss: 0.0361128/3811 - total loss: 0.2913 - classification loss: 0.1979 - dann loss: 2.4271 - reconstruction loss: 0.0360128/3811 - total loss: 0.2915 - classification loss: 0.2022 - dann loss: 2.4247 - reconstruction loss: 0.0360128/3811 - total loss: 0.2911 - classification loss: 0.2011 - dann loss: 2.4238 - reconstruction loss: 0.0358128/3811 - total loss: 0.2905 - classification loss: 0.1992 - dann loss: 2.4214 - reconstruction loss: 0.0356128/3811 - total loss: 0.2900 - classification loss: 0.1957 - dann loss: 2.4202 - reconstruction loss: 0.0355128/3811 - total loss: 0.2901 - classification loss: 0.1932 - dann loss: 2.4232 - reconstruction loss: 0.0356128/3811 - total loss: 0.2900 - classification loss: 0.1959 - dann loss: 2.4195 - reconstruction loss: 0.0355128/3811 - total loss: 0.2895 - classification loss: 0.1959 - dann loss: 2.4149 - reconstruction loss: 0.0355128/3811 - total loss: 0.2894 - classification loss: 0.1947 - dann loss: 2.4151 - reconstruction loss: 0.0355128/3811 - total loss: 0.2890 - classification loss: 0.1948 - dann loss: 2.4101 - reconstruction loss: 0.0356128/3811 - total loss: 0.2883 - classification loss: 0.1941 - dann loss: 2.4044 - reconstruction loss: 0.0356128/3742 - total loss: 0.3793 - classification loss: 0.4150 - dann loss: 3.0809 - reconstruction loss: 0.0372128/3742 - total loss: 0.3769 - classification loss: 0.4057 - dann loss: 3.0669 - reconstruction loss: 0.0370128/3742 - total loss: 0.3748 - classification loss: 0.3947 - dann loss: 3.0571 - reconstruction loss: 0.0371128/3742 - total loss: 0.3735 - classification loss: 0.3916 - dann loss: 3.0460 - reconstruction loss: 0.0372128/3742 - total loss: 0.3733 - classification loss: 0.3940 - dann loss: 3.0413 - reconstruction loss: 0.0372128/3742 - total loss: 0.3731 - classification loss: 0.4010 - dann loss: 3.0329 - reconstruction loss: 0.0372128/3742 - total loss: 0.3725 - classification loss: 0.4002 - dann loss: 3.0263 - reconstruction loss: 0.0373128/3742 - total loss: 0.3718 - classification loss: 0.3936 - dann loss: 3.0258 - reconstruction loss: 0.0373128/3742 - total loss: 0.3700 - classification loss: 0.3886 - dann loss: 3.0135 - reconstruction loss: 0.0373128/3742 - total loss: 0.3687 - classification loss: 0.3859 - dann loss: 3.0038 - reconstruction loss: 0.0371128/3742 - total loss: 0.3676 - classification loss: 0.3841 - dann loss: 2.9958 - reconstruction loss: 0.0371128/3742 - total loss: 0.3670 - classification loss: 0.3847 - dann loss: 2.9889 - reconstruction loss: 0.0370128/3742 - total loss: 0.3658 - classification loss: 0.3806 - dann loss: 2.9814 - reconstruction loss: 0.0370128/3742 - total loss: 0.3637 - classification loss: 0.3760 - dann loss: 2.9659 - reconstruction loss: 0.0369128/3742 - total loss: 0.3632 - classification loss: 0.3751 - dann loss: 2.9617 - reconstruction loss: 0.0369128/3742 - total loss: 0.3623 - classification loss: 0.3743 - dann loss: 2.9545 - reconstruction loss: 0.0368128/3742 - total loss: 0.3611 - classification loss: 0.3713 - dann loss: 2.9451 - reconstruction loss: 0.0368128/3742 - total loss: 0.3600 - classification loss: 0.3693 - dann loss: 2.9355 - reconstruction loss: 0.0369128/3742 - total loss: 0.3591 - classification loss: 0.3673 - dann loss: 2.9287 - reconstruction loss: 0.0369128/3742 - total loss: 0.3580 - classification loss: 0.3659 - dann loss: 2.9187 - reconstruction loss: 0.0369128/3742 - total loss: 0.3571 - classification loss: 0.3644 - dann loss: 2.9122 - reconstruction loss: 0.0368128/3742 - total loss: 0.3563 - classification loss: 0.3633 - dann loss: 2.9057 - reconstruction loss: 0.036729/3742 - total loss: 0.3564 - classification loss: 0.3670 - dann loss: 2.9028 - reconstruction loss: 0.03670/3742 - total loss: 0.3564 - classification loss: 0.3670 - dann loss: 2.9028 - reconstruction loss: 0.0367Epoch 4/150, Current strat Epoch 4/100
use_perm = True
switching perm
128/3742 - total loss: 0.3444 - classification loss: 0.3284 - dann loss: 2.8105 - reconstruction loss: 0.0381128/3742 - total loss: 0.3481 - classification loss: 0.3352 - dann loss: 2.8463 - reconstruction loss: 0.0374128/3742 - total loss: 0.3460 - classification loss: 0.3388 - dann loss: 2.8257 - reconstruction loss: 0.0369128/3742 - total loss: 0.3420 - classification loss: 0.3323 - dann loss: 2.7921 - reconstruction loss: 0.0369128/3742 - total loss: 0.3439 - classification loss: 0.3229 - dann loss: 2.8236 - reconstruction loss: 0.0366128/3742 - total loss: 0.3430 - classification loss: 0.3157 - dann loss: 2.8211 - reconstruction loss: 0.0366128/3742 - total loss: 0.3416 - classification loss: 0.3061 - dann loss: 2.8194 - reconstruction loss: 0.0364128/3742 - total loss: 0.3380 - classification loss: 0.2994 - dann loss: 2.7917 - reconstruction loss: 0.0361128/3742 - total loss: 0.3365 - classification loss: 0.2935 - dann loss: 2.7832 - reconstruction loss: 0.0361128/3742 - total loss: 0.3355 - classification loss: 0.2880 - dann loss: 2.7807 - reconstruction loss: 0.0358128/3742 - total loss: 0.3335 - classification loss: 0.2833 - dann loss: 2.7666 - reconstruction loss: 0.0357128/3742 - total loss: 0.3333 - classification loss: 0.2870 - dann loss: 2.7596 - reconstruction loss: 0.0358128/3742 - total loss: 0.3328 - classification loss: 0.2823 - dann loss: 2.7597 - reconstruction loss: 0.0357128/3742 - total loss: 0.3332 - classification loss: 0.2819 - dann loss: 2.7645 - reconstruction loss: 0.0357128/3742 - total loss: 0.3345 - classification loss: 0.2891 - dann loss: 2.7683 - reconstruction loss: 0.0360128/3742 - total loss: 0.3339 - classification loss: 0.2858 - dann loss: 2.7659 - reconstruction loss: 0.0359128/3742 - total loss: 0.3334 - classification loss: 0.2849 - dann loss: 2.7618 - reconstruction loss: 0.0359128/3742 - total loss: 0.3324 - classification loss: 0.2812 - dann loss: 2.7551 - reconstruction loss: 0.0360128/3742 - total loss: 0.3315 - classification loss: 0.2757 - dann loss: 2.7515 - reconstruction loss: 0.0359128/3742 - total loss: 0.3308 - classification loss: 0.2755 - dann loss: 2.7448 - reconstruction loss: 0.0359128/3742 - total loss: 0.3298 - classification loss: 0.2742 - dann loss: 2.7371 - reconstruction loss: 0.0359128/3742 - total loss: 0.3291 - classification loss: 0.2725 - dann loss: 2.7313 - reconstruction loss: 0.0359128/3742 - total loss: 0.3289 - classification loss: 0.2732 - dann loss: 2.7289 - reconstruction loss: 0.0359128/3742 - total loss: 0.3284 - classification loss: 0.2735 - dann loss: 2.7227 - reconstruction loss: 0.0360128/3742 - total loss: 0.3288 - classification loss: 0.2762 - dann loss: 2.7239 - reconstruction loss: 0.0360128/3742 - total loss: 0.3283 - classification loss: 0.2748 - dann loss: 2.7201 - reconstruction loss: 0.0360128/3742 - total loss: 0.3278 - classification loss: 0.2734 - dann loss: 2.7156 - reconstruction loss: 0.0361128/3742 - total loss: 0.3273 - classification loss: 0.2721 - dann loss: 2.7128 - reconstruction loss: 0.0360128/3742 - total loss: 0.3268 - classification loss: 0.2732 - dann loss: 2.7062 - reconstruction loss: 0.036029/3742 - total loss: 0.3262 - classification loss: 0.2719 - dann loss: 2.7012 - reconstruction loss: 0.03610/3742 - total loss: 0.3262 - classification loss: 0.2719 - dann loss: 2.7012 - reconstruction loss: 0.0361Epoch 5/150, Current strat Epoch 5/100
use_perm = True
switching perm
128/3742 - total loss: 0.3344 - classification loss: 0.2422 - dann loss: 2.8114 - reconstruction loss: 0.0363128/3742 - total loss: 0.3314 - classification loss: 0.2630 - dann loss: 2.7636 - reconstruction loss: 0.0359128/3742 - total loss: 0.3295 - classification loss: 0.2581 - dann loss: 2.7508 - reconstruction loss: 0.0357128/3742 - total loss: 0.3250 - classification loss: 0.2386 - dann loss: 2.7277 - reconstruction loss: 0.0355128/3742 - total loss: 0.3231 - classification loss: 0.2339 - dann loss: 2.7133 - reconstruction loss: 0.0355128/3742 - total loss: 0.3222 - classification loss: 0.2306 - dann loss: 2.7037 - reconstruction loss: 0.0359128/3742 - total loss: 0.3224 - classification loss: 0.2289 - dann loss: 2.7073 - reconstruction loss: 0.0360128/3742 - total loss: 0.3221 - classification loss: 0.2240 - dann loss: 2.7093 - reconstruction loss: 0.0360128/3742 - total loss: 0.3215 - classification loss: 0.2233 - dann loss: 2.7031 - reconstruction loss: 0.0361128/3742 - total loss: 0.3206 - classification loss: 0.2206 - dann loss: 2.6974 - reconstruction loss: 0.0360128/3742 - total loss: 0.3208 - classification loss: 0.2169 - dann loss: 2.7026 - reconstruction loss: 0.0360128/3742 - total loss: 0.3200 - classification loss: 0.2164 - dann loss: 2.6957 - reconstruction loss: 0.0360128/3742 - total loss: 0.3199 - classification loss: 0.2150 - dann loss: 2.6961 - reconstruction loss: 0.0360128/3742 - total loss: 0.3197 - classification loss: 0.2131 - dann loss: 2.6966 - reconstruction loss: 0.0359128/3742 - total loss: 0.3201 - classification loss: 0.2161 - dann loss: 2.6971 - reconstruction loss: 0.0359128/3742 - total loss: 0.3196 - classification loss: 0.2139 - dann loss: 2.6956 - reconstruction loss: 0.0358128/3742 - total loss: 0.3191 - classification loss: 0.2126 - dann loss: 2.6915 - reconstruction loss: 0.0358128/3742 - total loss: 0.3187 - classification loss: 0.2124 - dann loss: 2.6880 - reconstruction loss: 0.0358128/3742 - total loss: 0.3178 - classification loss: 0.2120 - dann loss: 2.6797 - reconstruction loss: 0.0358128/3811 - total loss: 0.2883 - classification loss: 0.1948 - dann loss: 2.4027 - reconstruction loss: 0.0357128/3811 - total loss: 0.2889 - classification loss: 0.1985 - dann loss: 2.4053 - reconstruction loss: 0.0357128/3811 - total loss: 0.2888 - classification loss: 0.1967 - dann loss: 2.4056 - reconstruction loss: 0.0357128/3811 - total loss: 0.2886 - classification loss: 0.1949 - dann loss: 2.4057 - reconstruction loss: 0.0357128/3811 - total loss: 0.2880 - classification loss: 0.1923 - dann loss: 2.4025 - reconstruction loss: 0.0357128/3811 - total loss: 0.2880 - classification loss: 0.1914 - dann loss: 2.4028 - reconstruction loss: 0.0358128/3811 - total loss: 0.2880 - classification loss: 0.1928 - dann loss: 2.4013 - reconstruction loss: 0.0357128/3811 - total loss: 0.2878 - classification loss: 0.1929 - dann loss: 2.3980 - reconstruction loss: 0.0358128/3811 - total loss: 0.2875 - classification loss: 0.1914 - dann loss: 2.3967 - reconstruction loss: 0.0359128/3811 - total loss: 0.2877 - classification loss: 0.1903 - dann loss: 2.3996 - reconstruction loss: 0.0359128/3811 - total loss: 0.2875 - classification loss: 0.1887 - dann loss: 2.3990 - reconstruction loss: 0.035998/3811 - total loss: 0.2869 - classification loss: 0.1891 - dann loss: 2.3932 - reconstruction loss: 0.03590/3811 - total loss: 0.2869 - classification loss: 0.1891 - dann loss: 2.3932 - reconstruction loss: 0.0359Epoch 6/150, Current strat Epoch 6/100
use_perm = True
switching perm
128/3811 - total loss: 0.3103 - classification loss: 0.1640 - dann loss: 2.6446 - reconstruction loss: 0.0368128/3811 - total loss: 0.2976 - classification loss: 0.1494 - dann loss: 2.5364 - reconstruction loss: 0.0363128/3811 - total loss: 0.2956 - classification loss: 0.1466 - dann loss: 2.5271 - reconstruction loss: 0.0353128/3811 - total loss: 0.2957 - classification loss: 0.1479 - dann loss: 2.5236 - reconstruction loss: 0.0357128/3811 - total loss: 0.2947 - classification loss: 0.1506 - dann loss: 2.5091 - reconstruction loss: 0.0359128/3811 - total loss: 0.2961 - classification loss: 0.1551 - dann loss: 2.5187 - reconstruction loss: 0.0358128/3811 - total loss: 0.2945 - classification loss: 0.1540 - dann loss: 2.5035 - reconstruction loss: 0.0359128/3811 - total loss: 0.2946 - classification loss: 0.1594 - dann loss: 2.5003 - reconstruction loss: 0.0358128/3811 - total loss: 0.2948 - classification loss: 0.1580 - dann loss: 2.5026 - reconstruction loss: 0.0360128/3811 - total loss: 0.2939 - classification loss: 0.1600 - dann loss: 2.4926 - reconstruction loss: 0.0358128/3811 - total loss: 0.2940 - classification loss: 0.1628 - dann loss: 2.4908 - reconstruction loss: 0.0359128/3811 - total loss: 0.2940 - classification loss: 0.1648 - dann loss: 2.4889 - reconstruction loss: 0.0358128/3811 - total loss: 0.2933 - classification loss: 0.1628 - dann loss: 2.4834 - reconstruction loss: 0.0358128/3811 - total loss: 0.2934 - classification loss: 0.1623 - dann loss: 2.4862 - reconstruction loss: 0.0357128/3811 - total loss: 0.2933 - classification loss: 0.1632 - dann loss: 2.4837 - reconstruction loss: 0.0358128/3811 - total loss: 0.2931 - classification loss: 0.1626 - dann loss: 2.4818 - reconstruction loss: 0.0358128/3811 - total loss: 0.2928 - classification loss: 0.1609 - dann loss: 2.4800 - reconstruction loss: 0.0359128/3811 - total loss: 0.2927 - classification loss: 0.1608 - dann loss: 2.4791 - reconstruction loss: 0.0359128/3811 - total loss: 0.2921 - classification loss: 0.1580 - dann loss: 2.4760 - reconstruction loss: 0.0359128/3811 - total loss: 0.2912 - classification loss: 0.1571 - dann loss: 2.4681 - reconstruction loss: 0.0359128/3811 - total loss: 0.2911 - classification loss: 0.1564 - dann loss: 2.4666 - reconstruction loss: 0.0360128/3811 - total loss: 0.2908 - classification loss: 0.1550 - dann loss: 2.4658 - reconstruction loss: 0.0359128/3811 - total loss: 0.2907 - classification loss: 0.1549 - dann loss: 2.4643 - reconstruction loss: 0.0359128/3811 - total loss: 0.2904 - classification loss: 0.1560 - dann loss: 2.4609 - reconstruction loss: 0.0359128/3811 - total loss: 0.2904 - classification loss: 0.1560 - dann loss: 2.4611 - reconstruction loss: 0.0359128/3811 - total loss: 0.2900 - classification loss: 0.1550 - dann loss: 2.4582 - reconstruction loss: 0.0359128/3811 - total loss: 0.2895 - classification loss: 0.1532 - dann loss: 2.4552 - reconstruction loss: 0.0358128/3811 - total loss: 0.2895 - classification loss: 0.1529 - dann loss: 2.4555 - reconstruction loss: 0.0358128/3811 - total loss: 0.2893 - classification loss: 0.1523 - dann loss: 2.4539 - reconstruction loss: 0.035898/3811 - total loss: 0.2896 - classification loss: 0.1540 - dann loss: 2.4558 - reconstruction loss: 0.03580/3811 - total loss: 0.2896 - classification loss: 0.1540 - dann loss: 2.4558 - reconstruction loss: 0.0358Epoch 7/150, Current strat Epoch 7/100
use_perm = True
switching perm
128/3811 - total loss: 0.2933 - classification loss: 0.1116 - dann loss: 2.5163 - reconstruction loss: 0.0381128/3811 - total loss: 0.2898 - classification loss: 0.1127 - dann loss: 2.4853 - reconstruction loss: 0.0376128/3811 - total loss: 0.2894 - classification loss: 0.1176 - dann loss: 2.4898 - reconstruction loss: 0.0359128/3811 - total loss: 0.2884 - classification loss: 0.1129 - dann loss: 2.4861 - reconstruction loss: 0.0356128/3811 - total loss: 0.2878 - classification loss: 0.1185 - dann loss: 2.4757 - reconstruction loss: 0.0355128/3811 - total loss: 0.2906 - classification loss: 0.1277 - dann loss: 2.4924 - reconstruction loss: 0.0357128/3811 - total loss: 0.2895 - classification loss: 0.1258 - dann loss: 2.4811 - reconstruction loss: 0.0359128/3811 - total loss: 0.2895 - classification loss: 0.1254 - dann loss: 2.4832 - reconstruction loss: 0.0358128/3811 - total loss: 0.2901 - classification loss: 0.1275 - dann loss: 2.4877 - reconstruction loss: 0.0358128/3811 - total loss: 0.2885 - classification loss: 0.1252 - dann loss: 2.4755 - reconstruction loss: 0.0355128/3811 - total loss: 0.2877 - classification loss: 0.1239 - dann loss: 2.4689 - reconstruction loss: 0.0355128/3811 - total loss: 0.2880 - classification loss: 0.1262 - dann loss: 2.4689 - reconstruction loss: 0.0356128/3811 - total loss: 0.2886 - classification loss: 0.1276 - dann loss: 2.4741 - reconstruction loss: 0.0356128/3811 - total loss: 0.2878 - classification loss: 0.1261 - dann loss: 2.4666 - reconstruction loss: 0.0357128/3811 - total loss: 0.2877 - classification loss: 0.1262 - dann loss: 2.4651 - reconstruction loss: 0.0358128/3811 - total loss: 0.2878 - classification loss: 0.1303 - dann loss: 2.4609 - reconstruction loss: 0.0358128/3811 - total loss: 0.2876 - classification loss: 0.1301 - dann loss: 2.4598 - reconstruction loss: 0.0357128/3811 - total loss: 0.2872 - classification loss: 0.1288 - dann loss: 2.4582 - reconstruction loss: 0.0356128/3811 - total loss: 0.2871 - classification loss: 0.1275 - dann loss: 2.4580 - reconstruction loss: 0.0356128/3811 - total loss: 0.2868 - classification loss: 0.1250 - dann loss: 2.4583 - reconstruction loss: 0.0355128/3811 - total loss: 0.2871 - classification loss: 0.1282 - dann loss: 2.4582 - reconstruction loss: 0.0356128/3811 - total loss: 0.2871 - classification loss: 0.1282 - dann loss: 2.4574 - reconstruction loss: 0.0357128/3811 - total loss: 0.2867 - classification loss: 0.1278 - dann loss: 2.4541 - reconstruction loss: 0.0356128/3811 - total loss: 0.2863 - classification loss: 0.1289 - dann loss: 2.4493 - reconstruction loss: 0.0356128/3811 - total loss: 0.2869 - classification loss: 0.1304 - dann loss: 2.4525 - reconstruction loss: 0.0357128/3811 - total loss: 0.2865 - classification loss: 0.1306 - dann loss: 2.4481 - reconstruction loss: 0.0357128/3811 - total loss: 0.2862 - classification loss: 0.1304 - dann loss: 2.4461 - reconstruction loss: 0.0357128/3811 - total loss: 0.2861 - classification loss: 0.1304 - dann loss: 2.4452 - reconstruction loss: 0.0357128/3811 - total loss: 0.2860 - classification loss: 0.1304 - dann loss: 2.4443 - reconstruction loss: 0.035798/3811 - total loss: 0.2859 - classification loss: 0.1291 - dann loss: 2.4439 - reconstruction loss: 0.03570/3811 - total loss: 0.2859 - classification loss: 0.1291 - dann loss: 2.4439 - reconstruction loss: 0.0357Epoch 8/150, Current strat Epoch 8/100
use_perm = True
switching perm
128/3811 - total loss: 0.2829 - classification loss: 0.0830 - dann loss: 2.4578 - reconstruction loss: 0.0360128/3811 - total loss: 0.2858 - classification loss: 0.0931 - dann loss: 2.4769 - reconstruction loss: 0.0359128/3811 - total loss: 0.2838 - classification loss: 0.1004 - dann loss: 2.4508 - reconstruction loss: 0.0359128/3811 - total loss: 0.2827 - classification loss: 0.1017 - dann loss: 2.4403 - reconstruction loss: 0.0357128/3811 - total loss: 0.2823 - classification loss: 0.1025 - dann loss: 2.4332 - reconstruction loss: 0.0360128/3811 - total loss: 0.2815 - classification loss: 0.0997 - dann loss: 2.4291 - reconstruction loss: 0.0357128/3811 - total loss: 0.2820 - classification loss: 0.0978 - dann loss: 2.4369 - reconstruction loss: 0.0357128/3811 - total loss: 0.2814 - classification loss: 0.0961 - dann loss: 2.4335 - reconstruction loss: 0.0355128/3811 - total loss: 0.2807 - classification loss: 0.0936 - dann loss: 2.4281 - reconstruction loss: 0.0356128/3811 - total loss: 0.2807 - classification loss: 0.0962 - dann loss: 2.4245 - reconstruction loss: 0.0358128/3811 - total loss: 0.2808 - classification loss: 0.0964 - dann loss: 2.4267 - reconstruction loss: 0.0356128/3811 - total loss: 0.2811 - classification loss: 0.0952 - dann loss: 2.4304 - reconstruction loss: 0.0357128/3811 - total loss: 0.2803 - classification loss: 0.0940 - dann loss: 2.4242 - reconstruction loss: 0.0356128/3811 - total loss: 0.2796 - classification loss: 0.0939 - dann loss: 2.4180 - reconstruction loss: 0.0355128/3811 - total loss: 0.2799 - classification loss: 0.0965 - dann loss: 2.4187 - reconstruction loss: 0.0355128/3811 - total loss: 0.2800 - classification loss: 0.0982 - dann loss: 2.4174 - reconstruction loss: 0.0356128/3811 - total loss: 0.2797 - classification loss: 0.0982 - dann loss: 2.4140 - reconstruction loss: 0.0356128/3811 - total loss: 0.2799 - classification loss: 0.1016 - dann loss: 2.4126 - reconstruction loss: 0.0356128/3811 - total loss: 0.2797 - classification loss: 0.1029 - dann loss: 2.4093 - reconstruction loss: 0.0356128/3811 - total loss: 0.2788 - classification loss: 0.1028 - dann loss: 2.4010 - reconstruction loss: 0.0356128/3811 - total loss: 0.2787 - classification loss: 0.1019 - dann loss: 2.3999 - reconstruction loss: 0.0357128/3811 - total loss: 0.2787 - classification loss: 0.1030 - dann loss: 2.3993 - reconstruction loss: 0.0356128/3811 - total loss: 0.2785 - classification loss: 0.1028 - dann loss: 2.3969 - reconstruction loss: 0.0357128/3811 - total loss: 0.2782 - classification loss: 0.1020 - dann loss: 2.3954 - reconstruction loss: 0.0355128/3811 - total loss: 0.2775 - classification loss: 0.1021 - dann loss: 2.3890 - reconstruction loss: 0.0355128/3811 - total loss: 0.2775 - classification loss: 0.1026 - dann loss: 2.3885 - reconstruction loss: 0.0355128/3811 - total loss: 0.2770 - classification loss: 0.1020 - dann loss: 2.3833 - reconstruction loss: 0.0356128/3811 - total loss: 0.2766 - classification loss: 0.1021 - dann loss: 2.3794 - reconstruction loss: 0.0356128/3811 - total loss: 0.2760 - classification loss: 0.1019 - dann loss: 2.3737 - reconstruction loss: 0.035598/3811 - total loss: 0.2762 - classification loss: 0.1036 - dann loss: 2.3731 - reconstruction loss: 0.03560/3811 - total loss: 0.2762 - classification loss: 0.1036 - dann loss: 2.3731 - reconstruction loss: 0.0356Epoch 9/150, Current strat Epoch 9/100
use_perm = True
switching perm
128/3811 - total loss: 0.2792 - classification loss: 0.0909 - dann loss: 2.4104 - reconstruction loss: 0.0363128/3811 - total loss: 0.2817 - classification loss: 0.0953 - dann loss: 2.4356 - reconstruction loss: 0.0357128/3811 - total loss: 0.2816 - classification loss: 0.0906 - dann loss: 2.4417 - reconstruction loss: 0.0355128/3811 - total loss: 0.2814 - classification loss: 0.0885 - dann loss: 2.4404 - reconstruction loss: 0.0357128/3811 - total loss: 0.2799 - classification loss: 0.0878 - dann loss: 2.4240 - reconstruction loss: 0.0359128/3811 - total loss: 0.2786 - classification loss: 0.0863 - dann loss: 2.4096 - reconstruction loss: 0.0362128/3811 - total loss: 0.2775 - classification loss: 0.0869 - dann loss: 2.4002 - reconstruction loss: 0.0360128/3811 - total loss: 0.2767 - classification loss: 0.0860 - dann loss: 2.3918 - reconstruction loss: 0.0361128/3811 - total loss: 0.2761 - classification loss: 0.0856 - dann loss: 2.3879 - reconstruction loss: 0.0359128/3811 - total loss: 0.2758 - classification loss: 0.0866 - dann loss: 2.3855 - reconstruction loss: 0.0358128/3811 - total loss: 0.2756 - classification loss: 0.0862 - dann loss: 2.3835 - reconstruction loss: 0.0357128/3811 - total loss: 0.2736 - classification loss: 0.0836 - dann loss: 2.3673 - reconstruction loss: 0.0357128/3811 - total loss: 0.2736 - classification loss: 0.0836 - dann loss: 2.3676 - reconstruction loss: 0.0356128/3811 - total loss: 0.2733 - classification loss: 0.0901 - dann loss: 2.3589 - reconstruction loss: 0.0356128/3811 - total loss: 0.2728 - classification loss: 0.0890 - dann loss: 2.3543 - reconstruction loss: 0.0356128/3811 - total loss: 0.2718 - classification loss: 0.0881 - dann loss: 2.3454 - reconstruction loss: 0.0356128/3811 - total loss: 0.2718 - classification loss: 0.0904 - dann loss: 2.3420 - reconstruction loss: 0.0357128/3811 - total loss: 0.2714 - classification loss: 0.0914 - dann loss: 2.3372 - reconstruction loss: 0.0357128/3811 - total loss: 0.2706 - classification loss: 0.0905 - dann loss: 2.3299 - reconstruction loss: 0.0357128/3811 - total loss: 0.2702 - classification loss: 0.0907 - dann loss: 2.3266 - reconstruction loss: 0.0356128/3811 - total loss: 0.2700 - classification loss: 0.0898 - dann loss: 2.3246 - reconstruction loss: 0.0357128/3811 - total loss: 0.2692 - classification loss: 0.0888 - dann loss: 2.3180 - reconstruction loss: 0.0356128/3811 - total loss: 0.2684 - classification loss: 0.0885 - dann loss: 2.3110 - reconstruction loss: 0.0356128/3811 - total loss: 0.2684 - classification loss: 0.0886 - dann loss: 2.3107 - reconstruction loss: 0.0356128/3811 - total loss: 0.2680 - classification loss: 0.0886 - dann loss: 2.3073 - reconstruction loss: 0.0355128/3811 - total loss: 0.2680 - classification loss: 0.0884 - dann loss: 2.3071 - reconstruction loss: 0.0355128/3811 - total loss: 0.2680 - classification loss: 0.0889 - dann loss: 2.3072 - reconstruction loss: 0.0355128/3811 - total loss: 0.2679 - classification loss: 0.0891 - dann loss: 2.3055 - reconstruction loss: 0.0355128/3811 - total loss: 0.2676 - classification loss: 0.0898 - dann loss: 2.3024 - reconstruction loss: 0.035598/3811 - total loss: 0.2677 - classification loss: 0.0902 - dann loss: 2.3025 - reconstruction loss: 0.03550/3811 - total loss: 0.2677 - classification loss: 0.0902 - dann loss: 2.3025 - reconstruction loss: 0.0355Epoch 10/150, Current strat Epoch 10/100
use_perm = True
switching perm
128/3811 - total loss: 0.2770 - classification loss: 0.0926 - dann loss: 2.3939 - reconstruction loss: 0.0355128/3811 - total loss: 0.2730 - classification loss: 0.0800 - dann loss: 2.3625 - reconstruction loss: 0.0359128/3811 - total loss: 0.2682 - classification loss: 0.0725 - dann loss: 2.3254 - reconstruction loss: 0.0355128/3811 - total loss: 0.2663 - classification loss: 0.0752 - dann loss: 2.3032 - reconstruction loss: 0.0355128/3811 - total loss: 0.2665 - classification loss: 0.0764 - dann loss: 2.3092 - reconstruction loss: 0.0350128/3811 - total loss: 0.2676 - classification loss: 0.0772 - dann loss: 2.3164 - reconstruction loss: 0.0353128/3811 - total loss: 0.2661 - classification loss: 0.0739 - dann loss: 2.3048 - reconstruction loss: 0.0353128/3811 - total loss: 0.2666 - classification loss: 0.0705 - dann loss: 2.3124 - reconstruction loss: 0.0353128/3811 - total loss: 0.2655 - classification loss: 0.0691 - dann loss: 2.3010 - reconstruction loss: 0.0356128/3811 - total loss: 0.2648 - classification loss: 0.0691 - dann loss: 2.2933 - reconstruction loss: 0.0356128/3742 - total loss: 0.3172 - classification loss: 0.2108 - dann loss: 2.6748 - reconstruction loss: 0.0358128/3742 - total loss: 0.3171 - classification loss: 0.2129 - dann loss: 2.6716 - reconstruction loss: 0.0358128/3742 - total loss: 0.3163 - classification loss: 0.2117 - dann loss: 2.6655 - reconstruction loss: 0.0357128/3742 - total loss: 0.3158 - classification loss: 0.2103 - dann loss: 2.6627 - reconstruction loss: 0.0357128/3742 - total loss: 0.3154 - classification loss: 0.2085 - dann loss: 2.6599 - reconstruction loss: 0.0357128/3742 - total loss: 0.3156 - classification loss: 0.2104 - dann loss: 2.6599 - reconstruction loss: 0.0357128/3742 - total loss: 0.3153 - classification loss: 0.2098 - dann loss: 2.6565 - reconstruction loss: 0.0358128/3742 - total loss: 0.3148 - classification loss: 0.2090 - dann loss: 2.6524 - reconstruction loss: 0.0358128/3742 - total loss: 0.3144 - classification loss: 0.2081 - dann loss: 2.6487 - reconstruction loss: 0.0358128/3742 - total loss: 0.3142 - classification loss: 0.2075 - dann loss: 2.6470 - reconstruction loss: 0.035929/3742 - total loss: 0.3129 - classification loss: 0.2040 - dann loss: 2.6369 - reconstruction loss: 0.03600/3742 - total loss: 0.3129 - classification loss: 0.2040 - dann loss: 2.6369 - reconstruction loss: 0.0360Epoch 6/150, Current strat Epoch 6/100
use_perm = True
switching perm
128/3742 - total loss: 0.3127 - classification loss: 0.1504 - dann loss: 2.6832 - reconstruction loss: 0.0367128/3742 - total loss: 0.3106 - classification loss: 0.1414 - dann loss: 2.6724 - reconstruction loss: 0.0365128/3742 - total loss: 0.3093 - classification loss: 0.1361 - dann loss: 2.6694 - reconstruction loss: 0.0359128/3742 - total loss: 0.3114 - classification loss: 0.1482 - dann loss: 2.6815 - reconstruction loss: 0.0355128/3742 - total loss: 0.3116 - classification loss: 0.1405 - dann loss: 2.6934 - reconstruction loss: 0.0353128/3742 - total loss: 0.3117 - classification loss: 0.1453 - dann loss: 2.6873 - reconstruction loss: 0.0355128/3742 - total loss: 0.3139 - classification loss: 0.1597 - dann loss: 2.6919 - reconstruction loss: 0.0359128/3742 - total loss: 0.3136 - classification loss: 0.1608 - dann loss: 2.6894 - reconstruction loss: 0.0357128/3742 - total loss: 0.3122 - classification loss: 0.1601 - dann loss: 2.6765 - reconstruction loss: 0.0356128/3742 - total loss: 0.3130 - classification loss: 0.1607 - dann loss: 2.6828 - reconstruction loss: 0.0358128/3742 - total loss: 0.3129 - classification loss: 0.1589 - dann loss: 2.6847 - reconstruction loss: 0.0357128/3742 - total loss: 0.3123 - classification loss: 0.1600 - dann loss: 2.6789 - reconstruction loss: 0.0356128/3742 - total loss: 0.3118 - classification loss: 0.1629 - dann loss: 2.6705 - reconstruction loss: 0.0355128/3742 - total loss: 0.3117 - classification loss: 0.1614 - dann loss: 2.6707 - reconstruction loss: 0.0356128/3742 - total loss: 0.3113 - classification loss: 0.1617 - dann loss: 2.6648 - reconstruction loss: 0.0357128/3742 - total loss: 0.3109 - classification loss: 0.1591 - dann loss: 2.6634 - reconstruction loss: 0.0359128/3742 - total loss: 0.3106 - classification loss: 0.1586 - dann loss: 2.6608 - reconstruction loss: 0.0358128/3742 - total loss: 0.3104 - classification loss: 0.1605 - dann loss: 2.6575 - reconstruction loss: 0.0358128/3742 - total loss: 0.3100 - classification loss: 0.1617 - dann loss: 2.6518 - reconstruction loss: 0.0358128/3742 - total loss: 0.3094 - classification loss: 0.1594 - dann loss: 2.6482 - reconstruction loss: 0.0358128/3742 - total loss: 0.3088 - classification loss: 0.1592 - dann loss: 2.6424 - reconstruction loss: 0.0358128/3742 - total loss: 0.3086 - classification loss: 0.1590 - dann loss: 2.6405 - reconstruction loss: 0.0358128/3742 - total loss: 0.3080 - classification loss: 0.1590 - dann loss: 2.6349 - reconstruction loss: 0.0357128/3742 - total loss: 0.3077 - classification loss: 0.1580 - dann loss: 2.6330 - reconstruction loss: 0.0357128/3742 - total loss: 0.3071 - classification loss: 0.1571 - dann loss: 2.6290 - reconstruction loss: 0.0357128/3742 - total loss: 0.3068 - classification loss: 0.1569 - dann loss: 2.6252 - reconstruction loss: 0.0357128/3742 - total loss: 0.3066 - classification loss: 0.1564 - dann loss: 2.6235 - reconstruction loss: 0.0358128/3742 - total loss: 0.3060 - classification loss: 0.1548 - dann loss: 2.6190 - reconstruction loss: 0.0358128/3742 - total loss: 0.3053 - classification loss: 0.1524 - dann loss: 2.6145 - reconstruction loss: 0.035829/3742 - total loss: 0.3050 - classification loss: 0.1538 - dann loss: 2.6100 - reconstruction loss: 0.03580/3742 - total loss: 0.3050 - classification loss: 0.1538 - dann loss: 2.6100 - reconstruction loss: 0.0358Epoch 7/150, Current strat Epoch 7/100
use_perm = True
switching perm
128/3742 - total loss: 0.3075 - classification loss: 0.1295 - dann loss: 2.6517 - reconstruction loss: 0.0367128/3742 - total loss: 0.3079 - classification loss: 0.1135 - dann loss: 2.6739 - reconstruction loss: 0.0364128/3742 - total loss: 0.3048 - classification loss: 0.1227 - dann loss: 2.6362 - reconstruction loss: 0.0362128/3742 - total loss: 0.3051 - classification loss: 0.1176 - dann loss: 2.6468 - reconstruction loss: 0.0358128/3742 - total loss: 0.3086 - classification loss: 0.1282 - dann loss: 2.6738 - reconstruction loss: 0.0355128/3742 - total loss: 0.3084 - classification loss: 0.1314 - dann loss: 2.6696 - reconstruction loss: 0.0354128/3742 - total loss: 0.3075 - classification loss: 0.1272 - dann loss: 2.6647 - reconstruction loss: 0.0354128/3742 - total loss: 0.3076 - classification loss: 0.1277 - dann loss: 2.6634 - reconstruction loss: 0.0356128/3742 - total loss: 0.3072 - classification loss: 0.1248 - dann loss: 2.6624 - reconstruction loss: 0.0356128/3742 - total loss: 0.3064 - classification loss: 0.1257 - dann loss: 2.6545 - reconstruction loss: 0.0355128/3742 - total loss: 0.3060 - classification loss: 0.1242 - dann loss: 2.6535 - reconstruction loss: 0.0353128/3742 - total loss: 0.3060 - classification loss: 0.1256 - dann loss: 2.6507 - reconstruction loss: 0.0355128/3742 - total loss: 0.3063 - classification loss: 0.1265 - dann loss: 2.6516 - reconstruction loss: 0.0356128/3742 - total loss: 0.3061 - classification loss: 0.1277 - dann loss: 2.6490 - reconstruction loss: 0.0356128/3742 - total loss: 0.3057 - classification loss: 0.1281 - dann loss: 2.6448 - reconstruction loss: 0.0356128/3742 - total loss: 0.3056 - classification loss: 0.1263 - dann loss: 2.6447 - reconstruction loss: 0.0356128/3742 - total loss: 0.3049 - classification loss: 0.1257 - dann loss: 2.6398 - reconstruction loss: 0.0355128/3742 - total loss: 0.3045 - classification loss: 0.1247 - dann loss: 2.6369 - reconstruction loss: 0.0355128/3742 - total loss: 0.3047 - classification loss: 0.1244 - dann loss: 2.6393 - reconstruction loss: 0.0354128/3742 - total loss: 0.3048 - classification loss: 0.1276 - dann loss: 2.6368 - reconstruction loss: 0.0355128/3742 - total loss: 0.3047 - classification loss: 0.1268 - dann loss: 2.6367 - reconstruction loss: 0.0354128/3742 - total loss: 0.3041 - classification loss: 0.1259 - dann loss: 2.6310 - reconstruction loss: 0.0355128/3742 - total loss: 0.3040 - classification loss: 0.1259 - dann loss: 2.6294 - reconstruction loss: 0.0356128/3742 - total loss: 0.3038 - classification loss: 0.1261 - dann loss: 2.6279 - reconstruction loss: 0.0355128/3742 - total loss: 0.3034 - classification loss: 0.1257 - dann loss: 2.6239 - reconstruction loss: 0.0355128/3742 - total loss: 0.3034 - classification loss: 0.1266 - dann loss: 2.6236 - reconstruction loss: 0.0355128/3742 - total loss: 0.3033 - classification loss: 0.1265 - dann loss: 2.6226 - reconstruction loss: 0.0355128/3742 - total loss: 0.3031 - classification loss: 0.1258 - dann loss: 2.6210 - reconstruction loss: 0.0355128/3742 - total loss: 0.3030 - classification loss: 0.1255 - dann loss: 2.6208 - reconstruction loss: 0.035529/3742 - total loss: 0.3028 - classification loss: 0.1276 - dann loss: 2.6158 - reconstruction loss: 0.03550/3742 - total loss: 0.3028 - classification loss: 0.1276 - dann loss: 2.6158 - reconstruction loss: 0.0355128/3811 - total loss: 0.2639 - classification loss: 0.0715 - dann loss: 2.2825 - reconstruction loss: 0.0357128/3811 - total loss: 0.2644 - classification loss: 0.0714 - dann loss: 2.2876 - reconstruction loss: 0.0356128/3811 - total loss: 0.2641 - classification loss: 0.0712 - dann loss: 2.2838 - reconstruction loss: 0.0357128/3811 - total loss: 0.2636 - classification loss: 0.0699 - dann loss: 2.2802 - reconstruction loss: 0.0358128/3811 - total loss: 0.2638 - classification loss: 0.0728 - dann loss: 2.2794 - reconstruction loss: 0.0358128/3811 - total loss: 0.2645 - classification loss: 0.0755 - dann loss: 2.2831 - reconstruction loss: 0.0359128/3811 - total loss: 0.2645 - classification loss: 0.0757 - dann loss: 2.2826 - reconstruction loss: 0.0359128/3811 - total loss: 0.2646 - classification loss: 0.0775 - dann loss: 2.2825 - reconstruction loss: 0.0358128/3811 - total loss: 0.2643 - classification loss: 0.0773 - dann loss: 2.2791 - reconstruction loss: 0.0358128/3811 - total loss: 0.2638 - classification loss: 0.0769 - dann loss: 2.2750 - reconstruction loss: 0.0358128/3811 - total loss: 0.2636 - classification loss: 0.0757 - dann loss: 2.2740 - reconstruction loss: 0.0358128/3811 - total loss: 0.2631 - classification loss: 0.0750 - dann loss: 2.2714 - reconstruction loss: 0.0356128/3811 - total loss: 0.2627 - classification loss: 0.0742 - dann loss: 2.2673 - reconstruction loss: 0.0356128/3811 - total loss: 0.2625 - classification loss: 0.0758 - dann loss: 2.2645 - reconstruction loss: 0.0356128/3811 - total loss: 0.2618 - classification loss: 0.0757 - dann loss: 2.2587 - reconstruction loss: 0.0355128/3811 - total loss: 0.2615 - classification loss: 0.0751 - dann loss: 2.2558 - reconstruction loss: 0.0355128/3811 - total loss: 0.2614 - classification loss: 0.0744 - dann loss: 2.2559 - reconstruction loss: 0.0355128/3811 - total loss: 0.2609 - classification loss: 0.0742 - dann loss: 2.2512 - reconstruction loss: 0.0355128/3811 - total loss: 0.2605 - classification loss: 0.0736 - dann loss: 2.2473 - reconstruction loss: 0.035598/3811 - total loss: 0.2600 - classification loss: 0.0746 - dann loss: 2.2415 - reconstruction loss: 0.03550/3811 - total loss: 0.2600 - classification loss: 0.0746 - dann loss: 2.2415 - reconstruction loss: 0.0355Epoch 11/150, Current strat Epoch 11/100
use_perm = True
switching perm
128/3811 - total loss: 0.2640 - classification loss: 0.0743 - dann loss: 2.2873 - reconstruction loss: 0.0348128/3811 - total loss: 0.2606 - classification loss: 0.0625 - dann loss: 2.2658 - reconstruction loss: 0.0347128/3811 - total loss: 0.2599 - classification loss: 0.0565 - dann loss: 2.2625 - reconstruction loss: 0.0350128/3811 - total loss: 0.2568 - classification loss: 0.0578 - dann loss: 2.2337 - reconstruction loss: 0.0346128/3811 - total loss: 0.2552 - classification loss: 0.0598 - dann loss: 2.2140 - reconstruction loss: 0.0348128/3811 - total loss: 0.2548 - classification loss: 0.0634 - dann loss: 2.2094 - reconstruction loss: 0.0344128/3811 - total loss: 0.2567 - classification loss: 0.0637 - dann loss: 2.2252 - reconstruction loss: 0.0348128/3811 - total loss: 0.2567 - classification loss: 0.0626 - dann loss: 2.2266 - reconstruction loss: 0.0347128/3811 - total loss: 0.2578 - classification loss: 0.0656 - dann loss: 2.2332 - reconstruction loss: 0.0349128/3811 - total loss: 0.2572 - classification loss: 0.0643 - dann loss: 2.2262 - reconstruction loss: 0.0352128/3811 - total loss: 0.2584 - classification loss: 0.0634 - dann loss: 2.2387 - reconstruction loss: 0.0352128/3811 - total loss: 0.2585 - classification loss: 0.0620 - dann loss: 2.2422 - reconstruction loss: 0.0352128/3811 - total loss: 0.2584 - classification loss: 0.0616 - dann loss: 2.2419 - reconstruction loss: 0.0351128/3811 - total loss: 0.2577 - classification loss: 0.0606 - dann loss: 2.2344 - reconstruction loss: 0.0352128/3811 - total loss: 0.2576 - classification loss: 0.0605 - dann loss: 2.2333 - reconstruction loss: 0.0353128/3811 - total loss: 0.2575 - classification loss: 0.0606 - dann loss: 2.2323 - reconstruction loss: 0.0353128/3811 - total loss: 0.2576 - classification loss: 0.0604 - dann loss: 2.2331 - reconstruction loss: 0.0353128/3811 - total loss: 0.2574 - classification loss: 0.0601 - dann loss: 2.2306 - reconstruction loss: 0.0354128/3811 - total loss: 0.2575 - classification loss: 0.0598 - dann loss: 2.2313 - reconstruction loss: 0.0355128/3811 - total loss: 0.2576 - classification loss: 0.0598 - dann loss: 2.2326 - reconstruction loss: 0.0355128/3811 - total loss: 0.2572 - classification loss: 0.0601 - dann loss: 2.2276 - reconstruction loss: 0.0355128/3811 - total loss: 0.2569 - classification loss: 0.0594 - dann loss: 2.2254 - reconstruction loss: 0.0356128/3811 - total loss: 0.2566 - classification loss: 0.0590 - dann loss: 2.2228 - reconstruction loss: 0.0355128/3811 - total loss: 0.2566 - classification loss: 0.0585 - dann loss: 2.2223 - reconstruction loss: 0.0356128/3811 - total loss: 0.2568 - classification loss: 0.0579 - dann loss: 2.2249 - reconstruction loss: 0.0356128/3811 - total loss: 0.2570 - classification loss: 0.0586 - dann loss: 2.2274 - reconstruction loss: 0.0355128/3811 - total loss: 0.2574 - classification loss: 0.0586 - dann loss: 2.2312 - reconstruction loss: 0.0355128/3811 - total loss: 0.2574 - classification loss: 0.0581 - dann loss: 2.2324 - reconstruction loss: 0.0355128/3811 - total loss: 0.2575 - classification loss: 0.0593 - dann loss: 2.2312 - reconstruction loss: 0.035598/3811 - total loss: 0.2573 - classification loss: 0.0609 - dann loss: 2.2284 - reconstruction loss: 0.03550/3811 - total loss: 0.2573 - classification loss: 0.0609 - dann loss: 2.2284 - reconstruction loss: 0.0355Epoch 12/150, Current strat Epoch 12/100
use_perm = True
switching perm
128/3811 - total loss: 0.2527 - classification loss: 0.0791 - dann loss: 2.1869 - reconstruction loss: 0.0326128/3811 - total loss: 0.2557 - classification loss: 0.0722 - dann loss: 2.2122 - reconstruction loss: 0.0341128/3811 - total loss: 0.2549 - classification loss: 0.0602 - dann loss: 2.2138 - reconstruction loss: 0.0344128/3811 - total loss: 0.2568 - classification loss: 0.0577 - dann loss: 2.2306 - reconstruction loss: 0.0349128/3811 - total loss: 0.2579 - classification loss: 0.0553 - dann loss: 2.2441 - reconstruction loss: 0.0350128/3811 - total loss: 0.2575 - classification loss: 0.0563 - dann loss: 2.2377 - reconstruction loss: 0.0351128/3811 - total loss: 0.2595 - classification loss: 0.0577 - dann loss: 2.2562 - reconstruction loss: 0.0351128/3811 - total loss: 0.2595 - classification loss: 0.0590 - dann loss: 2.2548 - reconstruction loss: 0.0351128/3811 - total loss: 0.2597 - classification loss: 0.0593 - dann loss: 2.2552 - reconstruction loss: 0.0352128/3811 - total loss: 0.2608 - classification loss: 0.0599 - dann loss: 2.2647 - reconstruction loss: 0.0354128/3811 - total loss: 0.2611 - classification loss: 0.0593 - dann loss: 2.2687 - reconstruction loss: 0.0354128/3811 - total loss: 0.2612 - classification loss: 0.0597 - dann loss: 2.2681 - reconstruction loss: 0.0356128/3811 - total loss: 0.2606 - classification loss: 0.0601 - dann loss: 2.2607 - reconstruction loss: 0.0356128/3811 - total loss: 0.2607 - classification loss: 0.0608 - dann loss: 2.2612 - reconstruction loss: 0.0357128/3811 - total loss: 0.2606 - classification loss: 0.0606 - dann loss: 2.2592 - reconstruction loss: 0.0358128/3811 - total loss: 0.2601 - classification loss: 0.0598 - dann loss: 2.2557 - reconstruction loss: 0.0357128/3811 - total loss: 0.2602 - classification loss: 0.0593 - dann loss: 2.2561 - reconstruction loss: 0.0358128/3811 - total loss: 0.2605 - classification loss: 0.0584 - dann loss: 2.2601 - reconstruction loss: 0.0358128/3811 - total loss: 0.2605 - classification loss: 0.0597 - dann loss: 2.2596 - reconstruction loss: 0.0358128/3811 - total loss: 0.2602 - classification loss: 0.0590 - dann loss: 2.2571 - reconstruction loss: 0.0357128/3811 - total loss: 0.2595 - classification loss: 0.0588 - dann loss: 2.2509 - reconstruction loss: 0.0357128/3811 - total loss: 0.2591 - classification loss: 0.0578 - dann loss: 2.2479 - reconstruction loss: 0.0357128/3811 - total loss: 0.2590 - classification loss: 0.0592 - dann loss: 2.2451 - reconstruction loss: 0.0357128/3811 - total loss: 0.2592 - classification loss: 0.0598 - dann loss: 2.2475 - reconstruction loss: 0.0356128/3811 - total loss: 0.2590 - classification loss: 0.0604 - dann loss: 2.2444 - reconstruction loss: 0.0356128/3811 - total loss: 0.2588 - classification loss: 0.0598 - dann loss: 2.2432 - reconstruction loss: 0.0357128/3811 - total loss: 0.2584 - classification loss: 0.0592 - dann loss: 2.2390 - reconstruction loss: 0.0357128/3811 - total loss: 0.2581 - classification loss: 0.0585 - dann loss: 2.2372 - reconstruction loss: 0.0357128/3811 - total loss: 0.2575 - classification loss: 0.0581 - dann loss: 2.2323 - reconstruction loss: 0.035698/3811 - total loss: 0.2574 - classification loss: 0.0588 - dann loss: 2.2314 - reconstruction loss: 0.03550/3811 - total loss: 0.2574 - classification loss: 0.0588 - dann loss: 2.2314 - reconstruction loss: 0.0355Epoch 13/150, Current strat Epoch 13/100
use_perm = True
switching perm
128/3811 - total loss: 0.2599 - classification loss: 0.0471 - dann loss: 2.2688 - reconstruction loss: 0.0354128/3811 - total loss: 0.2538 - classification loss: 0.0467 - dann loss: 2.2075 - reconstruction loss: 0.0354128/3811 - total loss: 0.2525 - classification loss: 0.0497 - dann loss: 2.1907 - reconstruction loss: 0.0356128/3811 - total loss: 0.2527 - classification loss: 0.0507 - dann loss: 2.1909 - reconstruction loss: 0.0357128/3811 - total loss: 0.2545 - classification loss: 0.0532 - dann loss: 2.2075 - reconstruction loss: 0.0356128/3811 - total loss: 0.2530 - classification loss: 0.0497 - dann loss: 2.1957 - reconstruction loss: 0.0355128/3811 - total loss: 0.2526 - classification loss: 0.0480 - dann loss: 2.1929 - reconstruction loss: 0.0356128/3811 - total loss: 0.2522 - classification loss: 0.0461 - dann loss: 2.1914 - reconstruction loss: 0.0356128/3811 - total loss: 0.2522 - classification loss: 0.0457 - dann loss: 2.1929 - reconstruction loss: 0.0354128/3811 - total loss: 0.2519 - classification loss: 0.0445 - dann loss: 2.1921 - reconstruction loss: 0.0354128/3811 - total loss: 0.2515 - classification loss: 0.0432 - dann loss: 2.1887 - reconstruction loss: 0.0353128/3811 - total loss: 0.2511 - classification loss: 0.0425 - dann loss: 2.1863 - reconstruction loss: 0.0352128/3811 - total loss: 0.2506 - classification loss: 0.0446 - dann loss: 2.1795 - reconstruction loss: 0.0352128/3811 - total loss: 0.2501 - classification loss: 0.0447 - dann loss: 2.1734 - reconstruction loss: 0.0353128/3811 - total loss: 0.2495 - classification loss: 0.0450 - dann loss: 2.1669 - reconstruction loss: 0.0354128/3811 - total loss: 0.2493 - classification loss: 0.0462 - dann loss: 2.1643 - reconstruction loss: 0.0354128/3811 - total loss: 0.2486 - classification loss: 0.0454 - dann loss: 2.1591 - reconstruction loss: 0.0352128/3811 - total loss: 0.2482 - classification loss: 0.0449 - dann loss: 2.1555 - reconstruction loss: 0.0352128/3811 - total loss: 0.2482 - classification loss: 0.0454 - dann loss: 2.1535 - reconstruction loss: 0.0354128/3811 - total loss: 0.2480 - classification loss: 0.0470 - dann loss: 2.1499 - reconstruction loss: 0.0354128/3811 - total loss: 0.2474 - classification loss: 0.0467 - dann loss: 2.1450 - reconstruction loss: 0.0353128/3811 - total loss: 0.2470 - classification loss: 0.0465 - dann loss: 2.1407 - reconstruction loss: 0.0354128/3811 - total loss: 0.2468 - classification loss: 0.0464 - dann loss: 2.1377 - reconstruction loss: 0.0355128/3811 - total loss: 0.2470 - classification loss: 0.0459 - dann loss: 2.1407 - reconstruction loss: 0.0354128/3811 - total loss: 0.2470 - classification loss: 0.0458 - dann loss: 2.1414 - reconstruction loss: 0.0354128/3811 - total loss: 0.2467 - classification loss: 0.0451 - dann loss: 2.1389 - reconstruction loss: 0.0354128/3811 - total loss: 0.2461 - classification loss: 0.0448 - dann loss: 2.1336 - reconstruction loss: 0.0354128/3811 - total loss: 0.2461 - classification loss: 0.0446 - dann loss: 2.1330 - reconstruction loss: 0.0354128/3811 - total loss: 0.2462 - classification loss: 0.0447 - dann loss: 2.1334 - reconstruction loss: 0.035498/3811 - total loss: 0.2460 - classification loss: 0.0456 - dann loss: 2.1309 - reconstruction loss: 0.03540/3811 - total loss: 0.2460 - classification loss: 0.0456 - dann loss: 2.1309 - reconstruction loss: 0.0354Epoch 14/150, Current strat Epoch 14/100
use_perm = True
switching perm
128/3811 - total loss: 0.2469 - classification loss: 0.0338 - dann loss: 2.1455 - reconstruction loss: 0.0363128/3811 - total loss: 0.2462 - classification loss: 0.0336 - dann loss: 2.1475 - reconstruction loss: 0.0352128/3811 - total loss: 0.2460 - classification loss: 0.0336 - dann loss: 2.1457 - reconstruction loss: 0.0351128/3811 - total loss: 0.2476 - classification loss: 0.0365 - dann loss: 2.1615 - reconstruction loss: 0.0348128/3811 - total loss: 0.2443 - classification loss: 0.0356 - dann loss: 2.1304 - reconstruction loss: 0.0346128/3811 - total loss: 0.2436 - classification loss: 0.0365 - dann loss: 2.1213 - reconstruction loss: 0.0348128/3811 - total loss: 0.2439 - classification loss: 0.0395 - dann loss: 2.1195 - reconstruction loss: 0.0350128/3811 - total loss: 0.2427 - classification loss: 0.0382 - dann loss: 2.1092 - reconstruction loss: 0.0350128/3811 - total loss: 0.2416 - classification loss: 0.0371 - dann loss: 2.1008 - reconstruction loss: 0.0347128/3811 - total loss: 0.2415 - classification loss: 0.0403 - dann loss: 2.0963 - reconstruction loss: 0.0348128/3811 - total loss: 0.2416 - classification loss: 0.0392 - dann loss: 2.0977 - reconstruction loss: 0.0349128/3811 - total loss: 0.2418 - classification loss: 0.0387 - dann loss: 2.1006 - reconstruction loss: 0.0349128/3811 - total loss: 0.2420 - classification loss: 0.0411 - dann loss: 2.0999 - reconstruction loss: 0.0349128/3811 - total loss: 0.2421 - classification loss: 0.0408 - dann loss: 2.0997 - reconstruction loss: 0.0351128/3811 - total loss: 0.2430 - classification loss: 0.0413 - dann loss: 2.1080 - reconstruction loss: 0.0351128/3811 - total loss: 0.2428 - classification loss: 0.0440 - dann loss: 2.1025 - reconstruction loss: 0.0352128/3811 - total loss: 0.2425 - classification loss: 0.0436 - dann loss: 2.0991 - reconstruction loss: 0.0353128/3811 - total loss: 0.2421 - classification loss: 0.0438 - dann loss: 2.0951 - reconstruction loss: 0.0353128/3811 - total loss: 0.2422 - classification loss: 0.0440 - dann loss: 2.0954 - reconstruction loss: 0.0353128/3811 - total loss: 0.2420 - classification loss: 0.0435 - dann loss: 2.0949 - reconstruction loss: 0.0351128/3811 - total loss: 0.2423 - classification loss: 0.0427 - dann loss: 2.0989 - reconstruction loss: 0.0352128/3811 - total loss: 0.2423 - classification loss: 0.0425 - dann loss: 2.0986 - reconstruction loss: 0.0353128/3811 - total loss: 0.2427 - classification loss: 0.0425 - dann loss: 2.1016 - reconstruction loss: 0.0353128/3811 - total loss: 0.2426 - classification loss: 0.0426 - dann loss: 2.1016 - reconstruction loss: 0.0353128/3811 - total loss: 0.2427 - classification loss: 0.0431 - dann loss: 2.1023 - reconstruction loss: 0.0353128/3811 - total loss: 0.2428 - classification loss: 0.0430 - dann loss: 2.1024 - reconstruction loss: 0.0353128/3811 - total loss: 0.2428 - classification loss: 0.0428 - dann loss: 2.1023 - reconstruction loss: 0.0354128/3811 - total loss: 0.2429 - classification loss: 0.0430 - dann loss: 2.1029 - reconstruction loss: 0.0354128/3811 - total loss: 0.2430 - classification loss: 0.0428 - dann loss: 2.1046 - reconstruction loss: 0.035498/3811 - total loss: 0.2427 - classification loss: 0.0432 - dann loss: 2.1007 - reconstruction loss: 0.03540/3811 - total loss: 0.2427 - classification loss: 0.0432 - dann loss: 2.1007 - reconstruction loss: 0.0354Epoch 15/150, Current strat Epoch 15/100
use_perm = True
switching perm
128/3811 - total loss: 0.2403 - classification loss: 0.0528 - dann loss: 2.0746 - reconstruction loss: 0.0344128/3811 - total loss: 0.2408 - classification loss: 0.0377 - dann loss: 2.0932 - reconstruction loss: 0.0347Epoch 8/150, Current strat Epoch 8/100
use_perm = True
switching perm
128/3742 - total loss: 0.3110 - classification loss: 0.0971 - dann loss: 2.7430 - reconstruction loss: 0.0338128/3742 - total loss: 0.3074 - classification loss: 0.0939 - dann loss: 2.6957 - reconstruction loss: 0.0355128/3742 - total loss: 0.3096 - classification loss: 0.0991 - dann loss: 2.7135 - reconstruction loss: 0.0354128/3742 - total loss: 0.3126 - classification loss: 0.1018 - dann loss: 2.7412 - reconstruction loss: 0.0354128/3742 - total loss: 0.3110 - classification loss: 0.1031 - dann loss: 2.7228 - reconstruction loss: 0.0355128/3742 - total loss: 0.3085 - classification loss: 0.0983 - dann loss: 2.7038 - reconstruction loss: 0.0354128/3742 - total loss: 0.3073 - classification loss: 0.0975 - dann loss: 2.6911 - reconstruction loss: 0.0356128/3742 - total loss: 0.3076 - classification loss: 0.0962 - dann loss: 2.6957 - reconstruction loss: 0.0355128/3742 - total loss: 0.3062 - classification loss: 0.0970 - dann loss: 2.6817 - reconstruction loss: 0.0354128/3742 - total loss: 0.3057 - classification loss: 0.0958 - dann loss: 2.6784 - reconstruction loss: 0.0354128/3742 - total loss: 0.3053 - classification loss: 0.1001 - dann loss: 2.6696 - reconstruction loss: 0.0354128/3742 - total loss: 0.3047 - classification loss: 0.0992 - dann loss: 2.6643 - reconstruction loss: 0.0354128/3742 - total loss: 0.3040 - classification loss: 0.0972 - dann loss: 2.6592 - reconstruction loss: 0.0354128/3742 - total loss: 0.3038 - classification loss: 0.0967 - dann loss: 2.6583 - reconstruction loss: 0.0354128/3742 - total loss: 0.3034 - classification loss: 0.0951 - dann loss: 2.6560 - reconstruction loss: 0.0353128/3742 - total loss: 0.3032 - classification loss: 0.0949 - dann loss: 2.6555 - reconstruction loss: 0.0352128/3742 - total loss: 0.3028 - classification loss: 0.0957 - dann loss: 2.6502 - reconstruction loss: 0.0353128/3742 - total loss: 0.3025 - classification loss: 0.0956 - dann loss: 2.6459 - reconstruction loss: 0.0354128/3742 - total loss: 0.3023 - classification loss: 0.0994 - dann loss: 2.6398 - reconstruction loss: 0.0354128/3742 - total loss: 0.3021 - classification loss: 0.0993 - dann loss: 2.6386 - reconstruction loss: 0.0354128/3742 - total loss: 0.3013 - classification loss: 0.0983 - dann loss: 2.6321 - reconstruction loss: 0.0353128/3742 - total loss: 0.3012 - classification loss: 0.0978 - dann loss: 2.6313 - reconstruction loss: 0.0354128/3742 - total loss: 0.3009 - classification loss: 0.0981 - dann loss: 2.6274 - reconstruction loss: 0.0354128/3742 - total loss: 0.3004 - classification loss: 0.0977 - dann loss: 2.6230 - reconstruction loss: 0.0354128/3742 - total loss: 0.3000 - classification loss: 0.0967 - dann loss: 2.6198 - reconstruction loss: 0.0354128/3742 - total loss: 0.2998 - classification loss: 0.0986 - dann loss: 2.6166 - reconstruction loss: 0.0354128/3742 - total loss: 0.2996 - classification loss: 0.0980 - dann loss: 2.6150 - reconstruction loss: 0.0353128/3742 - total loss: 0.2994 - classification loss: 0.0989 - dann loss: 2.6118 - reconstruction loss: 0.0354128/3742 - total loss: 0.2992 - classification loss: 0.0983 - dann loss: 2.6101 - reconstruction loss: 0.035429/3742 - total loss: 0.2988 - classification loss: 0.0976 - dann loss: 2.6069 - reconstruction loss: 0.03540/3742 - total loss: 0.2988 - classification loss: 0.0976 - dann loss: 2.6069 - reconstruction loss: 0.0354Epoch 9/150, Current strat Epoch 9/100
use_perm = True
switching perm
128/3742 - total loss: 0.3016 - classification loss: 0.0875 - dann loss: 2.6383 - reconstruction loss: 0.0363128/3742 - total loss: 0.3042 - classification loss: 0.0992 - dann loss: 2.6517 - reconstruction loss: 0.0364128/3742 - total loss: 0.3023 - classification loss: 0.0913 - dann loss: 2.6511 - reconstruction loss: 0.0351128/3742 - total loss: 0.3016 - classification loss: 0.0910 - dann loss: 2.6411 - reconstruction loss: 0.0355128/3742 - total loss: 0.3014 - classification loss: 0.0904 - dann loss: 2.6385 - reconstruction loss: 0.0356128/3742 - total loss: 0.3012 - classification loss: 0.0893 - dann loss: 2.6352 - reconstruction loss: 0.0359128/3742 - total loss: 0.3002 - classification loss: 0.0935 - dann loss: 2.6231 - reconstruction loss: 0.0357128/3742 - total loss: 0.2989 - classification loss: 0.0919 - dann loss: 2.6113 - reconstruction loss: 0.0357128/3742 - total loss: 0.2978 - classification loss: 0.0888 - dann loss: 2.6034 - reconstruction loss: 0.0357128/3742 - total loss: 0.2980 - classification loss: 0.0907 - dann loss: 2.6042 - reconstruction loss: 0.0356128/3742 - total loss: 0.2974 - classification loss: 0.0893 - dann loss: 2.6024 - reconstruction loss: 0.0353128/3742 - total loss: 0.2975 - classification loss: 0.0912 - dann loss: 2.6013 - reconstruction loss: 0.0354128/3742 - total loss: 0.2968 - classification loss: 0.0924 - dann loss: 2.5923 - reconstruction loss: 0.0355128/3742 - total loss: 0.2965 - classification loss: 0.0902 - dann loss: 2.5908 - reconstruction loss: 0.0355128/3742 - total loss: 0.2965 - classification loss: 0.0897 - dann loss: 2.5912 - reconstruction loss: 0.0355128/3742 - total loss: 0.2961 - classification loss: 0.0878 - dann loss: 2.5883 - reconstruction loss: 0.0356128/3742 - total loss: 0.2960 - classification loss: 0.0865 - dann loss: 2.5885 - reconstruction loss: 0.0356128/3742 - total loss: 0.2957 - classification loss: 0.0850 - dann loss: 2.5875 - reconstruction loss: 0.0355128/3742 - total loss: 0.2953 - classification loss: 0.0865 - dann loss: 2.5819 - reconstruction loss: 0.0356128/3742 - total loss: 0.2953 - classification loss: 0.0855 - dann loss: 2.5821 - reconstruction loss: 0.0357128/3742 - total loss: 0.2953 - classification loss: 0.0853 - dann loss: 2.5832 - reconstruction loss: 0.0356128/3742 - total loss: 0.2944 - classification loss: 0.0851 - dann loss: 2.5749 - reconstruction loss: 0.0356128/3742 - total loss: 0.2941 - classification loss: 0.0848 - dann loss: 2.5718 - reconstruction loss: 0.0355128/3742 - total loss: 0.2939 - classification loss: 0.0856 - dann loss: 2.5693 - reconstruction loss: 0.0356128/3742 - total loss: 0.2940 - classification loss: 0.0862 - dann loss: 2.5699 - reconstruction loss: 0.0355128/3742 - total loss: 0.2939 - classification loss: 0.0859 - dann loss: 2.5696 - reconstruction loss: 0.0355128/3742 - total loss: 0.2941 - classification loss: 0.0862 - dann loss: 2.5708 - reconstruction loss: 0.0355128/3742 - total loss: 0.2939 - classification loss: 0.0854 - dann loss: 2.5703 - reconstruction loss: 0.0355128/3742 - total loss: 0.2935 - classification loss: 0.0848 - dann loss: 2.5664 - reconstruction loss: 0.035529/3742 - total loss: 0.2927 - classification loss: 0.0836 - dann loss: 2.5598 - reconstruction loss: 0.03550/3742 - total loss: 0.2927 - classification loss: 0.0836 - dann loss: 2.5598 - reconstruction loss: 0.0355Epoch 10/150, Current strat Epoch 10/100
use_perm = True
switching perm
128/3742 - total loss: 0.2989 - classification loss: 0.0796 - dann loss: 2.6315 - reconstruction loss: 0.0347128/3742 - total loss: 0.3041 - classification loss: 0.0864 - dann loss: 2.6685 - reconstruction loss: 0.0358128/3742 - total loss: 0.3029 - classification loss: 0.0793 - dann loss: 2.6635 - reconstruction loss: 0.0358128/3742 - total loss: 0.3037 - classification loss: 0.0857 - dann loss: 2.6643 - reconstruction loss: 0.0358128/3742 - total loss: 0.3016 - classification loss: 0.0778 - dann loss: 2.6561 - reconstruction loss: 0.0353128/3742 - total loss: 0.3019 - classification loss: 0.0740 - dann loss: 2.6628 - reconstruction loss: 0.0353128/3742 - total loss: 0.3014 - classification loss: 0.0742 - dann loss: 2.6559 - reconstruction loss: 0.0355128/3742 - total loss: 0.2997 - classification loss: 0.0730 - dann loss: 2.6413 - reconstruction loss: 0.0353128/3742 - total loss: 0.2991 - classification loss: 0.0693 - dann loss: 2.6388 - reconstruction loss: 0.0354128/3742 - total loss: 0.2980 - classification loss: 0.0706 - dann loss: 2.6260 - reconstruction loss: 0.0354128/3742 - total loss: 0.2975 - classification loss: 0.0689 - dann loss: 2.6226 - reconstruction loss: 0.0354128/3811 - total loss: 0.2449 - classification loss: 0.0371 - dann loss: 2.1307 - reconstruction loss: 0.0352128/3811 - total loss: 0.2472 - classification loss: 0.0398 - dann loss: 2.1515 - reconstruction loss: 0.0350128/3811 - total loss: 0.2466 - classification loss: 0.0401 - dann loss: 2.1471 - reconstruction loss: 0.0348128/3811 - total loss: 0.2476 - classification loss: 0.0404 - dann loss: 2.1544 - reconstruction loss: 0.0352128/3811 - total loss: 0.2483 - classification loss: 0.0408 - dann loss: 2.1602 - reconstruction loss: 0.0352128/3811 - total loss: 0.2483 - classification loss: 0.0398 - dann loss: 2.1608 - reconstruction loss: 0.0353128/3811 - total loss: 0.2487 - classification loss: 0.0378 - dann loss: 2.1672 - reconstruction loss: 0.0352128/3811 - total loss: 0.2481 - classification loss: 0.0371 - dann loss: 2.1629 - reconstruction loss: 0.0351128/3811 - total loss: 0.2480 - classification loss: 0.0368 - dann loss: 2.1624 - reconstruction loss: 0.0351128/3811 - total loss: 0.2485 - classification loss: 0.0382 - dann loss: 2.1649 - reconstruction loss: 0.0352128/3811 - total loss: 0.2480 - classification loss: 0.0377 - dann loss: 2.1601 - reconstruction loss: 0.0353128/3811 - total loss: 0.2473 - classification loss: 0.0387 - dann loss: 2.1517 - reconstruction loss: 0.0353128/3811 - total loss: 0.2468 - classification loss: 0.0388 - dann loss: 2.1454 - reconstruction loss: 0.0354128/3811 - total loss: 0.2471 - classification loss: 0.0394 - dann loss: 2.1481 - reconstruction loss: 0.0355128/3811 - total loss: 0.2471 - classification loss: 0.0395 - dann loss: 2.1491 - reconstruction loss: 0.0353128/3811 - total loss: 0.2475 - classification loss: 0.0407 - dann loss: 2.1514 - reconstruction loss: 0.0353128/3811 - total loss: 0.2471 - classification loss: 0.0403 - dann loss: 2.1485 - reconstruction loss: 0.0353128/3811 - total loss: 0.2464 - classification loss: 0.0405 - dann loss: 2.1415 - reconstruction loss: 0.0352128/3811 - total loss: 0.2468 - classification loss: 0.0411 - dann loss: 2.1446 - reconstruction loss: 0.0353128/3811 - total loss: 0.2460 - classification loss: 0.0407 - dann loss: 2.1382 - reconstruction loss: 0.0352128/3811 - total loss: 0.2459 - classification loss: 0.0404 - dann loss: 2.1377 - reconstruction loss: 0.0352128/3811 - total loss: 0.2457 - classification loss: 0.0407 - dann loss: 2.1350 - reconstruction loss: 0.0352128/3811 - total loss: 0.2460 - classification loss: 0.0409 - dann loss: 2.1384 - reconstruction loss: 0.0351128/3811 - total loss: 0.2462 - classification loss: 0.0415 - dann loss: 2.1390 - reconstruction loss: 0.0351128/3811 - total loss: 0.2463 - classification loss: 0.0414 - dann loss: 2.1403 - reconstruction loss: 0.0352128/3811 - total loss: 0.2465 - classification loss: 0.0412 - dann loss: 2.1418 - reconstruction loss: 0.0352128/3811 - total loss: 0.2461 - classification loss: 0.0409 - dann loss: 2.1378 - reconstruction loss: 0.035398/3811 - total loss: 0.2461 - classification loss: 0.0408 - dann loss: 2.1380 - reconstruction loss: 0.03530/3811 - total loss: 0.2461 - classification loss: 0.0408 - dann loss: 2.1380 - reconstruction loss: 0.0353Epoch 16/150, Current strat Epoch 16/100
use_perm = True
switching perm
128/3811 - total loss: 0.2578 - classification loss: 0.0289 - dann loss: 2.2711 - reconstruction loss: 0.0347128/3811 - total loss: 0.2521 - classification loss: 0.0275 - dann loss: 2.2185 - reconstruction loss: 0.0344128/3811 - total loss: 0.2490 - classification loss: 0.0263 - dann loss: 2.1886 - reconstruction loss: 0.0344128/3811 - total loss: 0.2500 - classification loss: 0.0261 - dann loss: 2.1982 - reconstruction loss: 0.0345128/3811 - total loss: 0.2492 - classification loss: 0.0262 - dann loss: 2.1885 - reconstruction loss: 0.0346128/3811 - total loss: 0.2497 - classification loss: 0.0260 - dann loss: 2.1913 - reconstruction loss: 0.0349128/3811 - total loss: 0.2494 - classification loss: 0.0256 - dann loss: 2.1907 - reconstruction loss: 0.0347128/3811 - total loss: 0.2483 - classification loss: 0.0273 - dann loss: 2.1772 - reconstruction loss: 0.0349128/3811 - total loss: 0.2481 - classification loss: 0.0288 - dann loss: 2.1740 - reconstruction loss: 0.0348128/3811 - total loss: 0.2478 - classification loss: 0.0288 - dann loss: 2.1697 - reconstruction loss: 0.0349128/3811 - total loss: 0.2470 - classification loss: 0.0288 - dann loss: 2.1623 - reconstruction loss: 0.0348128/3811 - total loss: 0.2464 - classification loss: 0.0296 - dann loss: 2.1552 - reconstruction loss: 0.0348128/3811 - total loss: 0.2459 - classification loss: 0.0295 - dann loss: 2.1516 - reconstruction loss: 0.0347128/3811 - total loss: 0.2454 - classification loss: 0.0322 - dann loss: 2.1441 - reconstruction loss: 0.0347128/3811 - total loss: 0.2452 - classification loss: 0.0321 - dann loss: 2.1423 - reconstruction loss: 0.0347128/3811 - total loss: 0.2446 - classification loss: 0.0325 - dann loss: 2.1356 - reconstruction loss: 0.0347128/3811 - total loss: 0.2444 - classification loss: 0.0324 - dann loss: 2.1333 - reconstruction loss: 0.0347128/3811 - total loss: 0.2439 - classification loss: 0.0322 - dann loss: 2.1296 - reconstruction loss: 0.0347128/3811 - total loss: 0.2436 - classification loss: 0.0326 - dann loss: 2.1269 - reconstruction loss: 0.0345128/3811 - total loss: 0.2435 - classification loss: 0.0327 - dann loss: 2.1259 - reconstruction loss: 0.0345128/3811 - total loss: 0.2435 - classification loss: 0.0336 - dann loss: 2.1247 - reconstruction loss: 0.0346128/3811 - total loss: 0.2435 - classification loss: 0.0333 - dann loss: 2.1245 - reconstruction loss: 0.0346128/3811 - total loss: 0.2435 - classification loss: 0.0336 - dann loss: 2.1242 - reconstruction loss: 0.0347128/3811 - total loss: 0.2434 - classification loss: 0.0335 - dann loss: 2.1221 - reconstruction loss: 0.0348128/3811 - total loss: 0.2430 - classification loss: 0.0332 - dann loss: 2.1178 - reconstruction loss: 0.0348128/3811 - total loss: 0.2429 - classification loss: 0.0331 - dann loss: 2.1164 - reconstruction loss: 0.0349128/3811 - total loss: 0.2430 - classification loss: 0.0331 - dann loss: 2.1173 - reconstruction loss: 0.0349128/3811 - total loss: 0.2431 - classification loss: 0.0327 - dann loss: 2.1183 - reconstruction loss: 0.0350128/3811 - total loss: 0.2426 - classification loss: 0.0326 - dann loss: 2.1131 - reconstruction loss: 0.035198/3811 - total loss: 0.2429 - classification loss: 0.0326 - dann loss: 2.1152 - reconstruction loss: 0.03520/3811 - total loss: 0.2429 - classification loss: 0.0326 - dann loss: 2.1152 - reconstruction loss: 0.0352Epoch 17/150, Current strat Epoch 17/100
use_perm = True
switching perm
128/3811 - total loss: 0.2444 - classification loss: 0.0188 - dann loss: 2.1392 - reconstruction loss: 0.0358128/3811 - total loss: 0.2412 - classification loss: 0.0208 - dann loss: 2.1110 - reconstruction loss: 0.0350128/3811 - total loss: 0.2414 - classification loss: 0.0278 - dann loss: 2.1083 - reconstruction loss: 0.0347128/3811 - total loss: 0.2406 - classification loss: 0.0260 - dann loss: 2.1018 - reconstruction loss: 0.0348128/3811 - total loss: 0.2403 - classification loss: 0.0264 - dann loss: 2.0968 - reconstruction loss: 0.0350128/3811 - total loss: 0.2400 - classification loss: 0.0253 - dann loss: 2.0957 - reconstruction loss: 0.0349128/3811 - total loss: 0.2389 - classification loss: 0.0250 - dann loss: 2.0849 - reconstruction loss: 0.0349128/3811 - total loss: 0.2386 - classification loss: 0.0287 - dann loss: 2.0782 - reconstruction loss: 0.0348128/3811 - total loss: 0.2394 - classification loss: 0.0317 - dann loss: 2.0825 - reconstruction loss: 0.0350128/3811 - total loss: 0.2394 - classification loss: 0.0317 - dann loss: 2.0828 - reconstruction loss: 0.0350128/3811 - total loss: 0.2392 - classification loss: 0.0312 - dann loss: 2.0807 - reconstruction loss: 0.0350128/3811 - total loss: 0.2392 - classification loss: 0.0316 - dann loss: 2.0798 - reconstruction loss: 0.0350128/3811 - total loss: 0.2389 - classification loss: 0.0312 - dann loss: 2.0780 - reconstruction loss: 0.0350128/3811 - total loss: 0.2386 - classification loss: 0.0302 - dann loss: 2.0758 - reconstruction loss: 0.0350128/3811 - total loss: 0.2377 - classification loss: 0.0297 - dann loss: 2.0669 - reconstruction loss: 0.0350128/3811 - total loss: 0.2376 - classification loss: 0.0295 - dann loss: 2.0677 - reconstruction loss: 0.0348128/3811 - total loss: 0.2379 - classification loss: 0.0303 - dann loss: 2.0688 - reconstruction loss: 0.0350128/3811 - total loss: 0.2376 - classification loss: 0.0311 - dann loss: 2.0658 - reconstruction loss: 0.0350128/3811 - total loss: 0.2376 - classification loss: 0.0307 - dann loss: 2.0651 - reconstruction loss: 0.0350128/3811 - total loss: 0.2374 - classification loss: 0.0305 - dann loss: 2.0633 - reconstruction loss: 0.0350128/3811 - total loss: 0.2371 - classification loss: 0.0305 - dann loss: 2.0607 - reconstruction loss: 0.0350128/3811 - total loss: 0.2370 - classification loss: 0.0302 - dann loss: 2.0593 - reconstruction loss: 0.0350128/3811 - total loss: 0.2364 - classification loss: 0.0297 - dann loss: 2.0537 - reconstruction loss: 0.0351128/3811 - total loss: 0.2365 - classification loss: 0.0294 - dann loss: 2.0544 - reconstruction loss: 0.0352128/3811 - total loss: 0.2362 - classification loss: 0.0293 - dann loss: 2.0517 - reconstruction loss: 0.0352128/3811 - total loss: 0.2362 - classification loss: 0.0289 - dann loss: 2.0518 - reconstruction loss: 0.0352128/3811 - total loss: 0.2360 - classification loss: 0.0284 - dann loss: 2.0501 - reconstruction loss: 0.0352128/3811 - total loss: 0.2357 - classification loss: 0.0287 - dann loss: 2.0474 - reconstruction loss: 0.0352128/3811 - total loss: 0.2357 - classification loss: 0.0287 - dann loss: 2.0480 - reconstruction loss: 0.035198/3811 - total loss: 0.2359 - classification loss: 0.0286 - dann loss: 2.0491 - reconstruction loss: 0.03510/3811 - total loss: 0.2359 - classification loss: 0.0286 - dann loss: 2.0491 - reconstruction loss: 0.0351Epoch 18/150, Current strat Epoch 18/100
use_perm = True
switching perm
128/3811 - total loss: 0.2454 - classification loss: 0.0364 - dann loss: 2.1296 - reconstruction loss: 0.0360128/3811 - total loss: 0.2400 - classification loss: 0.0273 - dann loss: 2.0933 - reconstruction loss: 0.0349128/3811 - total loss: 0.2396 - classification loss: 0.0293 - dann loss: 2.0940 - reconstruction loss: 0.0340128/3811 - total loss: 0.2390 - classification loss: 0.0299 - dann loss: 2.0888 - reconstruction loss: 0.0339128/3811 - total loss: 0.2380 - classification loss: 0.0290 - dann loss: 2.0768 - reconstruction loss: 0.0343128/3811 - total loss: 0.2384 - classification loss: 0.0273 - dann loss: 2.0818 - reconstruction loss: 0.0343128/3811 - total loss: 0.2374 - classification loss: 0.0270 - dann loss: 2.0711 - reconstruction loss: 0.0345128/3811 - total loss: 0.2369 - classification loss: 0.0263 - dann loss: 2.0661 - reconstruction loss: 0.0345128/3811 - total loss: 0.2372 - classification loss: 0.0256 - dann loss: 2.0687 - reconstruction loss: 0.0347128/3811 - total loss: 0.2373 - classification loss: 0.0267 - dann loss: 2.0679 - reconstruction loss: 0.0347128/3811 - total loss: 0.2370 - classification loss: 0.0260 - dann loss: 2.0651 - reconstruction loss: 0.0349128/3811 - total loss: 0.2369 - classification loss: 0.0261 - dann loss: 2.0640 - reconstruction loss: 0.0349128/3811 - total loss: 0.2374 - classification loss: 0.0263 - dann loss: 2.0674 - reconstruction loss: 0.0350128/3811 - total loss: 0.2370 - classification loss: 0.0269 - dann loss: 2.0621 - reconstruction loss: 0.0351128/3811 - total loss: 0.2373 - classification loss: 0.0267 - dann loss: 2.0664 - reconstruction loss: 0.0350128/3811 - total loss: 0.2369 - classification loss: 0.0263 - dann loss: 2.0629 - reconstruction loss: 0.0350128/3811 - total loss: 0.2372 - classification loss: 0.0273 - dann loss: 2.0645 - reconstruction loss: 0.0350128/3811 - total loss: 0.2369 - classification loss: 0.0283 - dann loss: 2.0598 - reconstruction loss: 0.0351128/3811 - total loss: 0.2369 - classification loss: 0.0280 - dann loss: 2.0605 - reconstruction loss: 0.0351128/3811 - total loss: 0.2371 - classification loss: 0.0281 - dann loss: 2.0614 - reconstruction loss: 0.0351128/3811 - total loss: 0.2372 - classification loss: 0.0278 - dann loss: 2.0616 - reconstruction loss: 0.0353128/3811 - total loss: 0.2367 - classification loss: 0.0274 - dann loss: 2.0574 - reconstruction loss: 0.0352128/3811 - total loss: 0.2369 - classification loss: 0.0282 - dann loss: 2.0596 - reconstruction loss: 0.0351128/3811 - total loss: 0.2368 - classification loss: 0.0282 - dann loss: 2.0599 - reconstruction loss: 0.0350128/3811 - total loss: 0.2367 - classification loss: 0.0283 - dann loss: 2.0580 - reconstruction loss: 0.0350128/3811 - total loss: 0.2369 - classification loss: 0.0287 - dann loss: 2.0593 - reconstruction loss: 0.0351128/3811 - total loss: 0.2368 - classification loss: 0.0284 - dann loss: 2.0589 - reconstruction loss: 0.0351128/3811 - total loss: 0.2369 - classification loss: 0.0281 - dann loss: 2.0598 - reconstruction loss: 0.0351128/3811 - total loss: 0.2371 - classification loss: 0.0280 - dann loss: 2.0623 - reconstruction loss: 0.035198/3811 - total loss: 0.2371 - classification loss: 0.0282 - dann loss: 2.0620 - reconstruction loss: 0.03510/3811 - total loss: 0.2371 - classification loss: 0.0282 - dann loss: 2.0620 - reconstruction loss: 0.0351Epoch 19/150, Current strat Epoch 19/100
use_perm = True
switching perm
128/3811 - total loss: 0.2348 - classification loss: 0.0193 - dann loss: 2.0620 - reconstruction loss: 0.0333128/3811 - total loss: 0.2393 - classification loss: 0.0179 - dann loss: 2.1009 - reconstruction loss: 0.0343128/3811 - total loss: 0.2398 - classification loss: 0.0296 - dann loss: 2.0932 - reconstruction loss: 0.0344128/3811 - total loss: 0.2421 - classification loss: 0.0301 - dann loss: 2.1148 - reconstruction loss: 0.0345128/3811 - total loss: 0.2417 - classification loss: 0.0278 - dann loss: 2.1109 - reconstruction loss: 0.0347128/3811 - total loss: 0.2409 - classification loss: 0.0258 - dann loss: 2.1069 - reconstruction loss: 0.0346128/3811 - total loss: 0.2408 - classification loss: 0.0264 - dann loss: 2.1041 - reconstruction loss: 0.0347128/3811 - total loss: 0.2403 - classification loss: 0.0253 - dann loss: 2.0982 - reconstruction loss: 0.0349128/3811 - total loss: 0.2400 - classification loss: 0.0250 - dann loss: 2.0963 - reconstruction loss: 0.0348128/3811 - total loss: 0.2399 - classification loss: 0.0254 - dann loss: 2.0955 - reconstruction loss: 0.0348128/3811 - total loss: 0.2394 - classification loss: 0.0251 - dann loss: 2.0900 - reconstruction loss: 0.0348128/3811 - total loss: 0.2386 - classification loss: 0.0240 - dann loss: 2.0828 - reconstruction loss: 0.0349128/3811 - total loss: 0.2389 - classification loss: 0.0237 - dann loss: 2.0854 - reconstruction loss: 0.0350128/3811 - total loss: 0.2392 - classification loss: 0.0257 - dann loss: 2.0861 - reconstruction loss: 0.0350128/3811 - total loss: 0.2391 - classification loss: 0.0256 - dann loss: 2.0836 - reconstruction loss: 0.0353128/3811 - total loss: 0.2392 - classification loss: 0.0251 - dann loss: 2.0855 - reconstruction loss: 0.0352128/3811 - total loss: 0.2390 - classification loss: 0.0246 - dann loss: 2.0839 - reconstruction loss: 0.0351128/3811 - total loss: 0.2382 - classification loss: 0.0247 - dann loss: 2.0767 - reconstruction loss: 0.0351128/3811 - total loss: 0.2380 - classification loss: 0.0243 - dann loss: 2.0754 - reconstruction loss: 0.0351128/3811 - total loss: 0.2378 - classification loss: 0.0244 - dann loss: 2.0726 - reconstruction loss: 0.0351128/3811 - total loss: 0.2379 - classification loss: 0.0244 - dann loss: 2.0742 - reconstruction loss: 0.0351128/3811 - total loss: 0.2379 - classification loss: 0.0239 - dann loss: 2.0739 - reconstruction loss: 0.0351128/3811 - total loss: 0.2374 - classification loss: 0.0235 - dann loss: 2.0699 - reconstruction loss: 0.0351128/3811 - total loss: 0.2373 - classification loss: 0.0234 - dann loss: 2.0689 - reconstruction loss: 0.0350128/3811 - total loss: 0.2371 - classification loss: 0.0231 - dann loss: 2.0682 - reconstruction loss: 0.0350128/3811 - total loss: 0.2369 - classification loss: 0.0230 - dann loss: 2.0658 - reconstruction loss: 0.0350128/3811 - total loss: 0.2370 - classification loss: 0.0239 - dann loss: 2.0655 - reconstruction loss: 0.0351128/3811 - total loss: 0.2372 - classification loss: 0.0240 - dann loss: 2.0674 - reconstruction loss: 0.0351128/3811 - total loss: 0.2372 - classification loss: 0.0244 - dann loss: 2.0666 - reconstruction loss: 0.035198/3811 - total loss: 0.2367 - classification loss: 0.0243 - dann loss: 2.0624 - reconstruction loss: 0.03510/3811 - total loss: 0.2367 - classification loss: 0.0243 - dann loss: 2.0624 - reconstruction loss: 0.0351Epoch 20/150, Current strat Epoch 20/100
use_perm = True
switching perm
128/3811 - total loss: 0.2327 - classification loss: 0.0220 - dann loss: 2.0156 - reconstruction loss: 0.0362128/3811 - total loss: 0.2317 - classification loss: 0.0274 - dann loss: 2.0047 - reconstruction loss: 0.0356128/3811 - total loss: 0.2318 - classification loss: 0.0257 - dann loss: 2.0166 - reconstruction loss: 0.0344128/3811 - total loss: 0.2305 - classification loss: 0.0247 - dann loss: 2.0032 - reconstruction loss: 0.0347128/3811 - total loss: 0.2324 - classification loss: 0.0256 - dann loss: 2.0209 - reconstruction loss: 0.0347128/3811 - total loss: 0.2328 - classification loss: 0.0252 - dann loss: 2.0239 - reconstruction loss: 0.0349128/3811 - total loss: 0.2340 - classification loss: 0.0236 - dann loss: 2.0337 - reconstruction loss: 0.0354128/3811 - total loss: 0.2330 - classification loss: 0.0262 - dann loss: 2.0228 - reconstruction loss: 0.0352128/3811 - total loss: 0.2333 - classification loss: 0.0267 - dann loss: 2.0254 - reconstruction loss: 0.0351128/3811 - total loss: 0.2341 - classification loss: 0.0269 - dann loss: 2.0324 - reconstruction loss: 0.0352128/3811 - total loss: 0.2343 - classification loss: 0.0268 - dann loss: 2.0322 - reconstruction loss: 0.0355128/3811 - total loss: 0.2345 - classification loss: 0.0278 - dann loss: 2.0336 - reconstruction loss: 0.0355128/3811 - total loss: 0.2345 - classification loss: 0.0269 - dann loss: 2.0346 - reconstruction loss: 0.0354128/3811 - total loss: 0.2349 - classification loss: 0.0269 - dann loss: 2.0387 - reconstruction loss: 0.0354128/3811 - total loss: 0.2350 - classification loss: 0.0268 - dann loss: 2.0402 - reconstruction loss: 0.0354128/3811 - total loss: 0.2349 - classification loss: 0.0266 - dann loss: 2.0397 - reconstruction loss: 0.0353128/3811 - total loss: 0.2353 - classification loss: 0.0273 - dann loss: 2.0425 - reconstruction loss: 0.0353128/3811 - total loss: 0.2356 - classification loss: 0.0273 - dann loss: 2.0456 - reconstruction loss: 0.0354128/3811 - total loss: 0.2356 - classification loss: 0.0266 - dann loss: 2.0459 - reconstruction loss: 0.0354128/3811 - total loss: 0.2355 - classification loss: 0.0268 - dann loss: 2.0457 - reconstruction loss: 0.0353128/3811 - total loss: 0.2356 - classification loss: 0.0263 - dann loss: 2.0467 - reconstruction loss: 0.0354128/3811 - total loss: 0.2356 - classification loss: 0.0266 - dann loss: 2.0460 - reconstruction loss: 0.0354128/3811 - total loss: 0.2355 - classification loss: 0.0265 - dann loss: 2.0457 - reconstruction loss: 0.0353128/3811 - total loss: 0.2351 - classification loss: 0.0271 - dann loss: 2.0415 - reconstruction loss: 0.0353128/3811 - total loss: 0.2350 - classification loss: 0.0271 - dann loss: 2.0401 - reconstruction loss: 0.0353128/3811 - total loss: 0.2352 - classification loss: 0.0269 - dann loss: 2.0431 - reconstruction loss: 0.0353128/3811 - total loss: 0.2349 - classification loss: 0.0266 - dann loss: 2.0410 - reconstruction loss: 0.0352128/3811 - total loss: 0.2345 - classification loss: 0.0271 - dann loss: 2.0364 - reconstruction loss: 0.0352128/3811 - total loss: 0.2340 - classification loss: 0.0271 - dann loss: 2.0313 - reconstruction loss: 0.035198/3811 - total loss: 0.2339 - classification loss: 0.0274 - dann loss: 2.0303 - reconstruction loss: 0.03510/3811 - total loss: 0.2339 - classification loss: 0.0274 - dann loss: 2.0303 - reconstruction loss: 0.0351Epoch 21/150, Current strat Epoch 21/100
use_perm = True
switching perm
128/3811 - total loss: 0.2364 - classification loss: 0.0277 - dann loss: 2.0534 - reconstruction loss: 0.0353128/3811 - total loss: 0.2358 - classification loss: 0.0292 - dann loss: 2.0438 - reconstruction loss: 0.0356128/3811 - total loss: 0.2358 - classification loss: 0.0257 - dann loss: 2.0450 - reconstruction loss: 0.0359128/3811 - total loss: 0.2339 - classification loss: 0.0229 - dann loss: 2.0255 - reconstruction loss: 0.0363128/3811 - total loss: 0.2347 - classification loss: 0.0250 - dann loss: 2.0358 - reconstruction loss: 0.0358128/3811 - total loss: 0.2339 - classification loss: 0.0237 - dann loss: 2.0306 - reconstruction loss: 0.0356128/3811 - total loss: 0.2327 - classification loss: 0.0241 - dann loss: 2.0167 - reconstruction loss: 0.0357128/3811 - total loss: 0.2331 - classification loss: 0.0234 - dann loss: 2.0240 - reconstruction loss: 0.0355128/3811 - total loss: 0.2329 - classification loss: 0.0233 - dann loss: 2.0220 - reconstruction loss: 0.0355128/3811 - total loss: 0.2335 - classification loss: 0.0238 - dann loss: 2.0290 - reconstruction loss: 0.0353128/3811 - total loss: 0.2328 - classification loss: 0.0241 - dann loss: 2.0222 - reconstruction loss: 0.0352128/3811 - total loss: 0.2328 - classification loss: 0.0234 - dann loss: 2.0222 - reconstruction loss: 0.0353128/3811 - total loss: 0.2329 - classification loss: 0.0233 - dann loss: 2.0229 - reconstruction loss: 0.0354128/3811 - total loss: 0.2331 - classification loss: 0.0230 - dann loss: 2.0272 - reconstruction loss: 0.0352128/3811 - total loss: 0.2336 - classification loss: 0.0226 - dann loss: 2.0315 - reconstruction loss: 0.0352128/3811 - total loss: 0.2335 - classification loss: 0.0222 - dann loss: 2.0326 - reconstruction loss: 0.0350128/3811 - total loss: 0.2333 - classification loss: 0.0223 - dann loss: 2.0315 - reconstruction loss: 0.0349128/3811 - total loss: 0.2330 - classification loss: 0.0217 - dann loss: 2.0291 - reconstruction loss: 0.0350128/3811 - total loss: 0.2325 - classification loss: 0.0214 - dann loss: 2.0239 - reconstruction loss: 0.0350128/3811 - total loss: 0.2324 - classification loss: 0.0216 - dann loss: 2.0220 - reconstruction loss: 0.0351128/3811 - total loss: 0.2327 - classification loss: 0.0213 - dann loss: 2.0243 - reconstruction loss: 0.0351128/3811 - total loss: 0.2324 - classification loss: 0.0210 - dann loss: 2.0220 - reconstruction loss: 0.0351128/3811 - total loss: 0.2325 - classification loss: 0.0211 - dann loss: 2.0234 - reconstruction loss: 0.0351128/3811 - total loss: 0.2319 - classification loss: 0.0210 - dann loss: 2.0175 - reconstruction loss: 0.0351128/3811 - total loss: 0.2317 - classification loss: 0.0209 - dann loss: 2.0155 - reconstruction loss: 0.0351128/3811 - total loss: 0.2316 - classification loss: 0.0205 - dann loss: 2.0153 - reconstruction loss: 0.0350128/3811 - total loss: 0.2318 - classification loss: 0.0203 - dann loss: 2.0173 - reconstruction loss: 0.0351128/3811 - total loss: 0.2317 - classification loss: 0.0207 - dann loss: 2.0158 - reconstruction loss: 0.0351128/3811 - total loss: 0.2317 - classification loss: 0.0210 - dann loss: 2.0153 - reconstruction loss: 0.035198/3811 - total loss: 0.2316 - classification loss: 0.0215 - dann loss: 2.0135 - reconstruction loss: 0.03510/3811 - total loss: 0.2316 - classification loss: 0.0215 - dann loss: 2.0135 - reconstruction loss: 0.0351Epoch 22/150, Current strat Epoch 22/100
use_perm = True
switching perm
128/3811 - total loss: 0.2328 - classification loss: 0.0119 - dann loss: 2.0378 - reconstruction loss: 0.0348128/3811 - total loss: 0.2300 - classification loss: 0.0165 - dann loss: 2.0090 - reconstruction loss: 0.0343128/3811 - total loss: 0.2350 - classification loss: 0.0166 - dann loss: 2.0543 - reconstruction loss: 0.0348128/3811 - total loss: 0.2339 - classification loss: 0.0153 - dann loss: 2.0438 - reconstruction loss: 0.0350128/3811 - total loss: 0.2341 - classification loss: 0.0154 - dann loss: 2.0436 - reconstruction loss: 0.0353128/3811 - total loss: 0.2332 - classification loss: 0.0173 - dann loss: 2.0311 - reconstruction loss: 0.0355128/3742 - total loss: 0.2969 - classification loss: 0.0681 - dann loss: 2.6173 - reconstruction loss: 0.0355128/3742 - total loss: 0.2973 - classification loss: 0.0702 - dann loss: 2.6192 - reconstruction loss: 0.0355128/3742 - total loss: 0.2974 - classification loss: 0.0709 - dann loss: 2.6179 - reconstruction loss: 0.0356128/3742 - total loss: 0.2972 - classification loss: 0.0704 - dann loss: 2.6172 - reconstruction loss: 0.0355128/3742 - total loss: 0.2966 - classification loss: 0.0698 - dann loss: 2.6123 - reconstruction loss: 0.0355128/3742 - total loss: 0.2963 - classification loss: 0.0691 - dann loss: 2.6109 - reconstruction loss: 0.0354128/3742 - total loss: 0.2957 - classification loss: 0.0683 - dann loss: 2.6053 - reconstruction loss: 0.0355128/3742 - total loss: 0.2957 - classification loss: 0.0723 - dann loss: 2.6011 - reconstruction loss: 0.0354128/3742 - total loss: 0.2955 - classification loss: 0.0709 - dann loss: 2.6002 - reconstruction loss: 0.0355128/3742 - total loss: 0.2956 - classification loss: 0.0720 - dann loss: 2.5999 - reconstruction loss: 0.0355128/3742 - total loss: 0.2959 - classification loss: 0.0724 - dann loss: 2.6023 - reconstruction loss: 0.0355128/3742 - total loss: 0.2959 - classification loss: 0.0729 - dann loss: 2.6016 - reconstruction loss: 0.0355128/3742 - total loss: 0.2952 - classification loss: 0.0738 - dann loss: 2.5946 - reconstruction loss: 0.0355128/3742 - total loss: 0.2949 - classification loss: 0.0732 - dann loss: 2.5916 - reconstruction loss: 0.0355128/3742 - total loss: 0.2951 - classification loss: 0.0730 - dann loss: 2.5940 - reconstruction loss: 0.0355128/3742 - total loss: 0.2947 - classification loss: 0.0734 - dann loss: 2.5904 - reconstruction loss: 0.0354128/3742 - total loss: 0.2942 - classification loss: 0.0723 - dann loss: 2.5856 - reconstruction loss: 0.0355128/3742 - total loss: 0.2937 - classification loss: 0.0718 - dann loss: 2.5823 - reconstruction loss: 0.035429/3742 - total loss: 0.2935 - classification loss: 0.0726 - dann loss: 2.5786 - reconstruction loss: 0.03540/3742 - total loss: 0.2935 - classification loss: 0.0726 - dann loss: 2.5786 - reconstruction loss: 0.0354Epoch 11/150, Current strat Epoch 11/100
use_perm = True
switching perm
128/3742 - total loss: 0.2934 - classification loss: 0.0372 - dann loss: 2.6244 - reconstruction loss: 0.0341128/3742 - total loss: 0.2958 - classification loss: 0.0542 - dann loss: 2.6198 - reconstruction loss: 0.0355128/3742 - total loss: 0.2927 - classification loss: 0.0514 - dann loss: 2.5907 - reconstruction loss: 0.0357128/3742 - total loss: 0.2908 - classification loss: 0.0492 - dann loss: 2.5769 - reconstruction loss: 0.0352128/3742 - total loss: 0.2912 - classification loss: 0.0491 - dann loss: 2.5819 - reconstruction loss: 0.0351128/3742 - total loss: 0.2912 - classification loss: 0.0482 - dann loss: 2.5835 - reconstruction loss: 0.0351128/3742 - total loss: 0.2910 - classification loss: 0.0493 - dann loss: 2.5811 - reconstruction loss: 0.0350128/3742 - total loss: 0.2927 - classification loss: 0.0501 - dann loss: 2.5955 - reconstruction loss: 0.0352128/3742 - total loss: 0.2925 - classification loss: 0.0499 - dann loss: 2.5933 - reconstruction loss: 0.0352128/3742 - total loss: 0.2920 - classification loss: 0.0485 - dann loss: 2.5909 - reconstruction loss: 0.0351128/3742 - total loss: 0.2908 - classification loss: 0.0489 - dann loss: 2.5778 - reconstruction loss: 0.0352128/3742 - total loss: 0.2902 - classification loss: 0.0492 - dann loss: 2.5719 - reconstruction loss: 0.0351128/3742 - total loss: 0.2905 - classification loss: 0.0510 - dann loss: 2.5733 - reconstruction loss: 0.0351128/3742 - total loss: 0.2910 - classification loss: 0.0534 - dann loss: 2.5739 - reconstruction loss: 0.0353128/3742 - total loss: 0.2906 - classification loss: 0.0552 - dann loss: 2.5677 - reconstruction loss: 0.0353128/3742 - total loss: 0.2903 - classification loss: 0.0554 - dann loss: 2.5644 - reconstruction loss: 0.0354128/3742 - total loss: 0.2900 - classification loss: 0.0550 - dann loss: 2.5625 - reconstruction loss: 0.0353128/3742 - total loss: 0.2898 - classification loss: 0.0548 - dann loss: 2.5618 - reconstruction loss: 0.0352128/3742 - total loss: 0.2895 - classification loss: 0.0541 - dann loss: 2.5590 - reconstruction loss: 0.0352128/3742 - total loss: 0.2898 - classification loss: 0.0539 - dann loss: 2.5617 - reconstruction loss: 0.0352128/3742 - total loss: 0.2902 - classification loss: 0.0549 - dann loss: 2.5650 - reconstruction loss: 0.0353128/3742 - total loss: 0.2900 - classification loss: 0.0554 - dann loss: 2.5618 - reconstruction loss: 0.0353128/3742 - total loss: 0.2896 - classification loss: 0.0552 - dann loss: 2.5589 - reconstruction loss: 0.0353128/3742 - total loss: 0.2894 - classification loss: 0.0552 - dann loss: 2.5564 - reconstruction loss: 0.0353128/3742 - total loss: 0.2890 - classification loss: 0.0556 - dann loss: 2.5527 - reconstruction loss: 0.0353128/3742 - total loss: 0.2886 - classification loss: 0.0560 - dann loss: 2.5476 - reconstruction loss: 0.0354128/3742 - total loss: 0.2885 - classification loss: 0.0552 - dann loss: 2.5476 - reconstruction loss: 0.0353128/3742 - total loss: 0.2879 - classification loss: 0.0542 - dann loss: 2.5438 - reconstruction loss: 0.0352128/3742 - total loss: 0.2880 - classification loss: 0.0537 - dann loss: 2.5449 - reconstruction loss: 0.035229/3742 - total loss: 0.2874 - classification loss: 0.0533 - dann loss: 2.5396 - reconstruction loss: 0.03510/3742 - total loss: 0.2874 - classification loss: 0.0533 - dann loss: 2.5396 - reconstruction loss: 0.0351Epoch 12/150, Current strat Epoch 12/100
use_perm = True
switching perm
128/3742 - total loss: 0.2859 - classification loss: 0.0326 - dann loss: 2.5427 - reconstruction loss: 0.0355128/3742 - total loss: 0.2864 - classification loss: 0.0383 - dann loss: 2.5454 - reconstruction loss: 0.0350128/3742 - total loss: 0.2853 - classification loss: 0.0364 - dann loss: 2.5425 - reconstruction loss: 0.0343128/3742 - total loss: 0.2873 - classification loss: 0.0375 - dann loss: 2.5568 - reconstruction loss: 0.0349128/3742 - total loss: 0.2850 - classification loss: 0.0372 - dann loss: 2.5340 - reconstruction loss: 0.0348128/3742 - total loss: 0.2834 - classification loss: 0.0370 - dann loss: 2.5189 - reconstruction loss: 0.0348128/3742 - total loss: 0.2856 - classification loss: 0.0418 - dann loss: 2.5331 - reconstruction loss: 0.0351128/3742 - total loss: 0.2852 - classification loss: 0.0429 - dann loss: 2.5287 - reconstruction loss: 0.0351128/3742 - total loss: 0.2853 - classification loss: 0.0432 - dann loss: 2.5301 - reconstruction loss: 0.0350128/3742 - total loss: 0.2844 - classification loss: 0.0419 - dann loss: 2.5221 - reconstruction loss: 0.0349128/3742 - total loss: 0.2842 - classification loss: 0.0444 - dann loss: 2.5183 - reconstruction loss: 0.0349128/3742 - total loss: 0.2843 - classification loss: 0.0448 - dann loss: 2.5176 - reconstruction loss: 0.0351128/3742 - total loss: 0.2834 - classification loss: 0.0446 - dann loss: 2.5089 - reconstruction loss: 0.0350128/3742 - total loss: 0.2834 - classification loss: 0.0435 - dann loss: 2.5110 - reconstruction loss: 0.0350128/3742 - total loss: 0.2828 - classification loss: 0.0434 - dann loss: 2.5043 - reconstruction loss: 0.0351128/3742 - total loss: 0.2822 - classification loss: 0.0425 - dann loss: 2.4989 - reconstruction loss: 0.0351128/3742 - total loss: 0.2822 - classification loss: 0.0425 - dann loss: 2.4991 - reconstruction loss: 0.0351128/3742 - total loss: 0.2822 - classification loss: 0.0426 - dann loss: 2.4986 - reconstruction loss: 0.0351128/3742 - total loss: 0.2821 - classification loss: 0.0422 - dann loss: 2.4983 - reconstruction loss: 0.0351128/3742 - total loss: 0.2820 - classification loss: 0.0427 - dann loss: 2.4958 - reconstruction loss: 0.0352128/3742 - total loss: 0.2817 - classification loss: 0.0431 - dann loss: 2.4931 - reconstruction loss: 0.0351128/3742 - total loss: 0.2815 - classification loss: 0.0439 - dann loss: 2.4897 - reconstruction loss: 0.0351128/3742 - total loss: 0.2812 - classification loss: 0.0441 - dann loss: 2.4865 - reconstruction loss: 0.0352128/3811 - total loss: 0.2332 - classification loss: 0.0163 - dann loss: 2.0341 - reconstruction loss: 0.0352128/3811 - total loss: 0.2333 - classification loss: 0.0166 - dann loss: 2.0358 - reconstruction loss: 0.0351128/3811 - total loss: 0.2330 - classification loss: 0.0167 - dann loss: 2.0310 - reconstruction loss: 0.0353128/3811 - total loss: 0.2333 - classification loss: 0.0168 - dann loss: 2.0333 - reconstruction loss: 0.0354128/3811 - total loss: 0.2335 - classification loss: 0.0173 - dann loss: 2.0342 - reconstruction loss: 0.0354128/3811 - total loss: 0.2327 - classification loss: 0.0173 - dann loss: 2.0285 - reconstruction loss: 0.0352128/3811 - total loss: 0.2325 - classification loss: 0.0170 - dann loss: 2.0280 - reconstruction loss: 0.0350128/3811 - total loss: 0.2326 - classification loss: 0.0168 - dann loss: 2.0285 - reconstruction loss: 0.0350128/3811 - total loss: 0.2321 - classification loss: 0.0165 - dann loss: 2.0245 - reconstruction loss: 0.0350128/3811 - total loss: 0.2320 - classification loss: 0.0162 - dann loss: 2.0242 - reconstruction loss: 0.0350128/3811 - total loss: 0.2323 - classification loss: 0.0167 - dann loss: 2.0252 - reconstruction loss: 0.0351128/3811 - total loss: 0.2320 - classification loss: 0.0171 - dann loss: 2.0212 - reconstruction loss: 0.0352128/3811 - total loss: 0.2322 - classification loss: 0.0175 - dann loss: 2.0220 - reconstruction loss: 0.0353128/3811 - total loss: 0.2320 - classification loss: 0.0188 - dann loss: 2.0196 - reconstruction loss: 0.0352128/3811 - total loss: 0.2321 - classification loss: 0.0187 - dann loss: 2.0213 - reconstruction loss: 0.0351128/3811 - total loss: 0.2321 - classification loss: 0.0198 - dann loss: 2.0202 - reconstruction loss: 0.0352128/3811 - total loss: 0.2323 - classification loss: 0.0205 - dann loss: 2.0209 - reconstruction loss: 0.0352128/3811 - total loss: 0.2320 - classification loss: 0.0204 - dann loss: 2.0180 - reconstruction loss: 0.0352128/3811 - total loss: 0.2319 - classification loss: 0.0204 - dann loss: 2.0168 - reconstruction loss: 0.0352128/3811 - total loss: 0.2317 - classification loss: 0.0203 - dann loss: 2.0153 - reconstruction loss: 0.0351128/3811 - total loss: 0.2314 - classification loss: 0.0201 - dann loss: 2.0120 - reconstruction loss: 0.0352128/3811 - total loss: 0.2313 - classification loss: 0.0200 - dann loss: 2.0119 - reconstruction loss: 0.0352128/3811 - total loss: 0.2312 - classification loss: 0.0204 - dann loss: 2.0105 - reconstruction loss: 0.035298/3811 - total loss: 0.2310 - classification loss: 0.0204 - dann loss: 2.0082 - reconstruction loss: 0.03520/3811 - total loss: 0.2310 - classification loss: 0.0204 - dann loss: 2.0082 - reconstruction loss: 0.0352Epoch 23/150, Current strat Epoch 23/100
use_perm = True
switching perm
128/3811 - total loss: 0.2299 - classification loss: 0.0213 - dann loss: 1.9899 - reconstruction loss: 0.0360128/3811 - total loss: 0.2270 - classification loss: 0.0156 - dann loss: 1.9688 - reconstruction loss: 0.0357128/3811 - total loss: 0.2256 - classification loss: 0.0151 - dann loss: 1.9596 - reconstruction loss: 0.0352128/3811 - total loss: 0.2255 - classification loss: 0.0150 - dann loss: 1.9581 - reconstruction loss: 0.0353128/3811 - total loss: 0.2266 - classification loss: 0.0161 - dann loss: 1.9679 - reconstruction loss: 0.0352128/3811 - total loss: 0.2280 - classification loss: 0.0168 - dann loss: 1.9802 - reconstruction loss: 0.0354128/3811 - total loss: 0.2292 - classification loss: 0.0174 - dann loss: 1.9933 - reconstruction loss: 0.0352128/3811 - total loss: 0.2298 - classification loss: 0.0188 - dann loss: 1.9980 - reconstruction loss: 0.0352128/3811 - total loss: 0.2299 - classification loss: 0.0195 - dann loss: 2.0001 - reconstruction loss: 0.0350128/3811 - total loss: 0.2301 - classification loss: 0.0195 - dann loss: 2.0009 - reconstruction loss: 0.0350128/3811 - total loss: 0.2304 - classification loss: 0.0198 - dann loss: 2.0038 - reconstruction loss: 0.0351128/3811 - total loss: 0.2300 - classification loss: 0.0200 - dann loss: 1.9986 - reconstruction loss: 0.0351128/3811 - total loss: 0.2297 - classification loss: 0.0196 - dann loss: 1.9966 - reconstruction loss: 0.0351128/3811 - total loss: 0.2305 - classification loss: 0.0204 - dann loss: 2.0042 - reconstruction loss: 0.0351128/3811 - total loss: 0.2300 - classification loss: 0.0201 - dann loss: 2.0000 - reconstruction loss: 0.0350128/3811 - total loss: 0.2299 - classification loss: 0.0203 - dann loss: 1.9995 - reconstruction loss: 0.0350128/3811 - total loss: 0.2297 - classification loss: 0.0199 - dann loss: 1.9962 - reconstruction loss: 0.0351128/3811 - total loss: 0.2293 - classification loss: 0.0197 - dann loss: 1.9922 - reconstruction loss: 0.0351128/3811 - total loss: 0.2290 - classification loss: 0.0196 - dann loss: 1.9904 - reconstruction loss: 0.0350128/3811 - total loss: 0.2289 - classification loss: 0.0194 - dann loss: 1.9881 - reconstruction loss: 0.0352128/3811 - total loss: 0.2286 - classification loss: 0.0190 - dann loss: 1.9859 - reconstruction loss: 0.0351128/3811 - total loss: 0.2290 - classification loss: 0.0189 - dann loss: 1.9901 - reconstruction loss: 0.0351128/3811 - total loss: 0.2292 - classification loss: 0.0200 - dann loss: 1.9902 - reconstruction loss: 0.0352128/3811 - total loss: 0.2289 - classification loss: 0.0200 - dann loss: 1.9883 - reconstruction loss: 0.0351128/3811 - total loss: 0.2287 - classification loss: 0.0197 - dann loss: 1.9858 - reconstruction loss: 0.0352128/3811 - total loss: 0.2285 - classification loss: 0.0195 - dann loss: 1.9843 - reconstruction loss: 0.0351128/3811 - total loss: 0.2284 - classification loss: 0.0194 - dann loss: 1.9836 - reconstruction loss: 0.0351128/3811 - total loss: 0.2282 - classification loss: 0.0193 - dann loss: 1.9816 - reconstruction loss: 0.0351128/3811 - total loss: 0.2282 - classification loss: 0.0198 - dann loss: 1.9808 - reconstruction loss: 0.035198/3811 - total loss: 0.2282 - classification loss: 0.0199 - dann loss: 1.9817 - reconstruction loss: 0.03500/3811 - total loss: 0.2282 - classification loss: 0.0199 - dann loss: 1.9817 - reconstruction loss: 0.0350Epoch 24/150, Current strat Epoch 24/100
use_perm = True
switching perm
128/3811 - total loss: 0.2317 - classification loss: 0.0141 - dann loss: 2.0182 - reconstruction loss: 0.0356128/3811 - total loss: 0.2306 - classification loss: 0.0175 - dann loss: 2.0001 - reconstruction loss: 0.0360128/3811 - total loss: 0.2290 - classification loss: 0.0163 - dann loss: 1.9885 - reconstruction loss: 0.0356128/3811 - total loss: 0.2279 - classification loss: 0.0153 - dann loss: 1.9787 - reconstruction loss: 0.0356128/3811 - total loss: 0.2273 - classification loss: 0.0143 - dann loss: 1.9731 - reconstruction loss: 0.0357128/3811 - total loss: 0.2272 - classification loss: 0.0139 - dann loss: 1.9728 - reconstruction loss: 0.0357128/3811 - total loss: 0.2275 - classification loss: 0.0164 - dann loss: 1.9732 - reconstruction loss: 0.0357128/3811 - total loss: 0.2268 - classification loss: 0.0154 - dann loss: 1.9679 - reconstruction loss: 0.0355128/3811 - total loss: 0.2269 - classification loss: 0.0152 - dann loss: 1.9693 - reconstruction loss: 0.0356128/3811 - total loss: 0.2267 - classification loss: 0.0155 - dann loss: 1.9678 - reconstruction loss: 0.0355128/3811 - total loss: 0.2271 - classification loss: 0.0160 - dann loss: 1.9719 - reconstruction loss: 0.0353128/3811 - total loss: 0.2271 - classification loss: 0.0175 - dann loss: 1.9719 - reconstruction loss: 0.0352128/3811 - total loss: 0.2269 - classification loss: 0.0169 - dann loss: 1.9704 - reconstruction loss: 0.0353128/3811 - total loss: 0.2271 - classification loss: 0.0169 - dann loss: 1.9723 - reconstruction loss: 0.0353128/3811 - total loss: 0.2267 - classification loss: 0.0166 - dann loss: 1.9670 - reconstruction loss: 0.0354128/3811 - total loss: 0.2265 - classification loss: 0.0167 - dann loss: 1.9646 - reconstruction loss: 0.0354128/3811 - total loss: 0.2264 - classification loss: 0.0168 - dann loss: 1.9643 - reconstruction loss: 0.0354128/3811 - total loss: 0.2264 - classification loss: 0.0168 - dann loss: 1.9650 - reconstruction loss: 0.0353128/3811 - total loss: 0.2262 - classification loss: 0.0169 - dann loss: 1.9633 - reconstruction loss: 0.0352128/3811 - total loss: 0.2266 - classification loss: 0.0167 - dann loss: 1.9677 - reconstruction loss: 0.0352128/3811 - total loss: 0.2266 - classification loss: 0.0167 - dann loss: 1.9681 - reconstruction loss: 0.0352128/3811 - total loss: 0.2264 - classification loss: 0.0165 - dann loss: 1.9661 - reconstruction loss: 0.0352128/3811 - total loss: 0.2265 - classification loss: 0.0171 - dann loss: 1.9668 - reconstruction loss: 0.0351128/3811 - total loss: 0.2265 - classification loss: 0.0172 - dann loss: 1.9662 - reconstruction loss: 0.0352128/3811 - total loss: 0.2265 - classification loss: 0.0176 - dann loss: 1.9669 - reconstruction loss: 0.0351128/3811 - total loss: 0.2264 - classification loss: 0.0176 - dann loss: 1.9650 - reconstruction loss: 0.0351128/3811 - total loss: 0.2262 - classification loss: 0.0174 - dann loss: 1.9641 - reconstruction loss: 0.0351128/3811 - total loss: 0.2263 - classification loss: 0.0183 - dann loss: 1.9644 - reconstruction loss: 0.0351128/3811 - total loss: 0.2262 - classification loss: 0.0180 - dann loss: 1.9636 - reconstruction loss: 0.035098/3811 - total loss: 0.2264 - classification loss: 0.0179 - dann loss: 1.9665 - reconstruction loss: 0.03500/3811 - total loss: 0.2264 - classification loss: 0.0179 - dann loss: 1.9665 - reconstruction loss: 0.0350Epoch 25/150, Current strat Epoch 25/100
use_perm = True
switching perm
128/3811 - total loss: 0.2297 - classification loss: 0.0135 - dann loss: 1.9972 - reconstruction loss: 0.0358128/3811 - total loss: 0.2316 - classification loss: 0.0134 - dann loss: 2.0215 - reconstruction loss: 0.0351128/3811 - total loss: 0.2318 - classification loss: 0.0120 - dann loss: 2.0236 - reconstruction loss: 0.0353128/3811 - total loss: 0.2314 - classification loss: 0.0121 - dann loss: 2.0201 - reconstruction loss: 0.0352128/3811 - total loss: 0.2309 - classification loss: 0.0132 - dann loss: 2.0133 - reconstruction loss: 0.0353128/3811 - total loss: 0.2312 - classification loss: 0.0126 - dann loss: 2.0162 - reconstruction loss: 0.0354128/3811 - total loss: 0.2309 - classification loss: 0.0127 - dann loss: 2.0153 - reconstruction loss: 0.0351128/3811 - total loss: 0.2310 - classification loss: 0.0135 - dann loss: 2.0140 - reconstruction loss: 0.0354128/3811 - total loss: 0.2306 - classification loss: 0.0131 - dann loss: 2.0088 - reconstruction loss: 0.0355128/3811 - total loss: 0.2297 - classification loss: 0.0134 - dann loss: 1.9989 - reconstruction loss: 0.0356128/3811 - total loss: 0.2302 - classification loss: 0.0143 - dann loss: 2.0034 - reconstruction loss: 0.0356128/3811 - total loss: 0.2308 - classification loss: 0.0155 - dann loss: 2.0085 - reconstruction loss: 0.0355128/3811 - total loss: 0.2320 - classification loss: 0.0163 - dann loss: 2.0201 - reconstruction loss: 0.0355128/3811 - total loss: 0.2319 - classification loss: 0.0167 - dann loss: 2.0180 - reconstruction loss: 0.0356128/3811 - total loss: 0.2318 - classification loss: 0.0166 - dann loss: 2.0163 - reconstruction loss: 0.0356128/3811 - total loss: 0.2317 - classification loss: 0.0163 - dann loss: 2.0168 - reconstruction loss: 0.0355128/3811 - total loss: 0.2321 - classification loss: 0.0167 - dann loss: 2.0199 - reconstruction loss: 0.0355128/3811 - total loss: 0.2320 - classification loss: 0.0166 - dann loss: 2.0194 - reconstruction loss: 0.0355128/3811 - total loss: 0.2321 - classification loss: 0.0165 - dann loss: 2.0208 - reconstruction loss: 0.0355128/3811 - total loss: 0.2321 - classification loss: 0.0189 - dann loss: 2.0189 - reconstruction loss: 0.0354128/3811 - total loss: 0.2322 - classification loss: 0.0191 - dann loss: 2.0198 - reconstruction loss: 0.0354128/3811 - total loss: 0.2320 - classification loss: 0.0195 - dann loss: 2.0178 - reconstruction loss: 0.0354128/3811 - total loss: 0.2325 - classification loss: 0.0200 - dann loss: 2.0222 - reconstruction loss: 0.0354128/3811 - total loss: 0.2325 - classification loss: 0.0210 - dann loss: 2.0221 - reconstruction loss: 0.0353128/3811 - total loss: 0.2322 - classification loss: 0.0211 - dann loss: 2.0195 - reconstruction loss: 0.0352128/3811 - total loss: 0.2319 - classification loss: 0.0207 - dann loss: 2.0168 - reconstruction loss: 0.0351128/3811 - total loss: 0.2316 - classification loss: 0.0205 - dann loss: 2.0146 - reconstruction loss: 0.0351128/3811 - total loss: 0.2317 - classification loss: 0.0202 - dann loss: 2.0167 - reconstruction loss: 0.0351128/3811 - total loss: 0.2316 - classification loss: 0.0200 - dann loss: 2.0154 - reconstruction loss: 0.035098/3811 - total loss: 0.2316 - classification loss: 0.0199 - dann loss: 2.0158 - reconstruction loss: 0.03500/3811 - total loss: 0.2316 - classification loss: 0.0199 - dann loss: 2.0158 - reconstruction loss: 0.0350Epoch 26/150, Current strat Epoch 26/100
use_perm = True
switching perm
128/3811 - total loss: 0.2388 - classification loss: 0.0134 - dann loss: 2.0899 - reconstruction loss: 0.0355128/3811 - total loss: 0.2370 - classification loss: 0.0163 - dann loss: 2.0581 - reconstruction loss: 0.0369128/3811 - total loss: 0.2369 - classification loss: 0.0160 - dann loss: 2.0666 - reconstruction loss: 0.0359128/3811 - total loss: 0.2413 - classification loss: 0.0259 - dann loss: 2.0991 - reconstruction loss: 0.0360128/3811 - total loss: 0.2395 - classification loss: 0.0238 - dann loss: 2.0851 - reconstruction loss: 0.0357128/3811 - total loss: 0.2385 - classification loss: 0.0277 - dann loss: 2.0717 - reconstruction loss: 0.0356128/3811 - total loss: 0.2374 - classification loss: 0.0263 - dann loss: 2.0643 - reconstruction loss: 0.0355128/3811 - total loss: 0.2358 - classification loss: 0.0253 - dann loss: 2.0494 - reconstruction loss: 0.0354128/3811 - total loss: 0.2363 - classification loss: 0.0263 - dann loss: 2.0554 - reconstruction loss: 0.0352128/3811 - total loss: 0.2358 - classification loss: 0.0252 - dann loss: 2.0514 - reconstruction loss: 0.0351128/3811 - total loss: 0.2355 - classification loss: 0.0241 - dann loss: 2.0496 - reconstruction loss: 0.0351128/3811 - total loss: 0.2345 - classification loss: 0.0237 - dann loss: 2.0406 - reconstruction loss: 0.0351128/3811 - total loss: 0.2342 - classification loss: 0.0231 - dann loss: 2.0381 - reconstruction loss: 0.0350128/3811 - total loss: 0.2348 - classification loss: 0.0233 - dann loss: 2.0446 - reconstruction loss: 0.0350128/3811 - total loss: 0.2342 - classification loss: 0.0229 - dann loss: 2.0392 - reconstruction loss: 0.0350128/3811 - total loss: 0.2347 - classification loss: 0.0226 - dann loss: 2.0437 - reconstruction loss: 0.0350128/3811 - total loss: 0.2347 - classification loss: 0.0226 - dann loss: 2.0440 - reconstruction loss: 0.0351128/3811 - total loss: 0.2345 - classification loss: 0.0227 - dann loss: 2.0412 - reconstruction loss: 0.0351128/3811 - total loss: 0.2341 - classification loss: 0.0226 - dann loss: 2.0384 - reconstruction loss: 0.0350128/3811 - total loss: 0.2342 - classification loss: 0.0230 - dann loss: 2.0390 - reconstruction loss: 0.0350128/3811 - total loss: 0.2339 - classification loss: 0.0226 - dann loss: 2.0368 - reconstruction loss: 0.0349128/3811 - total loss: 0.2338 - classification loss: 0.0225 - dann loss: 2.0364 - reconstruction loss: 0.0349128/3811 - total loss: 0.2335 - classification loss: 0.0222 - dann loss: 2.0332 - reconstruction loss: 0.0349128/3811 - total loss: 0.2333 - classification loss: 0.0222 - dann loss: 2.0315 - reconstruction loss: 0.0350128/3811 - total loss: 0.2334 - classification loss: 0.0227 - dann loss: 2.0310 - reconstruction loss: 0.0350128/3811 - total loss: 0.2334 - classification loss: 0.0229 - dann loss: 2.0309 - reconstruction loss: 0.0350128/3811 - total loss: 0.2332 - classification loss: 0.0227 - dann loss: 2.0286 - reconstruction loss: 0.0351128/3811 - total loss: 0.2332 - classification loss: 0.0226 - dann loss: 2.0287 - reconstruction loss: 0.0351128/3811 - total loss: 0.2332 - classification loss: 0.0223 - dann loss: 2.0288 - reconstruction loss: 0.035198/3811 - total loss: 0.2329 - classification loss: 0.0222 - dann loss: 2.0257 - reconstruction loss: 0.0351128/3742 - total loss: 0.2809 - classification loss: 0.0445 - dann loss: 2.4827 - reconstruction loss: 0.0352128/3742 - total loss: 0.2807 - classification loss: 0.0448 - dann loss: 2.4803 - reconstruction loss: 0.0352128/3742 - total loss: 0.2805 - classification loss: 0.0452 - dann loss: 2.4779 - reconstruction loss: 0.0352128/3742 - total loss: 0.2805 - classification loss: 0.0451 - dann loss: 2.4787 - reconstruction loss: 0.0352128/3742 - total loss: 0.2805 - classification loss: 0.0449 - dann loss: 2.4779 - reconstruction loss: 0.0352128/3742 - total loss: 0.2803 - classification loss: 0.0451 - dann loss: 2.4761 - reconstruction loss: 0.035229/3742 - total loss: 0.2799 - classification loss: 0.0451 - dann loss: 2.4722 - reconstruction loss: 0.03520/3742 - total loss: 0.2799 - classification loss: 0.0451 - dann loss: 2.4722 - reconstruction loss: 0.0352Epoch 13/150, Current strat Epoch 13/100
use_perm = True
switching perm
128/3742 - total loss: 0.2895 - classification loss: 0.0621 - dann loss: 2.5388 - reconstruction loss: 0.0368128/3742 - total loss: 0.2816 - classification loss: 0.0489 - dann loss: 2.4814 - reconstruction loss: 0.0357128/3742 - total loss: 0.2833 - classification loss: 0.0451 - dann loss: 2.5010 - reconstruction loss: 0.0359128/3742 - total loss: 0.2859 - classification loss: 0.0468 - dann loss: 2.5246 - reconstruction loss: 0.0359128/3742 - total loss: 0.2856 - classification loss: 0.0478 - dann loss: 2.5226 - reconstruction loss: 0.0357128/3742 - total loss: 0.2851 - classification loss: 0.0462 - dann loss: 2.5193 - reconstruction loss: 0.0357128/3742 - total loss: 0.2838 - classification loss: 0.0449 - dann loss: 2.5090 - reconstruction loss: 0.0355128/3742 - total loss: 0.2829 - classification loss: 0.0453 - dann loss: 2.5004 - reconstruction loss: 0.0354128/3742 - total loss: 0.2828 - classification loss: 0.0443 - dann loss: 2.5000 - reconstruction loss: 0.0354128/3742 - total loss: 0.2823 - classification loss: 0.0448 - dann loss: 2.4944 - reconstruction loss: 0.0355128/3742 - total loss: 0.2820 - classification loss: 0.0440 - dann loss: 2.4928 - reconstruction loss: 0.0354128/3742 - total loss: 0.2814 - classification loss: 0.0440 - dann loss: 2.4868 - reconstruction loss: 0.0354128/3742 - total loss: 0.2811 - classification loss: 0.0434 - dann loss: 2.4845 - reconstruction loss: 0.0353128/3742 - total loss: 0.2809 - classification loss: 0.0434 - dann loss: 2.4825 - reconstruction loss: 0.0353128/3742 - total loss: 0.2806 - classification loss: 0.0437 - dann loss: 2.4799 - reconstruction loss: 0.0353128/3742 - total loss: 0.2805 - classification loss: 0.0431 - dann loss: 2.4801 - reconstruction loss: 0.0352128/3742 - total loss: 0.2801 - classification loss: 0.0435 - dann loss: 2.4755 - reconstruction loss: 0.0353128/3742 - total loss: 0.2796 - classification loss: 0.0426 - dann loss: 2.4716 - reconstruction loss: 0.0353128/3742 - total loss: 0.2793 - classification loss: 0.0430 - dann loss: 2.4674 - reconstruction loss: 0.0353128/3742 - total loss: 0.2794 - classification loss: 0.0440 - dann loss: 2.4669 - reconstruction loss: 0.0354128/3742 - total loss: 0.2791 - classification loss: 0.0438 - dann loss: 2.4642 - reconstruction loss: 0.0354128/3742 - total loss: 0.2786 - classification loss: 0.0434 - dann loss: 2.4585 - reconstruction loss: 0.0355128/3742 - total loss: 0.2783 - classification loss: 0.0434 - dann loss: 2.4563 - reconstruction loss: 0.0354128/3742 - total loss: 0.2781 - classification loss: 0.0431 - dann loss: 2.4554 - reconstruction loss: 0.0354128/3742 - total loss: 0.2779 - classification loss: 0.0437 - dann loss: 2.4527 - reconstruction loss: 0.0353128/3742 - total loss: 0.2775 - classification loss: 0.0432 - dann loss: 2.4490 - reconstruction loss: 0.0353128/3742 - total loss: 0.2772 - classification loss: 0.0428 - dann loss: 2.4460 - reconstruction loss: 0.0354128/3742 - total loss: 0.2769 - classification loss: 0.0431 - dann loss: 2.4437 - reconstruction loss: 0.0353128/3742 - total loss: 0.2765 - classification loss: 0.0428 - dann loss: 2.4399 - reconstruction loss: 0.035329/3742 - total loss: 0.2761 - classification loss: 0.0430 - dann loss: 2.4352 - reconstruction loss: 0.03540/3742 - total loss: 0.2761 - classification loss: 0.0430 - dann loss: 2.4352 - reconstruction loss: 0.0354Epoch 14/150, Current strat Epoch 14/100
use_perm = True
switching perm
128/3742 - total loss: 0.2847 - classification loss: 0.0427 - dann loss: 2.5070 - reconstruction loss: 0.0372128/3742 - total loss: 0.2798 - classification loss: 0.0364 - dann loss: 2.4748 - reconstruction loss: 0.0358128/3742 - total loss: 0.2789 - classification loss: 0.0327 - dann loss: 2.4692 - reconstruction loss: 0.0359128/3742 - total loss: 0.2805 - classification loss: 0.0346 - dann loss: 2.4782 - reconstruction loss: 0.0365128/3742 - total loss: 0.2796 - classification loss: 0.0335 - dann loss: 2.4735 - reconstruction loss: 0.0361128/3742 - total loss: 0.2795 - classification loss: 0.0325 - dann loss: 2.4741 - reconstruction loss: 0.0361128/3742 - total loss: 0.2791 - classification loss: 0.0325 - dann loss: 2.4713 - reconstruction loss: 0.0359128/3742 - total loss: 0.2783 - classification loss: 0.0327 - dann loss: 2.4641 - reconstruction loss: 0.0358128/3742 - total loss: 0.2774 - classification loss: 0.0331 - dann loss: 2.4556 - reconstruction loss: 0.0357128/3742 - total loss: 0.2775 - classification loss: 0.0334 - dann loss: 2.4560 - reconstruction loss: 0.0357128/3742 - total loss: 0.2775 - classification loss: 0.0334 - dann loss: 2.4559 - reconstruction loss: 0.0357128/3742 - total loss: 0.2770 - classification loss: 0.0349 - dann loss: 2.4517 - reconstruction loss: 0.0354128/3742 - total loss: 0.2770 - classification loss: 0.0355 - dann loss: 2.4514 - reconstruction loss: 0.0354128/3742 - total loss: 0.2769 - classification loss: 0.0354 - dann loss: 2.4503 - reconstruction loss: 0.0354128/3742 - total loss: 0.2765 - classification loss: 0.0359 - dann loss: 2.4459 - reconstruction loss: 0.0354128/3742 - total loss: 0.2765 - classification loss: 0.0373 - dann loss: 2.4444 - reconstruction loss: 0.0354128/3742 - total loss: 0.2766 - classification loss: 0.0371 - dann loss: 2.4446 - reconstruction loss: 0.0355128/3742 - total loss: 0.2759 - classification loss: 0.0368 - dann loss: 2.4384 - reconstruction loss: 0.0355128/3742 - total loss: 0.2759 - classification loss: 0.0375 - dann loss: 2.4371 - reconstruction loss: 0.0355128/3742 - total loss: 0.2755 - classification loss: 0.0368 - dann loss: 2.4350 - reconstruction loss: 0.0354128/3742 - total loss: 0.2753 - classification loss: 0.0370 - dann loss: 2.4325 - reconstruction loss: 0.0355128/3742 - total loss: 0.2754 - classification loss: 0.0372 - dann loss: 2.4329 - reconstruction loss: 0.0355128/3742 - total loss: 0.2750 - classification loss: 0.0366 - dann loss: 2.4293 - reconstruction loss: 0.0355128/3742 - total loss: 0.2750 - classification loss: 0.0368 - dann loss: 2.4289 - reconstruction loss: 0.0355128/3742 - total loss: 0.2749 - classification loss: 0.0372 - dann loss: 2.4275 - reconstruction loss: 0.0355128/3742 - total loss: 0.2747 - classification loss: 0.0368 - dann loss: 2.4257 - reconstruction loss: 0.0355128/3742 - total loss: 0.2745 - classification loss: 0.0368 - dann loss: 2.4244 - reconstruction loss: 0.0354128/3742 - total loss: 0.2743 - classification loss: 0.0368 - dann loss: 2.4230 - reconstruction loss: 0.0354128/3742 - total loss: 0.2740 - classification loss: 0.0372 - dann loss: 2.4201 - reconstruction loss: 0.035429/3742 - total loss: 0.2738 - classification loss: 0.0396 - dann loss: 2.4155 - reconstruction loss: 0.03530/3742 - total loss: 0.2738 - classification loss: 0.0396 - dann loss: 2.4155 - reconstruction loss: 0.0353Epoch 15/150, Current strat Epoch 15/100
use_perm = True
switching perm
128/3742 - total loss: 0.2669 - classification loss: 0.0345 - dann loss: 2.3619 - reconstruction loss: 0.0341128/3742 - total loss: 0.2737 - classification loss: 0.0375 - dann loss: 2.4150 - reconstruction loss: 0.0356128/3742 - total loss: 0.2750 - classification loss: 0.0394 - dann loss: 2.4247 - reconstruction loss: 0.03570/3811 - total loss: 0.2329 - classification loss: 0.0222 - dann loss: 2.0257 - reconstruction loss: 0.0351Epoch 27/150, Current strat Epoch 27/100
use_perm = True
switching perm
128/3811 - total loss: 0.2360 - classification loss: 0.0103 - dann loss: 2.0779 - reconstruction loss: 0.0340128/3811 - total loss: 0.2325 - classification loss: 0.0124 - dann loss: 2.0350 - reconstruction loss: 0.0348128/3811 - total loss: 0.2294 - classification loss: 0.0125 - dann loss: 2.0077 - reconstruction loss: 0.0342128/3811 - total loss: 0.2291 - classification loss: 0.0122 - dann loss: 2.0082 - reconstruction loss: 0.0338128/3811 - total loss: 0.2285 - classification loss: 0.0126 - dann loss: 2.0045 - reconstruction loss: 0.0335128/3811 - total loss: 0.2302 - classification loss: 0.0130 - dann loss: 2.0148 - reconstruction loss: 0.0343128/3811 - total loss: 0.2310 - classification loss: 0.0132 - dann loss: 2.0208 - reconstruction loss: 0.0345128/3811 - total loss: 0.2310 - classification loss: 0.0141 - dann loss: 2.0161 - reconstruction loss: 0.0350128/3811 - total loss: 0.2317 - classification loss: 0.0139 - dann loss: 2.0224 - reconstruction loss: 0.0351128/3811 - total loss: 0.2317 - classification loss: 0.0136 - dann loss: 2.0242 - reconstruction loss: 0.0349128/3811 - total loss: 0.2324 - classification loss: 0.0135 - dann loss: 2.0298 - reconstruction loss: 0.0351128/3811 - total loss: 0.2312 - classification loss: 0.0140 - dann loss: 2.0177 - reconstruction loss: 0.0350128/3811 - total loss: 0.2312 - classification loss: 0.0154 - dann loss: 2.0174 - reconstruction loss: 0.0350128/3811 - total loss: 0.2317 - classification loss: 0.0151 - dann loss: 2.0214 - reconstruction loss: 0.0351128/3811 - total loss: 0.2318 - classification loss: 0.0148 - dann loss: 2.0229 - reconstruction loss: 0.0350128/3811 - total loss: 0.2322 - classification loss: 0.0148 - dann loss: 2.0262 - reconstruction loss: 0.0351128/3811 - total loss: 0.2323 - classification loss: 0.0149 - dann loss: 2.0277 - reconstruction loss: 0.0351128/3811 - total loss: 0.2324 - classification loss: 0.0148 - dann loss: 2.0291 - reconstruction loss: 0.0351128/3811 - total loss: 0.2328 - classification loss: 0.0151 - dann loss: 2.0328 - reconstruction loss: 0.0350128/3811 - total loss: 0.2330 - classification loss: 0.0157 - dann loss: 2.0341 - reconstruction loss: 0.0351128/3811 - total loss: 0.2331 - classification loss: 0.0167 - dann loss: 2.0335 - reconstruction loss: 0.0351128/3811 - total loss: 0.2329 - classification loss: 0.0167 - dann loss: 2.0317 - reconstruction loss: 0.0351128/3811 - total loss: 0.2327 - classification loss: 0.0170 - dann loss: 2.0291 - reconstruction loss: 0.0352128/3811 - total loss: 0.2324 - classification loss: 0.0171 - dann loss: 2.0261 - reconstruction loss: 0.0352128/3811 - total loss: 0.2323 - classification loss: 0.0169 - dann loss: 2.0243 - reconstruction loss: 0.0352128/3811 - total loss: 0.2326 - classification loss: 0.0167 - dann loss: 2.0273 - reconstruction loss: 0.0352128/3811 - total loss: 0.2323 - classification loss: 0.0167 - dann loss: 2.0251 - reconstruction loss: 0.0352128/3811 - total loss: 0.2323 - classification loss: 0.0166 - dann loss: 2.0255 - reconstruction loss: 0.0352128/3811 - total loss: 0.2324 - classification loss: 0.0169 - dann loss: 2.0258 - reconstruction loss: 0.035198/3811 - total loss: 0.2325 - classification loss: 0.0169 - dann loss: 2.0277 - reconstruction loss: 0.03510/3811 - total loss: 0.2325 - classification loss: 0.0169 - dann loss: 2.0277 - reconstruction loss: 0.0351Epoch 28/150, Current strat Epoch 28/100
use_perm = True
switching perm
128/3811 - total loss: 0.2340 - classification loss: 0.0084 - dann loss: 2.0411 - reconstruction loss: 0.0363128/3811 - total loss: 0.2384 - classification loss: 0.0160 - dann loss: 2.0786 - reconstruction loss: 0.0361128/3811 - total loss: 0.2361 - classification loss: 0.0172 - dann loss: 2.0564 - reconstruction loss: 0.0360128/3811 - total loss: 0.2366 - classification loss: 0.0177 - dann loss: 2.0579 - reconstruction loss: 0.0363128/3811 - total loss: 0.2381 - classification loss: 0.0186 - dann loss: 2.0746 - reconstruction loss: 0.0360128/3811 - total loss: 0.2369 - classification loss: 0.0172 - dann loss: 2.0657 - reconstruction loss: 0.0358128/3811 - total loss: 0.2366 - classification loss: 0.0178 - dann loss: 2.0655 - reconstruction loss: 0.0353128/3811 - total loss: 0.2351 - classification loss: 0.0179 - dann loss: 2.0527 - reconstruction loss: 0.0350128/3811 - total loss: 0.2345 - classification loss: 0.0173 - dann loss: 2.0470 - reconstruction loss: 0.0352128/3811 - total loss: 0.2340 - classification loss: 0.0169 - dann loss: 2.0423 - reconstruction loss: 0.0351128/3811 - total loss: 0.2341 - classification loss: 0.0163 - dann loss: 2.0437 - reconstruction loss: 0.0351128/3811 - total loss: 0.2337 - classification loss: 0.0174 - dann loss: 2.0385 - reconstruction loss: 0.0351128/3811 - total loss: 0.2340 - classification loss: 0.0170 - dann loss: 2.0418 - reconstruction loss: 0.0352128/3811 - total loss: 0.2344 - classification loss: 0.0170 - dann loss: 2.0461 - reconstruction loss: 0.0351128/3811 - total loss: 0.2346 - classification loss: 0.0168 - dann loss: 2.0493 - reconstruction loss: 0.0349128/3811 - total loss: 0.2347 - classification loss: 0.0173 - dann loss: 2.0500 - reconstruction loss: 0.0349128/3811 - total loss: 0.2353 - classification loss: 0.0172 - dann loss: 2.0552 - reconstruction loss: 0.0351128/3811 - total loss: 0.2352 - classification loss: 0.0167 - dann loss: 2.0549 - reconstruction loss: 0.0351128/3811 - total loss: 0.2351 - classification loss: 0.0167 - dann loss: 2.0537 - reconstruction loss: 0.0350128/3811 - total loss: 0.2353 - classification loss: 0.0163 - dann loss: 2.0555 - reconstruction loss: 0.0351128/3811 - total loss: 0.2350 - classification loss: 0.0162 - dann loss: 2.0531 - reconstruction loss: 0.0350128/3811 - total loss: 0.2352 - classification loss: 0.0162 - dann loss: 2.0550 - reconstruction loss: 0.0351128/3811 - total loss: 0.2349 - classification loss: 0.0160 - dann loss: 2.0524 - reconstruction loss: 0.0351128/3811 - total loss: 0.2348 - classification loss: 0.0160 - dann loss: 2.0516 - reconstruction loss: 0.0351128/3811 - total loss: 0.2347 - classification loss: 0.0158 - dann loss: 2.0508 - reconstruction loss: 0.0351128/3811 - total loss: 0.2346 - classification loss: 0.0157 - dann loss: 2.0503 - reconstruction loss: 0.0350128/3811 - total loss: 0.2348 - classification loss: 0.0158 - dann loss: 2.0514 - reconstruction loss: 0.0351128/3811 - total loss: 0.2347 - classification loss: 0.0158 - dann loss: 2.0506 - reconstruction loss: 0.0350128/3811 - total loss: 0.2347 - classification loss: 0.0158 - dann loss: 2.0505 - reconstruction loss: 0.035098/3811 - total loss: 0.2346 - classification loss: 0.0160 - dann loss: 2.0498 - reconstruction loss: 0.03500/3811 - total loss: 0.2346 - classification loss: 0.0160 - dann loss: 2.0498 - reconstruction loss: 0.0350Epoch 29/150, Current strat Epoch 29/100
use_perm = True
switching perm
128/3811 - total loss: 0.2245 - classification loss: 0.0090 - dann loss: 1.9551 - reconstruction loss: 0.0351128/3811 - total loss: 0.2259 - classification loss: 0.0117 - dann loss: 1.9716 - reconstruction loss: 0.0344128/3811 - total loss: 0.2272 - classification loss: 0.0136 - dann loss: 1.9754 - reconstruction loss: 0.0353128/3811 - total loss: 0.2279 - classification loss: 0.0136 - dann loss: 1.9875 - reconstruction loss: 0.0347128/3811 - total loss: 0.2296 - classification loss: 0.0130 - dann loss: 2.0022 - reconstruction loss: 0.0351128/3811 - total loss: 0.2296 - classification loss: 0.0123 - dann loss: 2.0011 - reconstruction loss: 0.0353128/3811 - total loss: 0.2290 - classification loss: 0.0119 - dann loss: 1.9975 - reconstruction loss: 0.0351128/3811 - total loss: 0.2295 - classification loss: 0.0114 - dann loss: 2.0029 - reconstruction loss: 0.0350128/3811 - total loss: 0.2293 - classification loss: 0.0117 - dann loss: 1.9987 - reconstruction loss: 0.0354128/3811 - total loss: 0.2288 - classification loss: 0.0111 - dann loss: 1.9932 - reconstruction loss: 0.0354128/3811 - total loss: 0.2285 - classification loss: 0.0111 - dann loss: 1.9920 - reconstruction loss: 0.0353128/3811 - total loss: 0.2285 - classification loss: 0.0110 - dann loss: 1.9932 - reconstruction loss: 0.0351128/3811 - total loss: 0.2279 - classification loss: 0.0119 - dann loss: 1.9866 - reconstruction loss: 0.0351128/3811 - total loss: 0.2275 - classification loss: 0.0117 - dann loss: 1.9829 - reconstruction loss: 0.0350128/3811 - total loss: 0.2279 - classification loss: 0.0118 - dann loss: 1.9874 - reconstruction loss: 0.0350128/3811 - total loss: 0.2281 - classification loss: 0.0120 - dann loss: 1.9892 - reconstruction loss: 0.0350128/3811 - total loss: 0.2277 - classification loss: 0.0120 - dann loss: 1.9845 - reconstruction loss: 0.0350128/3811 - total loss: 0.2278 - classification loss: 0.0125 - dann loss: 1.9854 - reconstruction loss: 0.0350128/3811 - total loss: 0.2275 - classification loss: 0.0123 - dann loss: 1.9825 - reconstruction loss: 0.0350128/3811 - total loss: 0.2272 - classification loss: 0.0126 - dann loss: 1.9802 - reconstruction loss: 0.0349128/3811 - total loss: 0.2269 - classification loss: 0.0124 - dann loss: 1.9768 - reconstruction loss: 0.0349128/3811 - total loss: 0.2265 - classification loss: 0.0121 - dann loss: 1.9738 - reconstruction loss: 0.0349128/3811 - total loss: 0.2260 - classification loss: 0.0120 - dann loss: 1.9690 - reconstruction loss: 0.0349128/3811 - total loss: 0.2259 - classification loss: 0.0120 - dann loss: 1.9671 - reconstruction loss: 0.0350128/3811 - total loss: 0.2260 - classification loss: 0.0121 - dann loss: 1.9684 - reconstruction loss: 0.0349128/3811 - total loss: 0.2254 - classification loss: 0.0119 - dann loss: 1.9632 - reconstruction loss: 0.0349128/3811 - total loss: 0.2252 - classification loss: 0.0120 - dann loss: 1.9613 - reconstruction loss: 0.0349128/3811 - total loss: 0.2251 - classification loss: 0.0119 - dann loss: 1.9605 - reconstruction loss: 0.0349128/3811 - total loss: 0.2249 - classification loss: 0.0117 - dann loss: 1.9576 - reconstruction loss: 0.034998/3811 - total loss: 0.2246 - classification loss: 0.0115 - dann loss: 1.9553 - reconstruction loss: 0.03490/3811 - total loss: 0.2246 - classification loss: 0.0115 - dann loss: 1.9553 - reconstruction loss: 0.0349Epoch 30/150, Current strat Epoch 30/100
use_perm = True
switching perm
128/3811 - total loss: 0.2192 - classification loss: 0.0087 - dann loss: 1.9098 - reconstruction loss: 0.0342128/3811 - total loss: 0.2191 - classification loss: 0.0071 - dann loss: 1.9036 - reconstruction loss: 0.0350128/3811 - total loss: 0.2181 - classification loss: 0.0075 - dann loss: 1.8972 - reconstruction loss: 0.0346128/3811 - total loss: 0.2186 - classification loss: 0.0073 - dann loss: 1.9004 - reconstruction loss: 0.0348128/3811 - total loss: 0.2183 - classification loss: 0.0070 - dann loss: 1.8966 - reconstruction loss: 0.0349128/3811 - total loss: 0.2190 - classification loss: 0.0080 - dann loss: 1.9009 - reconstruction loss: 0.0351128/3811 - total loss: 0.2193 - classification loss: 0.0078 - dann loss: 1.9077 - reconstruction loss: 0.0347128/3811 - total loss: 0.2193 - classification loss: 0.0079 - dann loss: 1.9081 - reconstruction loss: 0.0346128/3811 - total loss: 0.2186 - classification loss: 0.0081 - dann loss: 1.9036 - reconstruction loss: 0.0343128/3811 - total loss: 0.2187 - classification loss: 0.0080 - dann loss: 1.9043 - reconstruction loss: 0.0344128/3811 - total loss: 0.2187 - classification loss: 0.0079 - dann loss: 1.9046 - reconstruction loss: 0.0344128/3811 - total loss: 0.2197 - classification loss: 0.0085 - dann loss: 1.9125 - reconstruction loss: 0.0346128/3811 - total loss: 0.2208 - classification loss: 0.0094 - dann loss: 1.9218 - reconstruction loss: 0.0346128/3811 - total loss: 0.2213 - classification loss: 0.0095 - dann loss: 1.9266 - reconstruction loss: 0.0346128/3811 - total loss: 0.2215 - classification loss: 0.0095 - dann loss: 1.9288 - reconstruction loss: 0.0346128/3811 - total loss: 0.2218 - classification loss: 0.0096 - dann loss: 1.9317 - reconstruction loss: 0.0345128/3811 - total loss: 0.2221 - classification loss: 0.0097 - dann loss: 1.9336 - reconstruction loss: 0.0347128/3811 - total loss: 0.2220 - classification loss: 0.0094 - dann loss: 1.9324 - reconstruction loss: 0.0348128/3811 - total loss: 0.2224 - classification loss: 0.0096 - dann loss: 1.9363 - reconstruction loss: 0.0347128/3811 - total loss: 0.2223 - classification loss: 0.0096 - dann loss: 1.9357 - reconstruction loss: 0.0347128/3811 - total loss: 0.2221 - classification loss: 0.0094 - dann loss: 1.9344 - reconstruction loss: 0.0347128/3811 - total loss: 0.2221 - classification loss: 0.0097 - dann loss: 1.9330 - reconstruction loss: 0.0347128/3811 - total loss: 0.2223 - classification loss: 0.0096 - dann loss: 1.9352 - reconstruction loss: 0.0348128/3811 - total loss: 0.2226 - classification loss: 0.0096 - dann loss: 1.9373 - reconstruction loss: 0.0349128/3811 - total loss: 0.2230 - classification loss: 0.0101 - dann loss: 1.9403 - reconstruction loss: 0.0349128/3811 - total loss: 0.2232 - classification loss: 0.0101 - dann loss: 1.9427 - reconstruction loss: 0.0349128/3811 - total loss: 0.2228 - classification loss: 0.0101 - dann loss: 1.9386 - reconstruction loss: 0.0349128/3811 - total loss: 0.2225 - classification loss: 0.0103 - dann loss: 1.9363 - reconstruction loss: 0.0349128/3811 - total loss: 0.2228 - classification loss: 0.0108 - dann loss: 1.9382 - reconstruction loss: 0.034998/3811 - total loss: 0.2228 - classification loss: 0.0111 - dann loss: 1.9379 - reconstruction loss: 0.03490/3811 - total loss: 0.2228 - classification loss: 0.0111 - dann loss: 1.9379 - reconstruction loss: 0.0349Epoch 31/150, Current strat Epoch 31/100
use_perm = True
switching perm
128/3811 - total loss: 0.2311 - classification loss: 0.0118 - dann loss: 2.0345 - reconstruction loss: 0.0331128/3811 - total loss: 0.2279 - classification loss: 0.0120 - dann loss: 1.9959 - reconstruction loss: 0.0338128/3811 - total loss: 0.2292 - classification loss: 0.0117 - dann loss: 2.0084 - reconstruction loss: 0.0340128/3811 - total loss: 0.2313 - classification loss: 0.0113 - dann loss: 2.0300 - reconstruction loss: 0.0340128/3811 - total loss: 0.2326 - classification loss: 0.0125 - dann loss: 2.0421 - reconstruction loss: 0.0339128/3811 - total loss: 0.2323 - classification loss: 0.0132 - dann loss: 2.0353 - reconstruction loss: 0.0343128/3811 - total loss: 0.2324 - classification loss: 0.0132 - dann loss: 2.0364 - reconstruction loss: 0.0343128/3811 - total loss: 0.2328 - classification loss: 0.0126 - dann loss: 2.0404 - reconstruction loss: 0.0344128/3811 - total loss: 0.2317 - classification loss: 0.0126 - dann loss: 2.0293 - reconstruction loss: 0.0344128/3811 - total loss: 0.2310 - classification loss: 0.0127 - dann loss: 2.0212 - reconstruction loss: 0.0346128/3811 - total loss: 0.2301 - classification loss: 0.0121 - dann loss: 2.0117 - reconstruction loss: 0.0347128/3811 - total loss: 0.2306 - classification loss: 0.0122 - dann loss: 2.0162 - reconstruction loss: 0.0347128/3811 - total loss: 0.2307 - classification loss: 0.0120 - dann loss: 2.0167 - reconstruction loss: 0.0348128/3811 - total loss: 0.2309 - classification loss: 0.0120 - dann loss: 2.0171 - reconstruction loss: 0.0349128/3811 - total loss: 0.2309 - classification loss: 0.0118 - dann loss: 2.0175 - reconstruction loss: 0.0349128/3811 - total loss: 0.2310 - classification loss: 0.0117 - dann loss: 2.0192 - reconstruction loss: 0.0348128/3811 - total loss: 0.2311 - classification loss: 0.0116 - dann loss: 2.0209 - reconstruction loss: 0.0348128/3811 - total loss: 0.2312 - classification loss: 0.0115 - dann loss: 2.0220 - reconstruction loss: 0.0348128/3811 - total loss: 0.2312 - classification loss: 0.0115 - dann loss: 2.0213 - reconstruction loss: 0.0349128/3811 - total loss: 0.2309 - classification loss: 0.0116 - dann loss: 2.0178 - reconstruction loss: 0.0349128/3811 - total loss: 0.2307 - classification loss: 0.0116 - dann loss: 2.0156 - reconstruction loss: 0.0349128/3811 - total loss: 0.2304 - classification loss: 0.0118 - dann loss: 2.0113 - reconstruction loss: 0.0351128/3742 - total loss: 0.2752 - classification loss: 0.0360 - dann loss: 2.4301 - reconstruction loss: 0.0358128/3742 - total loss: 0.2762 - classification loss: 0.0384 - dann loss: 2.4407 - reconstruction loss: 0.0354128/3742 - total loss: 0.2761 - classification loss: 0.0372 - dann loss: 2.4401 - reconstruction loss: 0.0355128/3742 - total loss: 0.2758 - classification loss: 0.0390 - dann loss: 2.4362 - reconstruction loss: 0.0354128/3742 - total loss: 0.2767 - classification loss: 0.0394 - dann loss: 2.4446 - reconstruction loss: 0.0353128/3742 - total loss: 0.2764 - classification loss: 0.0383 - dann loss: 2.4433 - reconstruction loss: 0.0353128/3742 - total loss: 0.2766 - classification loss: 0.0367 - dann loss: 2.4456 - reconstruction loss: 0.0355128/3742 - total loss: 0.2762 - classification loss: 0.0352 - dann loss: 2.4429 - reconstruction loss: 0.0355128/3742 - total loss: 0.2765 - classification loss: 0.0348 - dann loss: 2.4455 - reconstruction loss: 0.0356128/3742 - total loss: 0.2762 - classification loss: 0.0344 - dann loss: 2.4414 - reconstruction loss: 0.0358128/3742 - total loss: 0.2761 - classification loss: 0.0339 - dann loss: 2.4419 - reconstruction loss: 0.0356128/3742 - total loss: 0.2757 - classification loss: 0.0330 - dann loss: 2.4402 - reconstruction loss: 0.0355128/3742 - total loss: 0.2755 - classification loss: 0.0332 - dann loss: 2.4370 - reconstruction loss: 0.0356128/3742 - total loss: 0.2758 - classification loss: 0.0336 - dann loss: 2.4396 - reconstruction loss: 0.0356128/3742 - total loss: 0.2754 - classification loss: 0.0336 - dann loss: 2.4370 - reconstruction loss: 0.0355128/3742 - total loss: 0.2754 - classification loss: 0.0335 - dann loss: 2.4370 - reconstruction loss: 0.0354128/3742 - total loss: 0.2752 - classification loss: 0.0332 - dann loss: 2.4350 - reconstruction loss: 0.0354128/3742 - total loss: 0.2752 - classification loss: 0.0332 - dann loss: 2.4351 - reconstruction loss: 0.0354128/3742 - total loss: 0.2748 - classification loss: 0.0333 - dann loss: 2.4314 - reconstruction loss: 0.0354128/3742 - total loss: 0.2746 - classification loss: 0.0337 - dann loss: 2.4289 - reconstruction loss: 0.0354128/3742 - total loss: 0.2745 - classification loss: 0.0337 - dann loss: 2.4278 - reconstruction loss: 0.0355128/3742 - total loss: 0.2743 - classification loss: 0.0333 - dann loss: 2.4266 - reconstruction loss: 0.0354128/3742 - total loss: 0.2742 - classification loss: 0.0332 - dann loss: 2.4262 - reconstruction loss: 0.0353128/3742 - total loss: 0.2738 - classification loss: 0.0332 - dann loss: 2.4232 - reconstruction loss: 0.0353128/3742 - total loss: 0.2736 - classification loss: 0.0338 - dann loss: 2.4203 - reconstruction loss: 0.0353128/3742 - total loss: 0.2733 - classification loss: 0.0335 - dann loss: 2.4178 - reconstruction loss: 0.035229/3742 - total loss: 0.2730 - classification loss: 0.0342 - dann loss: 2.4151 - reconstruction loss: 0.03510/3742 - total loss: 0.2730 - classification loss: 0.0342 - dann loss: 2.4151 - reconstruction loss: 0.0351Epoch 16/150, Current strat Epoch 16/100
use_perm = True
switching perm
128/3742 - total loss: 0.2751 - classification loss: 0.0250 - dann loss: 2.4552 - reconstruction loss: 0.0339128/3742 - total loss: 0.2746 - classification loss: 0.0291 - dann loss: 2.4362 - reconstruction loss: 0.0351128/3742 - total loss: 0.2741 - classification loss: 0.0253 - dann loss: 2.4401 - reconstruction loss: 0.0345128/3742 - total loss: 0.2738 - classification loss: 0.0265 - dann loss: 2.4322 - reconstruction loss: 0.0349128/3742 - total loss: 0.2738 - classification loss: 0.0266 - dann loss: 2.4319 - reconstruction loss: 0.0349128/3742 - total loss: 0.2736 - classification loss: 0.0269 - dann loss: 2.4300 - reconstruction loss: 0.0349128/3742 - total loss: 0.2741 - classification loss: 0.0274 - dann loss: 2.4347 - reconstruction loss: 0.0349128/3742 - total loss: 0.2744 - classification loss: 0.0290 - dann loss: 2.4368 - reconstruction loss: 0.0348128/3742 - total loss: 0.2741 - classification loss: 0.0292 - dann loss: 2.4323 - reconstruction loss: 0.0350128/3742 - total loss: 0.2741 - classification loss: 0.0290 - dann loss: 2.4314 - reconstruction loss: 0.0351128/3742 - total loss: 0.2740 - classification loss: 0.0295 - dann loss: 2.4314 - reconstruction loss: 0.0349128/3742 - total loss: 0.2736 - classification loss: 0.0296 - dann loss: 2.4265 - reconstruction loss: 0.0349128/3742 - total loss: 0.2736 - classification loss: 0.0295 - dann loss: 2.4274 - reconstruction loss: 0.0349128/3742 - total loss: 0.2736 - classification loss: 0.0296 - dann loss: 2.4267 - reconstruction loss: 0.0350128/3742 - total loss: 0.2733 - classification loss: 0.0293 - dann loss: 2.4237 - reconstruction loss: 0.0350128/3742 - total loss: 0.2733 - classification loss: 0.0292 - dann loss: 2.4234 - reconstruction loss: 0.0350128/3742 - total loss: 0.2732 - classification loss: 0.0291 - dann loss: 2.4229 - reconstruction loss: 0.0350128/3742 - total loss: 0.2732 - classification loss: 0.0297 - dann loss: 2.4225 - reconstruction loss: 0.0350128/3742 - total loss: 0.2733 - classification loss: 0.0304 - dann loss: 2.4220 - reconstruction loss: 0.0350128/3742 - total loss: 0.2734 - classification loss: 0.0301 - dann loss: 2.4242 - reconstruction loss: 0.0349128/3742 - total loss: 0.2733 - classification loss: 0.0305 - dann loss: 2.4223 - reconstruction loss: 0.0350128/3742 - total loss: 0.2732 - classification loss: 0.0306 - dann loss: 2.4202 - reconstruction loss: 0.0351128/3742 - total loss: 0.2729 - classification loss: 0.0307 - dann loss: 2.4176 - reconstruction loss: 0.0351128/3742 - total loss: 0.2730 - classification loss: 0.0304 - dann loss: 2.4186 - reconstruction loss: 0.0351128/3742 - total loss: 0.2729 - classification loss: 0.0306 - dann loss: 2.4176 - reconstruction loss: 0.0351128/3742 - total loss: 0.2729 - classification loss: 0.0314 - dann loss: 2.4161 - reconstruction loss: 0.0351128/3742 - total loss: 0.2728 - classification loss: 0.0313 - dann loss: 2.4164 - reconstruction loss: 0.0351128/3742 - total loss: 0.2728 - classification loss: 0.0310 - dann loss: 2.4155 - reconstruction loss: 0.0351128/3742 - total loss: 0.2726 - classification loss: 0.0319 - dann loss: 2.4134 - reconstruction loss: 0.035129/3742 - total loss: 0.2726 - classification loss: 0.0325 - dann loss: 2.4120 - reconstruction loss: 0.03520/3742 - total loss: 0.2726 - classification loss: 0.0325 - dann loss: 2.4120 - reconstruction loss: 0.0352Epoch 17/150, Current strat Epoch 17/100
use_perm = True
switching perm
128/3742 - total loss: 0.2585 - classification loss: 0.0164 - dann loss: 2.2953 - reconstruction loss: 0.0342128/3742 - total loss: 0.2644 - classification loss: 0.0175 - dann loss: 2.3496 - reconstruction loss: 0.0346128/3742 - total loss: 0.2679 - classification loss: 0.0281 - dann loss: 2.3752 - reconstruction loss: 0.0345128/3742 - total loss: 0.2692 - classification loss: 0.0279 - dann loss: 2.3873 - reconstruction loss: 0.0346128/3742 - total loss: 0.2685 - classification loss: 0.0267 - dann loss: 2.3853 - reconstruction loss: 0.0341128/3742 - total loss: 0.2688 - classification loss: 0.0256 - dann loss: 2.3900 - reconstruction loss: 0.0341128/3742 - total loss: 0.2695 - classification loss: 0.0275 - dann loss: 2.3922 - reconstruction loss: 0.0344128/3742 - total loss: 0.2703 - classification loss: 0.0283 - dann loss: 2.3975 - reconstruction loss: 0.0347128/3742 - total loss: 0.2713 - classification loss: 0.0282 - dann loss: 2.4038 - reconstruction loss: 0.0351128/3742 - total loss: 0.2711 - classification loss: 0.0277 - dann loss: 2.4044 - reconstruction loss: 0.0349128/3742 - total loss: 0.2709 - classification loss: 0.0270 - dann loss: 2.4035 - reconstruction loss: 0.0349128/3742 - total loss: 0.2708 - classification loss: 0.0272 - dann loss: 2.4031 - reconstruction loss: 0.0348128/3742 - total loss: 0.2713 - classification loss: 0.0265 - dann loss: 2.4065 - reconstruction loss: 0.0350128/3742 - total loss: 0.2708 - classification loss: 0.0271 - dann loss: 2.4006 - reconstruction loss: 0.0351128/3742 - total loss: 0.2705 - classification loss: 0.0273 - dann loss: 2.3970 - reconstruction loss: 0.0351128/3811 - total loss: 0.2298 - classification loss: 0.0119 - dann loss: 2.0056 - reconstruction loss: 0.0350128/3811 - total loss: 0.2297 - classification loss: 0.0118 - dann loss: 2.0052 - reconstruction loss: 0.0350128/3811 - total loss: 0.2301 - classification loss: 0.0118 - dann loss: 2.0088 - reconstruction loss: 0.0351128/3811 - total loss: 0.2300 - classification loss: 0.0117 - dann loss: 2.0079 - reconstruction loss: 0.0350128/3811 - total loss: 0.2303 - classification loss: 0.0117 - dann loss: 2.0114 - reconstruction loss: 0.0350128/3811 - total loss: 0.2306 - classification loss: 0.0129 - dann loss: 2.0134 - reconstruction loss: 0.0349128/3811 - total loss: 0.2304 - classification loss: 0.0128 - dann loss: 2.0120 - reconstruction loss: 0.034998/3811 - total loss: 0.2310 - classification loss: 0.0130 - dann loss: 2.0170 - reconstruction loss: 0.03500/3811 - total loss: 0.2310 - classification loss: 0.0130 - dann loss: 2.0170 - reconstruction loss: 0.0350Epoch 32/150, Current strat Epoch 32/100
use_perm = True
switching perm
128/3811 - total loss: 0.2291 - classification loss: 0.0095 - dann loss: 2.0009 - reconstruction loss: 0.0350128/3811 - total loss: 0.2322 - classification loss: 0.0129 - dann loss: 2.0303 - reconstruction loss: 0.0348128/3811 - total loss: 0.2306 - classification loss: 0.0119 - dann loss: 2.0200 - reconstruction loss: 0.0343128/3811 - total loss: 0.2299 - classification loss: 0.0115 - dann loss: 2.0089 - reconstruction loss: 0.0349128/3811 - total loss: 0.2293 - classification loss: 0.0132 - dann loss: 2.0021 - reconstruction loss: 0.0347128/3811 - total loss: 0.2310 - classification loss: 0.0134 - dann loss: 2.0195 - reconstruction loss: 0.0346128/3811 - total loss: 0.2317 - classification loss: 0.0137 - dann loss: 2.0239 - reconstruction loss: 0.0349128/3811 - total loss: 0.2314 - classification loss: 0.0133 - dann loss: 2.0204 - reconstruction loss: 0.0350128/3811 - total loss: 0.2313 - classification loss: 0.0136 - dann loss: 2.0182 - reconstruction loss: 0.0351128/3811 - total loss: 0.2316 - classification loss: 0.0136 - dann loss: 2.0215 - reconstruction loss: 0.0350128/3811 - total loss: 0.2309 - classification loss: 0.0133 - dann loss: 2.0152 - reconstruction loss: 0.0351128/3811 - total loss: 0.2307 - classification loss: 0.0132 - dann loss: 2.0134 - reconstruction loss: 0.0351128/3811 - total loss: 0.2301 - classification loss: 0.0132 - dann loss: 2.0076 - reconstruction loss: 0.0351128/3811 - total loss: 0.2302 - classification loss: 0.0146 - dann loss: 2.0082 - reconstruction loss: 0.0349128/3811 - total loss: 0.2305 - classification loss: 0.0144 - dann loss: 2.0109 - reconstruction loss: 0.0350128/3811 - total loss: 0.2302 - classification loss: 0.0144 - dann loss: 2.0085 - reconstruction loss: 0.0349128/3811 - total loss: 0.2301 - classification loss: 0.0145 - dann loss: 2.0080 - reconstruction loss: 0.0349128/3811 - total loss: 0.2299 - classification loss: 0.0144 - dann loss: 2.0061 - reconstruction loss: 0.0349128/3811 - total loss: 0.2298 - classification loss: 0.0142 - dann loss: 2.0051 - reconstruction loss: 0.0348128/3811 - total loss: 0.2294 - classification loss: 0.0140 - dann loss: 2.0011 - reconstruction loss: 0.0349128/3811 - total loss: 0.2295 - classification loss: 0.0137 - dann loss: 2.0027 - reconstruction loss: 0.0348128/3811 - total loss: 0.2295 - classification loss: 0.0135 - dann loss: 2.0032 - reconstruction loss: 0.0348128/3811 - total loss: 0.2294 - classification loss: 0.0134 - dann loss: 2.0015 - reconstruction loss: 0.0349128/3811 - total loss: 0.2296 - classification loss: 0.0145 - dann loss: 2.0015 - reconstruction loss: 0.0350128/3811 - total loss: 0.2297 - classification loss: 0.0143 - dann loss: 2.0025 - reconstruction loss: 0.0350128/3811 - total loss: 0.2296 - classification loss: 0.0141 - dann loss: 2.0013 - reconstruction loss: 0.0350128/3811 - total loss: 0.2293 - classification loss: 0.0141 - dann loss: 1.9994 - reconstruction loss: 0.0349128/3811 - total loss: 0.2292 - classification loss: 0.0139 - dann loss: 1.9988 - reconstruction loss: 0.0349128/3811 - total loss: 0.2293 - classification loss: 0.0139 - dann loss: 2.0005 - reconstruction loss: 0.034998/3811 - total loss: 0.2292 - classification loss: 0.0148 - dann loss: 1.9983 - reconstruction loss: 0.03490/3811 - total loss: 0.2292 - classification loss: 0.0148 - dann loss: 1.9983 - reconstruction loss: 0.0349Epoch 33/150, Current strat Epoch 33/100
use_perm = True
switching perm
128/3811 - total loss: 0.2223 - classification loss: 0.0108 - dann loss: 1.9343 - reconstruction loss: 0.0347128/3811 - total loss: 0.2240 - classification loss: 0.0103 - dann loss: 1.9489 - reconstruction loss: 0.0351128/3811 - total loss: 0.2231 - classification loss: 0.0096 - dann loss: 1.9441 - reconstruction loss: 0.0347128/3811 - total loss: 0.2252 - classification loss: 0.0105 - dann loss: 1.9617 - reconstruction loss: 0.0350128/3811 - total loss: 0.2255 - classification loss: 0.0106 - dann loss: 1.9621 - reconstruction loss: 0.0352128/3811 - total loss: 0.2259 - classification loss: 0.0106 - dann loss: 1.9667 - reconstruction loss: 0.0353128/3811 - total loss: 0.2270 - classification loss: 0.0105 - dann loss: 1.9768 - reconstruction loss: 0.0353128/3811 - total loss: 0.2265 - classification loss: 0.0105 - dann loss: 1.9712 - reconstruction loss: 0.0354128/3811 - total loss: 0.2263 - classification loss: 0.0109 - dann loss: 1.9689 - reconstruction loss: 0.0354128/3811 - total loss: 0.2271 - classification loss: 0.0106 - dann loss: 1.9782 - reconstruction loss: 0.0353128/3811 - total loss: 0.2278 - classification loss: 0.0110 - dann loss: 1.9838 - reconstruction loss: 0.0354128/3811 - total loss: 0.2278 - classification loss: 0.0114 - dann loss: 1.9837 - reconstruction loss: 0.0353128/3811 - total loss: 0.2288 - classification loss: 0.0112 - dann loss: 1.9953 - reconstruction loss: 0.0352128/3811 - total loss: 0.2287 - classification loss: 0.0111 - dann loss: 1.9945 - reconstruction loss: 0.0352128/3811 - total loss: 0.2296 - classification loss: 0.0116 - dann loss: 2.0017 - reconstruction loss: 0.0353128/3811 - total loss: 0.2298 - classification loss: 0.0119 - dann loss: 2.0041 - reconstruction loss: 0.0353128/3811 - total loss: 0.2296 - classification loss: 0.0123 - dann loss: 2.0013 - reconstruction loss: 0.0352128/3811 - total loss: 0.2293 - classification loss: 0.0127 - dann loss: 1.9990 - reconstruction loss: 0.0352128/3811 - total loss: 0.2295 - classification loss: 0.0125 - dann loss: 2.0002 - reconstruction loss: 0.0352128/3811 - total loss: 0.2292 - classification loss: 0.0123 - dann loss: 1.9986 - reconstruction loss: 0.0352128/3811 - total loss: 0.2290 - classification loss: 0.0122 - dann loss: 1.9964 - reconstruction loss: 0.0352128/3811 - total loss: 0.2290 - classification loss: 0.0122 - dann loss: 1.9974 - reconstruction loss: 0.0351128/3811 - total loss: 0.2292 - classification loss: 0.0123 - dann loss: 1.9993 - reconstruction loss: 0.0351128/3811 - total loss: 0.2291 - classification loss: 0.0122 - dann loss: 1.9986 - reconstruction loss: 0.0351128/3811 - total loss: 0.2291 - classification loss: 0.0124 - dann loss: 1.9983 - reconstruction loss: 0.0350128/3811 - total loss: 0.2290 - classification loss: 0.0122 - dann loss: 1.9978 - reconstruction loss: 0.0350128/3811 - total loss: 0.2291 - classification loss: 0.0127 - dann loss: 1.9982 - reconstruction loss: 0.0350128/3811 - total loss: 0.2291 - classification loss: 0.0128 - dann loss: 1.9982 - reconstruction loss: 0.0350128/3811 - total loss: 0.2291 - classification loss: 0.0128 - dann loss: 1.9987 - reconstruction loss: 0.034998/3811 - total loss: 0.2291 - classification loss: 0.0128 - dann loss: 1.9992 - reconstruction loss: 0.03480/3811 - total loss: 0.2291 - classification loss: 0.0128 - dann loss: 1.9992 - reconstruction loss: 0.0348Epoch 34/150, Current strat Epoch 34/100
use_perm = True
switching perm
128/3811 - total loss: 0.2306 - classification loss: 0.0095 - dann loss: 2.0170 - reconstruction loss: 0.0349128/3811 - total loss: 0.2346 - classification loss: 0.0148 - dann loss: 2.0440 - reconstruction loss: 0.0360128/3811 - total loss: 0.2311 - classification loss: 0.0136 - dann loss: 2.0228 - reconstruction loss: 0.0343128/3811 - total loss: 0.2306 - classification loss: 0.0120 - dann loss: 2.0197 - reconstruction loss: 0.0343128/3811 - total loss: 0.2298 - classification loss: 0.0136 - dann loss: 2.0072 - reconstruction loss: 0.0347128/3811 - total loss: 0.2309 - classification loss: 0.0130 - dann loss: 2.0185 - reconstruction loss: 0.0347128/3811 - total loss: 0.2314 - classification loss: 0.0128 - dann loss: 2.0241 - reconstruction loss: 0.0347128/3811 - total loss: 0.2313 - classification loss: 0.0161 - dann loss: 2.0176 - reconstruction loss: 0.0349128/3811 - total loss: 0.2311 - classification loss: 0.0167 - dann loss: 2.0128 - reconstruction loss: 0.0352128/3811 - total loss: 0.2312 - classification loss: 0.0162 - dann loss: 2.0156 - reconstruction loss: 0.0350128/3811 - total loss: 0.2309 - classification loss: 0.0153 - dann loss: 2.0136 - reconstruction loss: 0.0350128/3811 - total loss: 0.2305 - classification loss: 0.0151 - dann loss: 2.0093 - reconstruction loss: 0.0351128/3811 - total loss: 0.2300 - classification loss: 0.0144 - dann loss: 2.0054 - reconstruction loss: 0.0351128/3811 - total loss: 0.2298 - classification loss: 0.0142 - dann loss: 2.0031 - reconstruction loss: 0.0351128/3811 - total loss: 0.2294 - classification loss: 0.0135 - dann loss: 2.0005 - reconstruction loss: 0.0351128/3811 - total loss: 0.2293 - classification loss: 0.0134 - dann loss: 1.9996 - reconstruction loss: 0.0349128/3811 - total loss: 0.2291 - classification loss: 0.0131 - dann loss: 1.9985 - reconstruction loss: 0.0349128/3811 - total loss: 0.2286 - classification loss: 0.0129 - dann loss: 1.9941 - reconstruction loss: 0.0349128/3811 - total loss: 0.2280 - classification loss: 0.0127 - dann loss: 1.9884 - reconstruction loss: 0.0349128/3811 - total loss: 0.2278 - classification loss: 0.0126 - dann loss: 1.9865 - reconstruction loss: 0.0349128/3811 - total loss: 0.2274 - classification loss: 0.0123 - dann loss: 1.9823 - reconstruction loss: 0.0349128/3811 - total loss: 0.2273 - classification loss: 0.0122 - dann loss: 1.9818 - reconstruction loss: 0.0349128/3811 - total loss: 0.2268 - classification loss: 0.0124 - dann loss: 1.9770 - reconstruction loss: 0.0348128/3811 - total loss: 0.2266 - classification loss: 0.0122 - dann loss: 1.9751 - reconstruction loss: 0.0348128/3811 - total loss: 0.2265 - classification loss: 0.0121 - dann loss: 1.9742 - reconstruction loss: 0.0348128/3811 - total loss: 0.2265 - classification loss: 0.0122 - dann loss: 1.9734 - reconstruction loss: 0.0349128/3811 - total loss: 0.2262 - classification loss: 0.0121 - dann loss: 1.9708 - reconstruction loss: 0.0349128/3811 - total loss: 0.2260 - classification loss: 0.0119 - dann loss: 1.9689 - reconstruction loss: 0.0348128/3811 - total loss: 0.2256 - classification loss: 0.0118 - dann loss: 1.9663 - reconstruction loss: 0.034898/3811 - total loss: 0.2257 - classification loss: 0.0117 - dann loss: 1.9668 - reconstruction loss: 0.03480/3811 - total loss: 0.2257 - classification loss: 0.0117 - dann loss: 1.9668 - reconstruction loss: 0.0348Epoch 35/150, Current strat Epoch 35/100
use_perm = True
switching perm
128/3811 - total loss: 0.2240 - classification loss: 0.0112 - dann loss: 1.9499 - reconstruction loss: 0.0349128/3811 - total loss: 0.2226 - classification loss: 0.0095 - dann loss: 1.9320 - reconstruction loss: 0.0355128/3811 - total loss: 0.2226 - classification loss: 0.0107 - dann loss: 1.9343 - reconstruction loss: 0.0351128/3811 - total loss: 0.2215 - classification loss: 0.0109 - dann loss: 1.9256 - reconstruction loss: 0.0348128/3811 - total loss: 0.2233 - classification loss: 0.0111 - dann loss: 1.9425 - reconstruction loss: 0.0349128/3811 - total loss: 0.2228 - classification loss: 0.0109 - dann loss: 1.9370 - reconstruction loss: 0.0350128/3811 - total loss: 0.2229 - classification loss: 0.0103 - dann loss: 1.9415 - reconstruction loss: 0.0347128/3811 - total loss: 0.2225 - classification loss: 0.0099 - dann loss: 1.9378 - reconstruction loss: 0.0346128/3811 - total loss: 0.2211 - classification loss: 0.0095 - dann loss: 1.9238 - reconstruction loss: 0.0347128/3811 - total loss: 0.2209 - classification loss: 0.0096 - dann loss: 1.9202 - reconstruction loss: 0.0349128/3811 - total loss: 0.2198 - classification loss: 0.0093 - dann loss: 1.9118 - reconstruction loss: 0.0347128/3811 - total loss: 0.2200 - classification loss: 0.0102 - dann loss: 1.9120 - reconstruction loss: 0.0347128/3811 - total loss: 0.2196 - classification loss: 0.0098 - dann loss: 1.9091 - reconstruction loss: 0.0347128/3811 - total loss: 0.2192 - classification loss: 0.0100 - dann loss: 1.9044 - reconstruction loss: 0.0347128/3811 - total loss: 0.2188 - classification loss: 0.0099 - dann loss: 1.9016 - reconstruction loss: 0.0346128/3811 - total loss: 0.2183 - classification loss: 0.0097 - dann loss: 1.8958 - reconstruction loss: 0.0346128/3811 - total loss: 0.2181 - classification loss: 0.0096 - dann loss: 1.8930 - reconstruction loss: 0.0348128/3811 - total loss: 0.2179 - classification loss: 0.0097 - dann loss: 1.8909 - reconstruction loss: 0.0348128/3811 - total loss: 0.2177 - classification loss: 0.0099 - dann loss: 1.8891 - reconstruction loss: 0.0348128/3811 - total loss: 0.2174 - classification loss: 0.0098 - dann loss: 1.8872 - reconstruction loss: 0.0347128/3811 - total loss: 0.2175 - classification loss: 0.0097 - dann loss: 1.8871 - reconstruction loss: 0.0347128/3811 - total loss: 0.2172 - classification loss: 0.0097 - dann loss: 1.8844 - reconstruction loss: 0.0347128/3811 - total loss: 0.2169 - classification loss: 0.0095 - dann loss: 1.8823 - reconstruction loss: 0.0347128/3811 - total loss: 0.2168 - classification loss: 0.0095 - dann loss: 1.8807 - reconstruction loss: 0.0347128/3811 - total loss: 0.2168 - classification loss: 0.0095 - dann loss: 1.8807 - reconstruction loss: 0.0348128/3811 - total loss: 0.2167 - classification loss: 0.0097 - dann loss: 1.8788 - reconstruction loss: 0.0348128/3811 - total loss: 0.2165 - classification loss: 0.0097 - dann loss: 1.8772 - reconstruction loss: 0.0348128/3811 - total loss: 0.2165 - classification loss: 0.0095 - dann loss: 1.8765 - reconstruction loss: 0.0348128/3811 - total loss: 0.2164 - classification loss: 0.0094 - dann loss: 1.8753 - reconstruction loss: 0.034898/3811 - total loss: 0.2160 - classification loss: 0.0095 - dann loss: 1.8721 - reconstruction loss: 0.03480/3811 - total loss: 0.2160 - classification loss: 0.0095 - dann loss: 1.8721 - reconstruction loss: 0.0348Epoch 36/150, Current strat Epoch 36/100
use_perm = True
switching perm
128/3811 - total loss: 0.2069 - classification loss: 0.0087 - dann loss: 1.7876 - reconstruction loss: 0.0340128/3811 - total loss: 0.2133 - classification loss: 0.0093 - dann loss: 1.8492 - reconstruction loss: 0.0344128/3811 - total loss: 0.2151 - classification loss: 0.0077 - dann loss: 1.8675 - reconstruction loss: 0.0344128/3811 - total loss: 0.2189 - classification loss: 0.0077 - dann loss: 1.9035 - reconstruction loss: 0.0348128/3811 - total loss: 0.2174 - classification loss: 0.0075 - dann loss: 1.8862 - reconstruction loss: 0.0350128/3811 - total loss: 0.2170 - classification loss: 0.0073 - dann loss: 1.8828 - reconstruction loss: 0.0349128/3811 - total loss: 0.2169 - classification loss: 0.0070 - dann loss: 1.8810 - reconstruction loss: 0.0351128/3811 - total loss: 0.2168 - classification loss: 0.0077 - dann loss: 1.8804 - reconstruction loss: 0.0349128/3811 - total loss: 0.2173 - classification loss: 0.0074 - dann loss: 1.8862 - reconstruction loss: 0.0349128/3811 - total loss: 0.2186 - classification loss: 0.0083 - dann loss: 1.8976 - reconstruction loss: 0.0350128/3811 - total loss: 0.2185 - classification loss: 0.0080 - dann loss: 1.8981 - reconstruction loss: 0.0349128/3811 - total loss: 0.2190 - classification loss: 0.0079 - dann loss: 1.9027 - reconstruction loss: 0.0349128/3811 - total loss: 0.2194 - classification loss: 0.0079 - dann loss: 1.9054 - reconstruction loss: 0.0352128/3811 - total loss: 0.2195 - classification loss: 0.0079 - dann loss: 1.9058 - reconstruction loss: 0.0352128/3742 - total loss: 0.2703 - classification loss: 0.0267 - dann loss: 2.3957 - reconstruction loss: 0.0351128/3742 - total loss: 0.2702 - classification loss: 0.0272 - dann loss: 2.3943 - reconstruction loss: 0.0351128/3742 - total loss: 0.2702 - classification loss: 0.0272 - dann loss: 2.3942 - reconstruction loss: 0.0350128/3742 - total loss: 0.2703 - classification loss: 0.0276 - dann loss: 2.3944 - reconstruction loss: 0.0351128/3742 - total loss: 0.2702 - classification loss: 0.0278 - dann loss: 2.3934 - reconstruction loss: 0.0351128/3742 - total loss: 0.2698 - classification loss: 0.0276 - dann loss: 2.3901 - reconstruction loss: 0.0351128/3742 - total loss: 0.2694 - classification loss: 0.0277 - dann loss: 2.3862 - reconstruction loss: 0.0351128/3742 - total loss: 0.2693 - classification loss: 0.0274 - dann loss: 2.3850 - reconstruction loss: 0.0351128/3742 - total loss: 0.2694 - classification loss: 0.0279 - dann loss: 2.3857 - reconstruction loss: 0.0350128/3742 - total loss: 0.2691 - classification loss: 0.0275 - dann loss: 2.3826 - reconstruction loss: 0.0351128/3742 - total loss: 0.2690 - classification loss: 0.0272 - dann loss: 2.3817 - reconstruction loss: 0.0352128/3742 - total loss: 0.2687 - classification loss: 0.0271 - dann loss: 2.3787 - reconstruction loss: 0.0352128/3742 - total loss: 0.2683 - classification loss: 0.0268 - dann loss: 2.3755 - reconstruction loss: 0.0351128/3742 - total loss: 0.2684 - classification loss: 0.0269 - dann loss: 2.3757 - reconstruction loss: 0.035129/3742 - total loss: 0.2679 - classification loss: 0.0266 - dann loss: 2.3720 - reconstruction loss: 0.03500/3742 - total loss: 0.2679 - classification loss: 0.0266 - dann loss: 2.3720 - reconstruction loss: 0.0350Epoch 18/150, Current strat Epoch 18/100
use_perm = True
switching perm
128/3742 - total loss: 0.2697 - classification loss: 0.0165 - dann loss: 2.3850 - reconstruction loss: 0.0369128/3742 - total loss: 0.2657 - classification loss: 0.0203 - dann loss: 2.3483 - reconstruction loss: 0.0361128/3742 - total loss: 0.2645 - classification loss: 0.0202 - dann loss: 2.3391 - reconstruction loss: 0.0357128/3742 - total loss: 0.2635 - classification loss: 0.0198 - dann loss: 2.3292 - reconstruction loss: 0.0357128/3742 - total loss: 0.2633 - classification loss: 0.0191 - dann loss: 2.3294 - reconstruction loss: 0.0355128/3742 - total loss: 0.2621 - classification loss: 0.0180 - dann loss: 2.3226 - reconstruction loss: 0.0351128/3742 - total loss: 0.2620 - classification loss: 0.0174 - dann loss: 2.3231 - reconstruction loss: 0.0350128/3742 - total loss: 0.2620 - classification loss: 0.0200 - dann loss: 2.3211 - reconstruction loss: 0.0348128/3742 - total loss: 0.2617 - classification loss: 0.0201 - dann loss: 2.3198 - reconstruction loss: 0.0347128/3742 - total loss: 0.2617 - classification loss: 0.0200 - dann loss: 2.3188 - reconstruction loss: 0.0348128/3742 - total loss: 0.2616 - classification loss: 0.0202 - dann loss: 2.3178 - reconstruction loss: 0.0348128/3742 - total loss: 0.2616 - classification loss: 0.0202 - dann loss: 2.3182 - reconstruction loss: 0.0347128/3742 - total loss: 0.2616 - classification loss: 0.0200 - dann loss: 2.3186 - reconstruction loss: 0.0347128/3742 - total loss: 0.2619 - classification loss: 0.0213 - dann loss: 2.3193 - reconstruction loss: 0.0349128/3742 - total loss: 0.2621 - classification loss: 0.0211 - dann loss: 2.3214 - reconstruction loss: 0.0348128/3742 - total loss: 0.2622 - classification loss: 0.0208 - dann loss: 2.3222 - reconstruction loss: 0.0348128/3742 - total loss: 0.2620 - classification loss: 0.0212 - dann loss: 2.3190 - reconstruction loss: 0.0349128/3742 - total loss: 0.2618 - classification loss: 0.0215 - dann loss: 2.3174 - reconstruction loss: 0.0349128/3742 - total loss: 0.2618 - classification loss: 0.0212 - dann loss: 2.3182 - reconstruction loss: 0.0349128/3742 - total loss: 0.2615 - classification loss: 0.0209 - dann loss: 2.3159 - reconstruction loss: 0.0348128/3742 - total loss: 0.2617 - classification loss: 0.0210 - dann loss: 2.3173 - reconstruction loss: 0.0349128/3742 - total loss: 0.2617 - classification loss: 0.0219 - dann loss: 2.3160 - reconstruction loss: 0.0349128/3742 - total loss: 0.2615 - classification loss: 0.0220 - dann loss: 2.3132 - reconstruction loss: 0.0349128/3742 - total loss: 0.2616 - classification loss: 0.0217 - dann loss: 2.3150 - reconstruction loss: 0.0349128/3742 - total loss: 0.2618 - classification loss: 0.0217 - dann loss: 2.3171 - reconstruction loss: 0.0349128/3742 - total loss: 0.2618 - classification loss: 0.0217 - dann loss: 2.3166 - reconstruction loss: 0.0349128/3742 - total loss: 0.2617 - classification loss: 0.0215 - dann loss: 2.3161 - reconstruction loss: 0.0350128/3742 - total loss: 0.2618 - classification loss: 0.0214 - dann loss: 2.3170 - reconstruction loss: 0.0350128/3742 - total loss: 0.2618 - classification loss: 0.0217 - dann loss: 2.3164 - reconstruction loss: 0.035029/3742 - total loss: 0.2618 - classification loss: 0.0223 - dann loss: 2.3155 - reconstruction loss: 0.03500/3742 - total loss: 0.2618 - classification loss: 0.0223 - dann loss: 2.3155 - reconstruction loss: 0.0350Epoch 19/150, Current strat Epoch 19/100
use_perm = True
switching perm
128/3742 - total loss: 0.2585 - classification loss: 0.0155 - dann loss: 2.3025 - reconstruction loss: 0.0334128/3742 - total loss: 0.2616 - classification loss: 0.0229 - dann loss: 2.3250 - reconstruction loss: 0.0335128/3742 - total loss: 0.2640 - classification loss: 0.0216 - dann loss: 2.3384 - reconstruction loss: 0.0350128/3742 - total loss: 0.2627 - classification loss: 0.0197 - dann loss: 2.3266 - reconstruction loss: 0.0350128/3742 - total loss: 0.2615 - classification loss: 0.0208 - dann loss: 2.3114 - reconstruction loss: 0.0354128/3742 - total loss: 0.2613 - classification loss: 0.0198 - dann loss: 2.3133 - reconstruction loss: 0.0350128/3742 - total loss: 0.2607 - classification loss: 0.0191 - dann loss: 2.3083 - reconstruction loss: 0.0349128/3742 - total loss: 0.2616 - classification loss: 0.0201 - dann loss: 2.3171 - reconstruction loss: 0.0349128/3742 - total loss: 0.2614 - classification loss: 0.0206 - dann loss: 2.3152 - reconstruction loss: 0.0348128/3742 - total loss: 0.2611 - classification loss: 0.0214 - dann loss: 2.3112 - reconstruction loss: 0.0348128/3742 - total loss: 0.2611 - classification loss: 0.0205 - dann loss: 2.3134 - reconstruction loss: 0.0347128/3742 - total loss: 0.2608 - classification loss: 0.0206 - dann loss: 2.3089 - reconstruction loss: 0.0349128/3742 - total loss: 0.2604 - classification loss: 0.0203 - dann loss: 2.3051 - reconstruction loss: 0.0349128/3742 - total loss: 0.2606 - classification loss: 0.0202 - dann loss: 2.3068 - reconstruction loss: 0.0349128/3742 - total loss: 0.2605 - classification loss: 0.0198 - dann loss: 2.3065 - reconstruction loss: 0.0348128/3742 - total loss: 0.2604 - classification loss: 0.0194 - dann loss: 2.3057 - reconstruction loss: 0.0348128/3742 - total loss: 0.2602 - classification loss: 0.0190 - dann loss: 2.3048 - reconstruction loss: 0.0348128/3742 - total loss: 0.2603 - classification loss: 0.0188 - dann loss: 2.3054 - reconstruction loss: 0.0349128/3742 - total loss: 0.2597 - classification loss: 0.0184 - dann loss: 2.2997 - reconstruction loss: 0.0349128/3742 - total loss: 0.2594 - classification loss: 0.0186 - dann loss: 2.2964 - reconstruction loss: 0.0349128/3742 - total loss: 0.2595 - classification loss: 0.0186 - dann loss: 2.2963 - reconstruction loss: 0.0350128/3742 - total loss: 0.2593 - classification loss: 0.0184 - dann loss: 2.2943 - reconstruction loss: 0.0351128/3742 - total loss: 0.2591 - classification loss: 0.0182 - dann loss: 2.2924 - reconstruction loss: 0.0351128/3742 - total loss: 0.2590 - classification loss: 0.0182 - dann loss: 2.2915 - reconstruction loss: 0.0350128/3742 - total loss: 0.2589 - classification loss: 0.0180 - dann loss: 2.2911 - reconstruction loss: 0.0350128/3742 - total loss: 0.2587 - classification loss: 0.0181 - dann loss: 2.2888 - reconstruction loss: 0.0350128/3742 - total loss: 0.2584 - classification loss: 0.0181 - dann loss: 2.2865 - reconstruction loss: 0.0349128/3811 - total loss: 0.2199 - classification loss: 0.0079 - dann loss: 1.9096 - reconstruction loss: 0.0352128/3811 - total loss: 0.2191 - classification loss: 0.0079 - dann loss: 1.9025 - reconstruction loss: 0.0351128/3811 - total loss: 0.2202 - classification loss: 0.0082 - dann loss: 1.9118 - reconstruction loss: 0.0352128/3811 - total loss: 0.2201 - classification loss: 0.0081 - dann loss: 1.9113 - reconstruction loss: 0.0352128/3811 - total loss: 0.2204 - classification loss: 0.0080 - dann loss: 1.9147 - reconstruction loss: 0.0352128/3811 - total loss: 0.2210 - classification loss: 0.0082 - dann loss: 1.9208 - reconstruction loss: 0.0351128/3811 - total loss: 0.2211 - classification loss: 0.0082 - dann loss: 1.9214 - reconstruction loss: 0.0351128/3811 - total loss: 0.2215 - classification loss: 0.0086 - dann loss: 1.9261 - reconstruction loss: 0.0350128/3811 - total loss: 0.2218 - classification loss: 0.0089 - dann loss: 1.9292 - reconstruction loss: 0.0350128/3811 - total loss: 0.2220 - classification loss: 0.0090 - dann loss: 1.9313 - reconstruction loss: 0.0350128/3811 - total loss: 0.2226 - classification loss: 0.0091 - dann loss: 1.9367 - reconstruction loss: 0.0350128/3811 - total loss: 0.2228 - classification loss: 0.0092 - dann loss: 1.9394 - reconstruction loss: 0.0349128/3811 - total loss: 0.2228 - classification loss: 0.0091 - dann loss: 1.9399 - reconstruction loss: 0.0349128/3811 - total loss: 0.2228 - classification loss: 0.0090 - dann loss: 1.9397 - reconstruction loss: 0.0349128/3811 - total loss: 0.2228 - classification loss: 0.0093 - dann loss: 1.9402 - reconstruction loss: 0.034998/3811 - total loss: 0.2230 - classification loss: 0.0091 - dann loss: 1.9416 - reconstruction loss: 0.03490/3811 - total loss: 0.2230 - classification loss: 0.0091 - dann loss: 1.9416 - reconstruction loss: 0.0349Epoch 37/150, Current strat Epoch 37/100
use_perm = True
switching perm
128/3811 - total loss: 0.2241 - classification loss: 0.0134 - dann loss: 1.9523 - reconstruction loss: 0.0344128/3811 - total loss: 0.2244 - classification loss: 0.0106 - dann loss: 1.9588 - reconstruction loss: 0.0343128/3811 - total loss: 0.2247 - classification loss: 0.0123 - dann loss: 1.9617 - reconstruction loss: 0.0341128/3811 - total loss: 0.2254 - classification loss: 0.0114 - dann loss: 1.9696 - reconstruction loss: 0.0342128/3811 - total loss: 0.2275 - classification loss: 0.0113 - dann loss: 1.9905 - reconstruction loss: 0.0341128/3811 - total loss: 0.2285 - classification loss: 0.0114 - dann loss: 1.9972 - reconstruction loss: 0.0345128/3811 - total loss: 0.2296 - classification loss: 0.0112 - dann loss: 2.0090 - reconstruction loss: 0.0345128/3811 - total loss: 0.2303 - classification loss: 0.0113 - dann loss: 2.0170 - reconstruction loss: 0.0344128/3811 - total loss: 0.2313 - classification loss: 0.0118 - dann loss: 2.0248 - reconstruction loss: 0.0345128/3811 - total loss: 0.2325 - classification loss: 0.0124 - dann loss: 2.0361 - reconstruction loss: 0.0346128/3811 - total loss: 0.2335 - classification loss: 0.0129 - dann loss: 2.0466 - reconstruction loss: 0.0345128/3811 - total loss: 0.2340 - classification loss: 0.0127 - dann loss: 2.0503 - reconstruction loss: 0.0346128/3811 - total loss: 0.2346 - classification loss: 0.0127 - dann loss: 2.0571 - reconstruction loss: 0.0345128/3811 - total loss: 0.2344 - classification loss: 0.0127 - dann loss: 2.0549 - reconstruction loss: 0.0346128/3811 - total loss: 0.2352 - classification loss: 0.0130 - dann loss: 2.0619 - reconstruction loss: 0.0346128/3811 - total loss: 0.2362 - classification loss: 0.0134 - dann loss: 2.0724 - reconstruction loss: 0.0346128/3811 - total loss: 0.2358 - classification loss: 0.0135 - dann loss: 2.0679 - reconstruction loss: 0.0346128/3811 - total loss: 0.2356 - classification loss: 0.0141 - dann loss: 2.0649 - reconstruction loss: 0.0346128/3811 - total loss: 0.2359 - classification loss: 0.0141 - dann loss: 2.0671 - reconstruction loss: 0.0347128/3811 - total loss: 0.2356 - classification loss: 0.0142 - dann loss: 2.0646 - reconstruction loss: 0.0347128/3811 - total loss: 0.2355 - classification loss: 0.0141 - dann loss: 2.0640 - reconstruction loss: 0.0347128/3811 - total loss: 0.2358 - classification loss: 0.0139 - dann loss: 2.0660 - reconstruction loss: 0.0347128/3811 - total loss: 0.2359 - classification loss: 0.0139 - dann loss: 2.0663 - reconstruction loss: 0.0348128/3811 - total loss: 0.2357 - classification loss: 0.0137 - dann loss: 2.0645 - reconstruction loss: 0.0349128/3811 - total loss: 0.2360 - classification loss: 0.0142 - dann loss: 2.0659 - reconstruction loss: 0.0349128/3811 - total loss: 0.2361 - classification loss: 0.0141 - dann loss: 2.0674 - reconstruction loss: 0.0349128/3811 - total loss: 0.2360 - classification loss: 0.0141 - dann loss: 2.0666 - reconstruction loss: 0.0349128/3811 - total loss: 0.2360 - classification loss: 0.0142 - dann loss: 2.0669 - reconstruction loss: 0.0349128/3811 - total loss: 0.2359 - classification loss: 0.0141 - dann loss: 2.0653 - reconstruction loss: 0.035098/3811 - total loss: 0.2358 - classification loss: 0.0140 - dann loss: 2.0645 - reconstruction loss: 0.03500/3811 - total loss: 0.2358 - classification loss: 0.0140 - dann loss: 2.0645 - reconstruction loss: 0.0350Epoch 38/150, Current strat Epoch 38/100
use_perm = True
switching perm
128/3811 - total loss: 0.2448 - classification loss: 0.0113 - dann loss: 2.1727 - reconstruction loss: 0.0330128/3811 - total loss: 0.2392 - classification loss: 0.0142 - dann loss: 2.1021 - reconstruction loss: 0.0344128/3811 - total loss: 0.2375 - classification loss: 0.0118 - dann loss: 2.0879 - reconstruction loss: 0.0344128/3811 - total loss: 0.2382 - classification loss: 0.0119 - dann loss: 2.0908 - reconstruction loss: 0.0349128/3811 - total loss: 0.2371 - classification loss: 0.0117 - dann loss: 2.0780 - reconstruction loss: 0.0352128/3811 - total loss: 0.2358 - classification loss: 0.0106 - dann loss: 2.0695 - reconstruction loss: 0.0348128/3811 - total loss: 0.2353 - classification loss: 0.0104 - dann loss: 2.0627 - reconstruction loss: 0.0350128/3811 - total loss: 0.2360 - classification loss: 0.0115 - dann loss: 2.0692 - reconstruction loss: 0.0350128/3811 - total loss: 0.2340 - classification loss: 0.0111 - dann loss: 2.0482 - reconstruction loss: 0.0350128/3811 - total loss: 0.2335 - classification loss: 0.0113 - dann loss: 2.0438 - reconstruction loss: 0.0350128/3811 - total loss: 0.2320 - classification loss: 0.0108 - dann loss: 2.0280 - reconstruction loss: 0.0351128/3811 - total loss: 0.2318 - classification loss: 0.0112 - dann loss: 2.0255 - reconstruction loss: 0.0352128/3811 - total loss: 0.2313 - classification loss: 0.0115 - dann loss: 2.0206 - reconstruction loss: 0.0352128/3811 - total loss: 0.2312 - classification loss: 0.0112 - dann loss: 2.0191 - reconstruction loss: 0.0352128/3811 - total loss: 0.2310 - classification loss: 0.0109 - dann loss: 2.0183 - reconstruction loss: 0.0351128/3811 - total loss: 0.2302 - classification loss: 0.0108 - dann loss: 2.0107 - reconstruction loss: 0.0351128/3811 - total loss: 0.2298 - classification loss: 0.0109 - dann loss: 2.0073 - reconstruction loss: 0.0350128/3811 - total loss: 0.2296 - classification loss: 0.0108 - dann loss: 2.0060 - reconstruction loss: 0.0349128/3811 - total loss: 0.2299 - classification loss: 0.0111 - dann loss: 2.0089 - reconstruction loss: 0.0349128/3811 - total loss: 0.2294 - classification loss: 0.0109 - dann loss: 2.0048 - reconstruction loss: 0.0348128/3811 - total loss: 0.2292 - classification loss: 0.0107 - dann loss: 2.0017 - reconstruction loss: 0.0350128/3811 - total loss: 0.2291 - classification loss: 0.0105 - dann loss: 2.0014 - reconstruction loss: 0.0349128/3811 - total loss: 0.2289 - classification loss: 0.0103 - dann loss: 1.9992 - reconstruction loss: 0.0349128/3811 - total loss: 0.2290 - classification loss: 0.0101 - dann loss: 1.9997 - reconstruction loss: 0.0350128/3811 - total loss: 0.2285 - classification loss: 0.0099 - dann loss: 1.9958 - reconstruction loss: 0.0349128/3811 - total loss: 0.2281 - classification loss: 0.0098 - dann loss: 1.9919 - reconstruction loss: 0.0349128/3811 - total loss: 0.2278 - classification loss: 0.0104 - dann loss: 1.9885 - reconstruction loss: 0.0349128/3811 - total loss: 0.2277 - classification loss: 0.0103 - dann loss: 1.9879 - reconstruction loss: 0.0348128/3811 - total loss: 0.2275 - classification loss: 0.0103 - dann loss: 1.9853 - reconstruction loss: 0.034998/3811 - total loss: 0.2271 - classification loss: 0.0101 - dann loss: 1.9826 - reconstruction loss: 0.03480/3811 - total loss: 0.2271 - classification loss: 0.0101 - dann loss: 1.9826 - reconstruction loss: 0.0348Epoch 39/150, Current strat Epoch 39/100
use_perm = True
switching perm
128/3811 - total loss: 0.2251 - classification loss: 0.0080 - dann loss: 1.9647 - reconstruction loss: 0.0347128/3811 - total loss: 0.2182 - classification loss: 0.0085 - dann loss: 1.8923 - reconstruction loss: 0.0351128/3811 - total loss: 0.2182 - classification loss: 0.0078 - dann loss: 1.8933 - reconstruction loss: 0.0351128/3811 - total loss: 0.2198 - classification loss: 0.0081 - dann loss: 1.9099 - reconstruction loss: 0.0350128/3811 - total loss: 0.2207 - classification loss: 0.0082 - dann loss: 1.9194 - reconstruction loss: 0.0349128/3811 - total loss: 0.2213 - classification loss: 0.0084 - dann loss: 1.9244 - reconstruction loss: 0.0350128/3811 - total loss: 0.2218 - classification loss: 0.0101 - dann loss: 1.9266 - reconstruction loss: 0.0351128/3811 - total loss: 0.2226 - classification loss: 0.0111 - dann loss: 1.9356 - reconstruction loss: 0.0349128/3811 - total loss: 0.2219 - classification loss: 0.0110 - dann loss: 1.9290 - reconstruction loss: 0.0349128/3811 - total loss: 0.2230 - classification loss: 0.0108 - dann loss: 1.9413 - reconstruction loss: 0.0348128/3811 - total loss: 0.2232 - classification loss: 0.0110 - dann loss: 1.9436 - reconstruction loss: 0.0346128/3811 - total loss: 0.2227 - classification loss: 0.0106 - dann loss: 1.9389 - reconstruction loss: 0.0347128/3811 - total loss: 0.2223 - classification loss: 0.0103 - dann loss: 1.9348 - reconstruction loss: 0.0347128/3811 - total loss: 0.2221 - classification loss: 0.0101 - dann loss: 1.9336 - reconstruction loss: 0.0346128/3811 - total loss: 0.2217 - classification loss: 0.0100 - dann loss: 1.9284 - reconstruction loss: 0.0348128/3811 - total loss: 0.2214 - classification loss: 0.0098 - dann loss: 1.9257 - reconstruction loss: 0.0348128/3811 - total loss: 0.2208 - classification loss: 0.0096 - dann loss: 1.9199 - reconstruction loss: 0.0348128/3811 - total loss: 0.2205 - classification loss: 0.0095 - dann loss: 1.9174 - reconstruction loss: 0.0348128/3811 - total loss: 0.2209 - classification loss: 0.0093 - dann loss: 1.9208 - reconstruction loss: 0.0348128/3811 - total loss: 0.2208 - classification loss: 0.0102 - dann loss: 1.9193 - reconstruction loss: 0.0348128/3811 - total loss: 0.2208 - classification loss: 0.0100 - dann loss: 1.9193 - reconstruction loss: 0.0348128/3811 - total loss: 0.2207 - classification loss: 0.0099 - dann loss: 1.9187 - reconstruction loss: 0.0348128/3811 - total loss: 0.2203 - classification loss: 0.0098 - dann loss: 1.9151 - reconstruction loss: 0.0348128/3811 - total loss: 0.2199 - classification loss: 0.0098 - dann loss: 1.9112 - reconstruction loss: 0.0347128/3811 - total loss: 0.2198 - classification loss: 0.0097 - dann loss: 1.9103 - reconstruction loss: 0.0348128/3811 - total loss: 0.2199 - classification loss: 0.0098 - dann loss: 1.9106 - reconstruction loss: 0.0348128/3811 - total loss: 0.2199 - classification loss: 0.0098 - dann loss: 1.9103 - reconstruction loss: 0.0348128/3811 - total loss: 0.2196 - classification loss: 0.0101 - dann loss: 1.9078 - reconstruction loss: 0.0347128/3811 - total loss: 0.2196 - classification loss: 0.0099 - dann loss: 1.9087 - reconstruction loss: 0.034798/3811 - total loss: 0.2199 - classification loss: 0.0098 - dann loss: 1.9114 - reconstruction loss: 0.03480/3811 - total loss: 0.2199 - classification loss: 0.0098 - dann loss: 1.9114 - reconstruction loss: 0.0348Early stopping at epoch 19, restoring model parameters from this epoch
adam
Epoch 40/150, Current strat Epoch 1/50
use_perm = False
switching perm
128/3811 - total loss: 0.0092 - classification loss: 0.0921 - dann loss: 1.9595 - reconstruction loss: 0.0406128/3811 - total loss: 0.0078 - classification loss: 0.0776 - dann loss: 1.9717 - reconstruction loss: 0.0392128/3811 - total loss: 0.0066 - classification loss: 0.0661 - dann loss: 1.9993 - reconstruction loss: 0.0378128/3811 - total loss: 0.0067 - classification loss: 0.0672 - dann loss: 2.0047 - reconstruction loss: 0.0376128/3811 - total loss: 0.0061 - classification loss: 0.0607 - dann loss: 2.0146 - reconstruction loss: 0.0371128/3811 - total loss: 0.0062 - classification loss: 0.0616 - dann loss: 2.0192 - reconstruction loss: 0.0370128/3811 - total loss: 0.0059 - classification loss: 0.0590 - dann loss: 2.0260 - reconstruction loss: 0.0371128/3811 - total loss: 0.0056 - classification loss: 0.0564 - dann loss: 2.0218 - reconstruction loss: 0.0370128/3811 - total loss: 0.0052 - classification loss: 0.0524 - dann loss: 2.0239 - reconstruction loss: 0.0369128/3811 - total loss: 0.0050 - classification loss: 0.0499 - dann loss: 2.0243 - reconstruction loss: 0.0369128/3811 - total loss: 0.0047 - classification loss: 0.0473 - dann loss: 2.0282 - reconstruction loss: 0.0370128/3811 - total loss: 0.0045 - classification loss: 0.0455 - dann loss: 2.0343 - reconstruction loss: 0.0371128/3811 - total loss: 0.0043 - classification loss: 0.0431 - dann loss: 2.0328 - reconstruction loss: 0.0370128/3811 - total loss: 0.0042 - classification loss: 0.0415 - dann loss: 2.0348 - reconstruction loss: 0.0371128/3811 - total loss: 0.0041 - classification loss: 0.0414 - dann loss: 2.0352 - reconstruction loss: 0.0369128/3811 - total loss: 0.0040 - classification loss: 0.0403 - dann loss: 2.0337 - reconstruction loss: 0.0370128/3811 - total loss: 0.0039 - classification loss: 0.0393 - dann loss: 2.0361 - reconstruction loss: 0.0369128/3811 - total loss: 0.0038 - classification loss: 0.0381 - dann loss: 2.0379 - reconstruction loss: 0.0370128/3811 - total loss: 0.0037 - classification loss: 0.0367 - dann loss: 2.0386 - reconstruction loss: 0.0370128/3811 - total loss: 0.0036 - classification loss: 0.0358 - dann loss: 2.0354 - reconstruction loss: 0.0371128/3811 - total loss: 0.0035 - classification loss: 0.0347 - dann loss: 2.0306 - reconstruction loss: 0.0371128/3811 - total loss: 0.0034 - classification loss: 0.0341 - dann loss: 2.0295 - reconstruction loss: 0.0370128/3811 - total loss: 0.0034 - classification loss: 0.0336 - dann loss: 2.0293 - reconstruction loss: 0.0370128/3811 - total loss: 0.0033 - classification loss: 0.0332 - dann loss: 2.0273 - reconstruction loss: 0.0370128/3811 - total loss: 0.0032 - classification loss: 0.0324 - dann loss: 2.0281 - reconstruction loss: 0.0370128/3811 - total loss: 0.0032 - classification loss: 0.0316 - dann loss: 2.0296 - reconstruction loss: 0.0371128/3811 - total loss: 0.0031 - classification loss: 0.0314 - dann loss: 2.0294 - reconstruction loss: 0.0372128/3811 - total loss: 0.0031 - classification loss: 0.0308 - dann loss: 2.0302 - reconstruction loss: 0.0373128/3811 - total loss: 0.0030 - classification loss: 0.0300 - dann loss: 2.0304 - reconstruction loss: 0.037298/3811 - total loss: 0.0029 - classification loss: 0.0295 - dann loss: 2.0325 - reconstruction loss: 0.03720/3811 - total loss: 0.0029 - classification loss: 0.0295 - dann loss: 2.0325 - reconstruction loss: 0.0372Epoch 41/150, Current strat Epoch 2/50
use_perm = False
switching perm
128/3811 - total loss: 0.0008 - classification loss: 0.0081 - dann loss: 1.9934 - reconstruction loss: 0.0349128/3811 - total loss: 0.0010 - classification loss: 0.0100 - dann loss: 2.0307 - reconstruction loss: 0.0352128/3811 - total loss: 0.0009 - classification loss: 0.0093 - dann loss: 2.0169 - reconstruction loss: 0.0359128/3811 - total loss: 0.0011 - classification loss: 0.0112 - dann loss: 2.0264 - reconstruction loss: 0.0360128/3811 - total loss: 0.0013 - classification loss: 0.0133 - dann loss: 2.0311 - reconstruction loss: 0.0356128/3811 - total loss: 0.0013 - classification loss: 0.0129 - dann loss: 2.0420 - reconstruction loss: 0.0359128/3811 - total loss: 0.0012 - classification loss: 0.0121 - dann loss: 2.0384 - reconstruction loss: 0.0362128/3811 - total loss: 0.0013 - classification loss: 0.0134 - dann loss: 2.0289 - reconstruction loss: 0.0362128/3811 - total loss: 0.0013 - classification loss: 0.0126 - dann loss: 2.0253 - reconstruction loss: 0.0363128/3811 - total loss: 0.0013 - classification loss: 0.0126 - dann loss: 2.0160 - reconstruction loss: 0.0365128/3811 - total loss: 0.0015 - classification loss: 0.0145 - dann loss: 2.0228 - reconstruction loss: 0.0367128/3811 - total loss: 0.0015 - classification loss: 0.0148 - dann loss: 2.0299 - reconstruction loss: 0.0367128/3811 - total loss: 0.0015 - classification loss: 0.0147 - dann loss: 2.0388 - reconstruction loss: 0.0368128/3811 - total loss: 0.0014 - classification loss: 0.0141 - dann loss: 2.0375 - reconstruction loss: 0.0369128/3811 - total loss: 0.0014 - classification loss: 0.0140 - dann loss: 2.0425 - reconstruction loss: 0.0369128/3811 - total loss: 0.0013 - classification loss: 0.0134 - dann loss: 2.0411 - reconstruction loss: 0.0369128/3811 - total loss: 0.0013 - classification loss: 0.0131 - dann loss: 2.0413 - reconstruction loss: 0.0369128/3811 - total loss: 0.0013 - classification loss: 0.0127 - dann loss: 2.0381 - reconstruction loss: 0.0369128/3811 - total loss: 0.0013 - classification loss: 0.0125 - dann loss: 2.0417 - reconstruction loss: 0.0369128/3811 - total loss: 0.0012 - classification loss: 0.0123 - dann loss: 2.0353 - reconstruction loss: 0.0369128/3811 - total loss: 0.0012 - classification loss: 0.0121 - dann loss: 2.0315 - reconstruction loss: 0.0369128/3811 - total loss: 0.0012 - classification loss: 0.0119 - dann loss: 2.0309 - reconstruction loss: 0.0370128/3811 - total loss: 0.0012 - classification loss: 0.0118 - dann loss: 2.0319 - reconstruction loss: 0.0370128/3811 - total loss: 0.0012 - classification loss: 0.0116 - dann loss: 2.0299 - reconstruction loss: 0.0371128/3811 - total loss: 0.0011 - classification loss: 0.0113 - dann loss: 2.0302 - reconstruction loss: 0.0372128/3811 - total loss: 0.0012 - classification loss: 0.0116 - dann loss: 2.0297 - reconstruction loss: 0.0372128/3811 - total loss: 0.0012 - classification loss: 0.0122 - dann loss: 2.0283 - reconstruction loss: 0.0371128/3811 - total loss: 0.0012 - classification loss: 0.0120 - dann loss: 2.0292 - reconstruction loss: 0.0372128/3811 - total loss: 0.0012 - classification loss: 0.0117 - dann loss: 2.0310 - reconstruction loss: 0.037298/3811 - total loss: 0.0012 - classification loss: 0.0117 - dann loss: 2.0323 - reconstruction loss: 0.03720/3811 - total loss: 0.0012 - classification loss: 0.0117 - dann loss: 2.0323 - reconstruction loss: 0.0372Epoch 42/150, Current strat Epoch 3/50
use_perm = False
switching perm
128/3811 - total loss: 0.0009 - classification loss: 0.0085 - dann loss: 1.9866 - reconstruction loss: 0.0376128/3811 - total loss: 0.0007 - classification loss: 0.0068 - dann loss: 2.0148 - reconstruction loss: 0.0377128/3811 - total loss: 0.0006 - classification loss: 0.0063 - dann loss: 2.0157 - reconstruction loss: 0.0371128/3811 - total loss: 0.0006 - classification loss: 0.0058 - dann loss: 2.0178 - reconstruction loss: 0.0366128/3811 - total loss: 0.0006 - classification loss: 0.0059 - dann loss: 2.0159 - reconstruction loss: 0.0367128/3811 - total loss: 0.0006 - classification loss: 0.0059 - dann loss: 2.0148 - reconstruction loss: 0.0369128/3811 - total loss: 0.0009 - classification loss: 0.0091 - dann loss: 2.0127 - reconstruction loss: 0.0370128/3811 - total loss: 0.0008 - classification loss: 0.0084 - dann loss: 2.0071 - reconstruction loss: 0.0371128/3811 - total loss: 0.0008 - classification loss: 0.0081 - dann loss: 2.0123 - reconstruction loss: 0.0371128/3811 - total loss: 0.0008 - classification loss: 0.0077 - dann loss: 2.0134 - reconstruction loss: 0.0370128/3811 - total loss: 0.0009 - classification loss: 0.0089 - dann loss: 2.0185 - reconstruction loss: 0.0371128/3811 - total loss: 0.0009 - classification loss: 0.0088 - dann loss: 2.0261 - reconstruction loss: 0.0371128/3811 - total loss: 0.0009 - classification loss: 0.0086 - dann loss: 2.0294 - reconstruction loss: 0.0371128/3811 - total loss: 0.0008 - classification loss: 0.0085 - dann loss: 2.0310 - reconstruction loss: 0.0372128/3811 - total loss: 0.0009 - classification loss: 0.0092 - dann loss: 2.0293 - reconstruction loss: 0.0371128/3811 - total loss: 0.0009 - classification loss: 0.0089 - dann loss: 2.0278 - reconstruction loss: 0.0371128/3811 - total loss: 0.0009 - classification loss: 0.0086 - dann loss: 2.0265 - reconstruction loss: 0.0371128/3811 - total loss: 0.0009 - classification loss: 0.0091 - dann loss: 2.0234 - reconstruction loss: 0.0371128/3811 - total loss: 0.0009 - classification loss: 0.0088 - dann loss: 2.0237 - reconstruction loss: 0.0372128/3811 - total loss: 0.0009 - classification loss: 0.0085 - dann loss: 2.0274 - reconstruction loss: 0.0372128/3811 - total loss: 0.0008 - classification loss: 0.0084 - dann loss: 2.0313 - reconstruction loss: 0.0371128/3811 - total loss: 0.0008 - classification loss: 0.0083 - dann loss: 2.0309 - reconstruction loss: 0.0371128/3811 - total loss: 0.0008 - classification loss: 0.0081 - dann loss: 2.0332 - reconstruction loss: 0.0371128/3811 - total loss: 0.0008 - classification loss: 0.0080 - dann loss: 2.0317 - reconstruction loss: 0.0370128/3811 - total loss: 0.0008 - classification loss: 0.0078 - dann loss: 2.0318 - reconstruction loss: 0.0371128/3811 - total loss: 0.0008 - classification loss: 0.0076 - dann loss: 2.0348 - reconstruction loss: 0.0371128/3811 - total loss: 0.0008 - classification loss: 0.0075 - dann loss: 2.0349 - reconstruction loss: 0.0370128/3811 - total loss: 0.0007 - classification loss: 0.0075 - dann loss: 2.0343 - reconstruction loss: 0.0371128/3811 - total loss: 0.0007 - classification loss: 0.0074 - dann loss: 2.0324 - reconstruction loss: 0.037198/3811 - total loss: 0.0007 - classification loss: 0.0074 - dann loss: 2.0319 - reconstruction loss: 0.03720/3811 - total loss: 0.0007 - classification loss: 0.0074 - dann loss: 2.0319 - reconstruction loss: 0.0372Epoch 43/150, Current strat Epoch 4/50
use_perm = False
switching perm
128/3811 - total loss: 0.0004 - classification loss: 0.0040 - dann loss: 2.0666 - reconstruction loss: 0.0383128/3811 - total loss: 0.0004 - classification loss: 0.0037 - dann loss: 2.0342 - reconstruction loss: 0.0373128/3811 - total loss: 0.0003 - classification loss: 0.0034 - dann loss: 2.0217 - reconstruction loss: 0.0369128/3811 - total loss: 0.0003 - classification loss: 0.0032 - dann loss: 2.0109 - reconstruction loss: 0.0363128/3811 - total loss: 0.0003 - classification loss: 0.0034 - dann loss: 2.0161 - reconstruction loss: 0.0365128/3811 - total loss: 0.0006 - classification loss: 0.0064 - dann loss: 2.0215 - reconstruction loss: 0.0365128/3811 - total loss: 0.0006 - classification loss: 0.0061 - dann loss: 2.0208 - reconstruction loss: 0.0366128/3811 - total loss: 0.0006 - classification loss: 0.0060 - dann loss: 2.0290 - reconstruction loss: 0.0366128/3811 - total loss: 0.0006 - classification loss: 0.0058 - dann loss: 2.0291 - reconstruction loss: 0.0366128/3811 - total loss: 0.0006 - classification loss: 0.0056 - dann loss: 2.0250 - reconstruction loss: 0.0370128/3811 - total loss: 0.0005 - classification loss: 0.0054 - dann loss: 2.0292 - reconstruction loss: 0.0369128/3811 - total loss: 0.0005 - classification loss: 0.0052 - dann loss: 2.0317 - reconstruction loss: 0.0369128/3811 - total loss: 0.0005 - classification loss: 0.0051 - dann loss: 2.0360 - reconstruction loss: 0.0369128/3811 - total loss: 0.0005 - classification loss: 0.0050 - dann loss: 2.0331 - reconstruction loss: 0.0370128/3811 - total loss: 0.0005 - classification loss: 0.0048 - dann loss: 2.0327 - reconstruction loss: 0.0370128/3811 - total loss: 0.0005 - classification loss: 0.0047 - dann loss: 2.0349 - reconstruction loss: 0.0372128/3811 - total loss: 0.0005 - classification loss: 0.0051 - dann loss: 2.0337 - reconstruction loss: 0.0371128/3811 - total loss: 0.0005 - classification loss: 0.0051 - dann loss: 2.0309 - reconstruction loss: 0.0370128/3811 - total loss: 0.0005 - classification loss: 0.0052 - dann loss: 2.0313 - reconstruction loss: 0.0369128/3811 - total loss: 0.0005 - classification loss: 0.0055 - dann loss: 2.0317 - reconstruction loss: 0.0369128/3811 - total loss: 0.0005 - classification loss: 0.0054 - dann loss: 2.0310 - reconstruction loss: 0.0370128/3811 - total loss: 0.0005 - classification loss: 0.0053 - dann loss: 2.0330 - reconstruction loss: 0.0370128/3811 - total loss: 0.0005 - classification loss: 0.0054 - dann loss: 2.0322 - reconstruction loss: 0.0370128/3811 - total loss: 0.0006 - classification loss: 0.0057 - dann loss: 2.0309 - reconstruction loss: 0.0370128/3811 - total loss: 0.0006 - classification loss: 0.0061 - dann loss: 2.0315 - reconstruction loss: 0.0371128/3811 - total loss: 0.0006 - classification loss: 0.0060 - dann loss: 2.0289 - reconstruction loss: 0.0372128/3811 - total loss: 0.0006 - classification loss: 0.0059 - dann loss: 2.0316 - reconstruction loss: 0.0371128/3811 - total loss: 0.0006 - classification loss: 0.0058 - dann loss: 2.0342 - reconstruction loss: 0.0371128/3811 - total loss: 0.0006 - classification loss: 0.0058 - dann loss: 2.0337 - reconstruction loss: 0.037298/3811 - total loss: 0.0006 - classification loss: 0.0057 - dann loss: 2.0315 - reconstruction loss: 0.03720/3811 - total loss: 0.0006 - classification loss: 0.0057 - dann loss: 2.0315 - reconstruction loss: 0.0372Epoch 44/150, Current strat Epoch 5/50
use_perm = False
switching perm
128/3811 - total loss: 0.0004 - classification loss: 0.0038 - dann loss: 2.0794 - reconstruction loss: 0.0393128/3811 - total loss: 0.0004 - classification loss: 0.0036 - dann loss: 2.0960 - reconstruction loss: 0.0385128/3811 - total loss: 0.0003 - classification loss: 0.0033 - dann loss: 2.0687 - reconstruction loss: 0.0382128/3811 - total loss: 0.0004 - classification loss: 0.0037 - dann loss: 2.0431 - reconstruction loss: 0.0379128/3811 - total loss: 0.0004 - classification loss: 0.0038 - dann loss: 2.0297 - reconstruction loss: 0.0377128/3811 - total loss: 0.0004 - classification loss: 0.0035 - dann loss: 2.0215 - reconstruction loss: 0.0379128/3811 - total loss: 0.0003 - classification loss: 0.0034 - dann loss: 2.0236 - reconstruction loss: 0.0381128/3811 - total loss: 0.0003 - classification loss: 0.0033 - dann loss: 2.0310 - reconstruction loss: 0.0377128/3811 - total loss: 0.0004 - classification loss: 0.0041 - dann loss: 2.0427 - reconstruction loss: 0.0377128/3811 - total loss: 0.0004 - classification loss: 0.0040 - dann loss: 2.0345 - reconstruction loss: 0.0376128/3811 - total loss: 0.0004 - classification loss: 0.0039 - dann loss: 2.0319 - reconstruction loss: 0.0376128/3811 - total loss: 0.0004 - classification loss: 0.0039 - dann loss: 2.0299 - reconstruction loss: 0.0375128/3811 - total loss: 0.0004 - classification loss: 0.0044 - dann loss: 2.0367 - reconstruction loss: 0.0374128/3811 - total loss: 0.0004 - classification loss: 0.0043 - dann loss: 2.0381 - reconstruction loss: 0.0373128/3811 - total loss: 0.0005 - classification loss: 0.0047 - dann loss: 2.0365 - reconstruction loss: 0.0374128/3811 - total loss: 0.0005 - classification loss: 0.0048 - dann loss: 2.0392 - reconstruction loss: 0.0374128/3811 - total loss: 0.0005 - classification loss: 0.0049 - dann loss: 2.0435 - reconstruction loss: 0.0374128/3811 - total loss: 0.0005 - classification loss: 0.0048 - dann loss: 2.0408 - reconstruction loss: 0.0374128/3811 - total loss: 0.0005 - classification loss: 0.0048 - dann loss: 2.0400 - reconstruction loss: 0.0374128/3811 - total loss: 0.0005 - classification loss: 0.0046 - dann loss: 2.0415 - reconstruction loss: 0.0373128/3811 - total loss: 0.0005 - classification loss: 0.0046 - dann loss: 2.0431 - reconstruction loss: 0.0373128/3811 - total loss: 0.0005 - classification loss: 0.0047 - dann loss: 2.0421 - reconstruction loss: 0.0372128/3811 - total loss: 0.0005 - classification loss: 0.0046 - dann loss: 2.0383 - reconstruction loss: 0.0371128/3811 - total loss: 0.0005 - classification loss: 0.0046 - dann loss: 2.0383 - reconstruction loss: 0.0372128/3811 - total loss: 0.0004 - classification loss: 0.0045 - dann loss: 2.0364 - reconstruction loss: 0.0372128/3811 - total loss: 0.0004 - classification loss: 0.0044 - dann loss: 2.0346 - reconstruction loss: 0.0372128/3811 - total loss: 0.0004 - classification loss: 0.0043 - dann loss: 2.0334 - reconstruction loss: 0.0371128/3811 - total loss: 0.0004 - classification loss: 0.0043 - dann loss: 2.0329 - reconstruction loss: 0.0371128/3811 - total loss: 0.0004 - classification loss: 0.0042 - dann loss: 2.0296 - reconstruction loss: 0.037198/3811 - total loss: 0.0005 - classification loss: 0.0045 - dann loss: 2.0327 - reconstruction loss: 0.03720/3811 - total loss: 0.0005 - classification loss: 0.0045 - dann loss: 2.0327 - reconstruction loss: 0.0372Epoch 45/150, Current strat Epoch 6/50
use_perm = False
switching perm
128/3811 - total loss: 0.0002 - classification loss: 0.0025 - dann loss: 1.9987 - reconstruction loss: 0.0371128/3811 - total loss: 0.0002 - classification loss: 0.0024 - dann loss: 2.0030 - reconstruction loss: 0.0375128/3811 - total loss: 0.0002 - classification loss: 0.0025 - dann loss: 2.0252 - reconstruction loss: 0.0378128/3811 - total loss: 0.0002 - classification loss: 0.0024 - dann loss: 2.0429 - reconstruction loss: 0.0381128/3811 - total loss: 0.0003 - classification loss: 0.0026 - dann loss: 2.0470 - reconstruction loss: 0.0380128/3811 - total loss: 0.0003 - classification loss: 0.0027 - dann loss: 2.0477 - reconstruction loss: 0.0378128/3811 - total loss: 0.0003 - classification loss: 0.0030 - dann loss: 2.0418 - reconstruction loss: 0.0377128/3811 - total loss: 0.0003 - classification loss: 0.0029 - dann loss: 2.0360 - reconstruction loss: 0.0377128/3811 - total loss: 0.0003 - classification loss: 0.0028 - dann loss: 2.0358 - reconstruction loss: 0.0375128/3811 - total loss: 0.0003 - classification loss: 0.0026 - dann loss: 2.0477 - reconstruction loss: 0.0375128/3811 - total loss: 0.0003 - classification loss: 0.0026 - dann loss: 2.0439 - reconstruction loss: 0.0373128/3811 - total loss: 0.0003 - classification loss: 0.0025 - dann loss: 2.0425 - reconstruction loss: 0.0373128/3811 - total loss: 0.0003 - classification loss: 0.0026 - dann loss: 2.0432 - reconstruction loss: 0.0374128/3811 - total loss: 0.0003 - classification loss: 0.0026 - dann loss: 2.0449 - reconstruction loss: 0.0375128/3811 - total loss: 0.0003 - classification loss: 0.0026 - dann loss: 2.0420 - reconstruction loss: 0.0374128/3811 - total loss: 0.0003 - classification loss: 0.0028 - dann loss: 2.0419 - reconstruction loss: 0.0373128/3811 - total loss: 0.0003 - classification loss: 0.0029 - dann loss: 2.0433 - reconstruction loss: 0.0373128/3811 - total loss: 0.0003 - classification loss: 0.0029 - dann loss: 2.0441 - reconstruction loss: 0.0373128/3811 - total loss: 0.0003 - classification loss: 0.0028 - dann loss: 2.0422 - reconstruction loss: 0.0372128/3811 - total loss: 0.0003 - classification loss: 0.0030 - dann loss: 2.0452 - reconstruction loss: 0.0372128/3811 - total loss: 0.0003 - classification loss: 0.0030 - dann loss: 2.0424 - reconstruction loss: 0.0372128/3811 - total loss: 0.0003 - classification loss: 0.0029 - dann loss: 2.0404 - reconstruction loss: 0.0372128/3811 - total loss: 0.0003 - classification loss: 0.0031 - dann loss: 2.0407 - reconstruction loss: 0.0373128/3811 - total loss: 0.0003 - classification loss: 0.0030 - dann loss: 2.0386 - reconstruction loss: 0.0373128/3811 - total loss: 0.0003 - classification loss: 0.0030 - dann loss: 2.0359 - reconstruction loss: 0.0373128/3811 - total loss: 0.0003 - classification loss: 0.0030 - dann loss: 2.0330 - reconstruction loss: 0.0373128/3811 - total loss: 0.0003 - classification loss: 0.0029 - dann loss: 2.0303 - reconstruction loss: 0.0373128/3811 - total loss: 0.0003 - classification loss: 0.0029 - dann loss: 2.0301 - reconstruction loss: 0.0372128/3811 - total loss: 0.0003 - classification loss: 0.0030 - dann loss: 2.0306 - reconstruction loss: 0.0372128/3742 - total loss: 0.2583 - classification loss: 0.0183 - dann loss: 2.2845 - reconstruction loss: 0.0350128/3742 - total loss: 0.2582 - classification loss: 0.0187 - dann loss: 2.2844 - reconstruction loss: 0.034929/3742 - total loss: 0.2581 - classification loss: 0.0191 - dann loss: 2.2823 - reconstruction loss: 0.03490/3742 - total loss: 0.2581 - classification loss: 0.0191 - dann loss: 2.2823 - reconstruction loss: 0.0349Epoch 20/150, Current strat Epoch 20/100
use_perm = True
switching perm
128/3742 - total loss: 0.2601 - classification loss: 0.0212 - dann loss: 2.3005 - reconstruction loss: 0.0349128/3742 - total loss: 0.2596 - classification loss: 0.0204 - dann loss: 2.2919 - reconstruction loss: 0.0354128/3742 - total loss: 0.2599 - classification loss: 0.0210 - dann loss: 2.2935 - reconstruction loss: 0.0355128/3742 - total loss: 0.2609 - classification loss: 0.0204 - dann loss: 2.3069 - reconstruction loss: 0.0353128/3742 - total loss: 0.2615 - classification loss: 0.0213 - dann loss: 2.3093 - reconstruction loss: 0.0356128/3742 - total loss: 0.2608 - classification loss: 0.0207 - dann loss: 2.3025 - reconstruction loss: 0.0356128/3742 - total loss: 0.2602 - classification loss: 0.0220 - dann loss: 2.2962 - reconstruction loss: 0.0355128/3742 - total loss: 0.2596 - classification loss: 0.0216 - dann loss: 2.2896 - reconstruction loss: 0.0356128/3742 - total loss: 0.2597 - classification loss: 0.0211 - dann loss: 2.2927 - reconstruction loss: 0.0354128/3742 - total loss: 0.2594 - classification loss: 0.0212 - dann loss: 2.2889 - reconstruction loss: 0.0354128/3742 - total loss: 0.2589 - classification loss: 0.0212 - dann loss: 2.2849 - reconstruction loss: 0.0354128/3742 - total loss: 0.2590 - classification loss: 0.0203 - dann loss: 2.2872 - reconstruction loss: 0.0353128/3742 - total loss: 0.2590 - classification loss: 0.0206 - dann loss: 2.2878 - reconstruction loss: 0.0352128/3742 - total loss: 0.2588 - classification loss: 0.0203 - dann loss: 2.2853 - reconstruction loss: 0.0352128/3742 - total loss: 0.2586 - classification loss: 0.0207 - dann loss: 2.2839 - reconstruction loss: 0.0352128/3742 - total loss: 0.2587 - classification loss: 0.0209 - dann loss: 2.2848 - reconstruction loss: 0.0352128/3742 - total loss: 0.2586 - classification loss: 0.0207 - dann loss: 2.2837 - reconstruction loss: 0.0352128/3742 - total loss: 0.2587 - classification loss: 0.0204 - dann loss: 2.2840 - reconstruction loss: 0.0353128/3742 - total loss: 0.2583 - classification loss: 0.0200 - dann loss: 2.2806 - reconstruction loss: 0.0353128/3742 - total loss: 0.2584 - classification loss: 0.0201 - dann loss: 2.2831 - reconstruction loss: 0.0351128/3742 - total loss: 0.2581 - classification loss: 0.0204 - dann loss: 2.2805 - reconstruction loss: 0.0350128/3742 - total loss: 0.2584 - classification loss: 0.0203 - dann loss: 2.2837 - reconstruction loss: 0.0350128/3742 - total loss: 0.2580 - classification loss: 0.0200 - dann loss: 2.2806 - reconstruction loss: 0.0349128/3742 - total loss: 0.2580 - classification loss: 0.0203 - dann loss: 2.2803 - reconstruction loss: 0.0349128/3742 - total loss: 0.2579 - classification loss: 0.0200 - dann loss: 2.2800 - reconstruction loss: 0.0349128/3742 - total loss: 0.2580 - classification loss: 0.0198 - dann loss: 2.2808 - reconstruction loss: 0.0350128/3742 - total loss: 0.2578 - classification loss: 0.0207 - dann loss: 2.2775 - reconstruction loss: 0.0349128/3742 - total loss: 0.2577 - classification loss: 0.0205 - dann loss: 2.2772 - reconstruction loss: 0.0349128/3742 - total loss: 0.2575 - classification loss: 0.0207 - dann loss: 2.2753 - reconstruction loss: 0.034929/3742 - total loss: 0.2579 - classification loss: 0.0211 - dann loss: 2.2785 - reconstruction loss: 0.03490/3742 - total loss: 0.2579 - classification loss: 0.0211 - dann loss: 2.2785 - reconstruction loss: 0.0349Epoch 21/150, Current strat Epoch 21/100
use_perm = True
switching perm
128/3742 - total loss: 0.2558 - classification loss: 0.0131 - dann loss: 2.2622 - reconstruction loss: 0.0354128/3742 - total loss: 0.2598 - classification loss: 0.0137 - dann loss: 2.3070 - reconstruction loss: 0.0346128/3742 - total loss: 0.2575 - classification loss: 0.0180 - dann loss: 2.2801 - reconstruction loss: 0.0346128/3742 - total loss: 0.2564 - classification loss: 0.0164 - dann loss: 2.2724 - reconstruction loss: 0.0344128/3742 - total loss: 0.2573 - classification loss: 0.0160 - dann loss: 2.2834 - reconstruction loss: 0.0342128/3742 - total loss: 0.2566 - classification loss: 0.0148 - dann loss: 2.2784 - reconstruction loss: 0.0341128/3742 - total loss: 0.2571 - classification loss: 0.0141 - dann loss: 2.2837 - reconstruction loss: 0.0341128/3742 - total loss: 0.2573 - classification loss: 0.0134 - dann loss: 2.2839 - reconstruction loss: 0.0345128/3742 - total loss: 0.2582 - classification loss: 0.0138 - dann loss: 2.2923 - reconstruction loss: 0.0345128/3742 - total loss: 0.2587 - classification loss: 0.0138 - dann loss: 2.2951 - reconstruction loss: 0.0347128/3742 - total loss: 0.2584 - classification loss: 0.0164 - dann loss: 2.2889 - reconstruction loss: 0.0348128/3742 - total loss: 0.2587 - classification loss: 0.0168 - dann loss: 2.2917 - reconstruction loss: 0.0348128/3742 - total loss: 0.2585 - classification loss: 0.0166 - dann loss: 2.2906 - reconstruction loss: 0.0348128/3742 - total loss: 0.2581 - classification loss: 0.0168 - dann loss: 2.2875 - reconstruction loss: 0.0346128/3742 - total loss: 0.2584 - classification loss: 0.0169 - dann loss: 2.2909 - reconstruction loss: 0.0345128/3742 - total loss: 0.2586 - classification loss: 0.0171 - dann loss: 2.2915 - reconstruction loss: 0.0346128/3742 - total loss: 0.2587 - classification loss: 0.0170 - dann loss: 2.2925 - reconstruction loss: 0.0347128/3742 - total loss: 0.2581 - classification loss: 0.0170 - dann loss: 2.2867 - reconstruction loss: 0.0347128/3742 - total loss: 0.2582 - classification loss: 0.0172 - dann loss: 2.2870 - reconstruction loss: 0.0347128/3742 - total loss: 0.2581 - classification loss: 0.0169 - dann loss: 2.2864 - reconstruction loss: 0.0347128/3742 - total loss: 0.2579 - classification loss: 0.0168 - dann loss: 2.2845 - reconstruction loss: 0.0348128/3742 - total loss: 0.2578 - classification loss: 0.0172 - dann loss: 2.2822 - reconstruction loss: 0.0348128/3742 - total loss: 0.2575 - classification loss: 0.0175 - dann loss: 2.2802 - reconstruction loss: 0.0347128/3742 - total loss: 0.2575 - classification loss: 0.0172 - dann loss: 2.2807 - reconstruction loss: 0.0346128/3742 - total loss: 0.2573 - classification loss: 0.0172 - dann loss: 2.2784 - reconstruction loss: 0.0347128/3742 - total loss: 0.2570 - classification loss: 0.0170 - dann loss: 2.2762 - reconstruction loss: 0.0346128/3742 - total loss: 0.2569 - classification loss: 0.0170 - dann loss: 2.2743 - reconstruction loss: 0.0347128/3742 - total loss: 0.2568 - classification loss: 0.0170 - dann loss: 2.2733 - reconstruction loss: 0.0347128/3742 - total loss: 0.2569 - classification loss: 0.0168 - dann loss: 2.2743 - reconstruction loss: 0.034829/3742 - total loss: 0.2569 - classification loss: 0.0171 - dann loss: 2.2730 - reconstruction loss: 0.03480/3742 - total loss: 0.2569 - classification loss: 0.0171 - dann loss: 2.2730 - reconstruction loss: 0.0348Epoch 22/150, Current strat Epoch 22/100
use_perm = True
switching perm
128/3742 - total loss: 0.2564 - classification loss: 0.0187 - dann loss: 2.2689 - reconstruction loss: 0.0345128/3742 - total loss: 0.2576 - classification loss: 0.0165 - dann loss: 2.2767 - reconstruction loss: 0.0354128/3742 - total loss: 0.2554 - classification loss: 0.0151 - dann loss: 2.2597 - reconstruction loss: 0.0349128/3742 - total loss: 0.2563 - classification loss: 0.0144 - dann loss: 2.2702 - reconstruction loss: 0.0348128/3742 - total loss: 0.2579 - classification loss: 0.0135 - dann loss: 2.2865 - reconstruction loss: 0.0349128/3742 - total loss: 0.2586 - classification loss: 0.0135 - dann loss: 2.2959 - reconstruction loss: 0.0345128/3742 - total loss: 0.2582 - classification loss: 0.0137 - dann loss: 2.2929 - reconstruction loss: 0.034498/3811 - total loss: 0.0003 - classification loss: 0.0031 - dann loss: 2.0324 - reconstruction loss: 0.03710/3811 - total loss: 0.0003 - classification loss: 0.0031 - dann loss: 2.0324 - reconstruction loss: 0.0371Epoch 46/150, Current strat Epoch 7/50
use_perm = False
switching perm
128/3811 - total loss: 0.0001 - classification loss: 0.0012 - dann loss: 2.0759 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0012 - dann loss: 2.0789 - reconstruction loss: 0.0362128/3811 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.0729 - reconstruction loss: 0.0364128/3811 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.0618 - reconstruction loss: 0.0366128/3811 - total loss: 0.0002 - classification loss: 0.0019 - dann loss: 2.0410 - reconstruction loss: 0.0361128/3811 - total loss: 0.0002 - classification loss: 0.0019 - dann loss: 2.0313 - reconstruction loss: 0.0359128/3811 - total loss: 0.0002 - classification loss: 0.0018 - dann loss: 2.0279 - reconstruction loss: 0.0361128/3811 - total loss: 0.0002 - classification loss: 0.0019 - dann loss: 2.0311 - reconstruction loss: 0.0361128/3811 - total loss: 0.0002 - classification loss: 0.0021 - dann loss: 2.0321 - reconstruction loss: 0.0362128/3811 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.0310 - reconstruction loss: 0.0362128/3811 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.0291 - reconstruction loss: 0.0362128/3811 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.0324 - reconstruction loss: 0.0364128/3811 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.0303 - reconstruction loss: 0.0364128/3811 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.0340 - reconstruction loss: 0.0365128/3811 - total loss: 0.0002 - classification loss: 0.0021 - dann loss: 2.0339 - reconstruction loss: 0.0365128/3811 - total loss: 0.0002 - classification loss: 0.0021 - dann loss: 2.0329 - reconstruction loss: 0.0365128/3811 - total loss: 0.0002 - classification loss: 0.0021 - dann loss: 2.0337 - reconstruction loss: 0.0365128/3811 - total loss: 0.0002 - classification loss: 0.0022 - dann loss: 2.0371 - reconstruction loss: 0.0365128/3811 - total loss: 0.0002 - classification loss: 0.0021 - dann loss: 2.0312 - reconstruction loss: 0.0366128/3811 - total loss: 0.0002 - classification loss: 0.0022 - dann loss: 2.0323 - reconstruction loss: 0.0367128/3811 - total loss: 0.0002 - classification loss: 0.0022 - dann loss: 2.0337 - reconstruction loss: 0.0367128/3811 - total loss: 0.0002 - classification loss: 0.0024 - dann loss: 2.0329 - reconstruction loss: 0.0368128/3811 - total loss: 0.0002 - classification loss: 0.0025 - dann loss: 2.0326 - reconstruction loss: 0.0369128/3811 - total loss: 0.0002 - classification loss: 0.0024 - dann loss: 2.0280 - reconstruction loss: 0.0369128/3811 - total loss: 0.0002 - classification loss: 0.0025 - dann loss: 2.0295 - reconstruction loss: 0.0369128/3811 - total loss: 0.0002 - classification loss: 0.0025 - dann loss: 2.0309 - reconstruction loss: 0.0370128/3811 - total loss: 0.0002 - classification loss: 0.0024 - dann loss: 2.0280 - reconstruction loss: 0.0371128/3811 - total loss: 0.0002 - classification loss: 0.0024 - dann loss: 2.0313 - reconstruction loss: 0.0370128/3811 - total loss: 0.0002 - classification loss: 0.0024 - dann loss: 2.0306 - reconstruction loss: 0.037198/3811 - total loss: 0.0002 - classification loss: 0.0025 - dann loss: 2.0324 - reconstruction loss: 0.03720/3811 - total loss: 0.0002 - classification loss: 0.0025 - dann loss: 2.0324 - reconstruction loss: 0.0372Epoch 47/150, Current strat Epoch 8/50
use_perm = False
switching perm
128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 1.9737 - reconstruction loss: 0.0361128/3811 - total loss: 0.0001 - classification loss: 0.0015 - dann loss: 1.9856 - reconstruction loss: 0.0365128/3811 - total loss: 0.0002 - classification loss: 0.0019 - dann loss: 1.9887 - reconstruction loss: 0.0363128/3811 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 1.9842 - reconstruction loss: 0.0367128/3811 - total loss: 0.0002 - classification loss: 0.0018 - dann loss: 1.9950 - reconstruction loss: 0.0367128/3811 - total loss: 0.0002 - classification loss: 0.0018 - dann loss: 1.9845 - reconstruction loss: 0.0370128/3811 - total loss: 0.0002 - classification loss: 0.0021 - dann loss: 1.9930 - reconstruction loss: 0.0370128/3811 - total loss: 0.0002 - classification loss: 0.0021 - dann loss: 1.9976 - reconstruction loss: 0.0370128/3811 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 1.9975 - reconstruction loss: 0.0369128/3811 - total loss: 0.0002 - classification loss: 0.0023 - dann loss: 2.0005 - reconstruction loss: 0.0370128/3811 - total loss: 0.0002 - classification loss: 0.0023 - dann loss: 2.0055 - reconstruction loss: 0.0372128/3811 - total loss: 0.0002 - classification loss: 0.0022 - dann loss: 2.0148 - reconstruction loss: 0.0373128/3811 - total loss: 0.0002 - classification loss: 0.0023 - dann loss: 2.0130 - reconstruction loss: 0.0373128/3811 - total loss: 0.0002 - classification loss: 0.0023 - dann loss: 2.0123 - reconstruction loss: 0.0374128/3811 - total loss: 0.0002 - classification loss: 0.0023 - dann loss: 2.0111 - reconstruction loss: 0.0375128/3811 - total loss: 0.0002 - classification loss: 0.0023 - dann loss: 2.0164 - reconstruction loss: 0.0375128/3811 - total loss: 0.0002 - classification loss: 0.0024 - dann loss: 2.0245 - reconstruction loss: 0.0375128/3811 - total loss: 0.0002 - classification loss: 0.0024 - dann loss: 2.0241 - reconstruction loss: 0.0375128/3811 - total loss: 0.0002 - classification loss: 0.0023 - dann loss: 2.0358 - reconstruction loss: 0.0374128/3811 - total loss: 0.0002 - classification loss: 0.0023 - dann loss: 2.0382 - reconstruction loss: 0.0374128/3811 - total loss: 0.0002 - classification loss: 0.0023 - dann loss: 2.0424 - reconstruction loss: 0.0374128/3811 - total loss: 0.0002 - classification loss: 0.0022 - dann loss: 2.0406 - reconstruction loss: 0.0374128/3811 - total loss: 0.0002 - classification loss: 0.0023 - dann loss: 2.0411 - reconstruction loss: 0.0374128/3811 - total loss: 0.0002 - classification loss: 0.0022 - dann loss: 2.0389 - reconstruction loss: 0.0373128/3811 - total loss: 0.0002 - classification loss: 0.0022 - dann loss: 2.0350 - reconstruction loss: 0.0373128/3811 - total loss: 0.0002 - classification loss: 0.0022 - dann loss: 2.0343 - reconstruction loss: 0.0373128/3811 - total loss: 0.0002 - classification loss: 0.0021 - dann loss: 2.0335 - reconstruction loss: 0.0372128/3811 - total loss: 0.0002 - classification loss: 0.0021 - dann loss: 2.0352 - reconstruction loss: 0.0372128/3811 - total loss: 0.0002 - classification loss: 0.0021 - dann loss: 2.0331 - reconstruction loss: 0.037298/3811 - total loss: 0.0002 - classification loss: 0.0021 - dann loss: 2.0317 - reconstruction loss: 0.03720/3811 - total loss: 0.0002 - classification loss: 0.0021 - dann loss: 2.0317 - reconstruction loss: 0.0372Epoch 48/150, Current strat Epoch 9/50
use_perm = False
switching perm
128/3811 - total loss: 0.0001 - classification loss: 0.0015 - dann loss: 2.0324 - reconstruction loss: 0.0361128/3811 - total loss: 0.0002 - classification loss: 0.0023 - dann loss: 2.0754 - reconstruction loss: 0.0367128/3811 - total loss: 0.0003 - classification loss: 0.0032 - dann loss: 2.0636 - reconstruction loss: 0.0372128/3811 - total loss: 0.0003 - classification loss: 0.0030 - dann loss: 2.0592 - reconstruction loss: 0.0371128/3811 - total loss: 0.0003 - classification loss: 0.0027 - dann loss: 2.0611 - reconstruction loss: 0.0372128/3811 - total loss: 0.0003 - classification loss: 0.0027 - dann loss: 2.0600 - reconstruction loss: 0.0371128/3811 - total loss: 0.0003 - classification loss: 0.0026 - dann loss: 2.0608 - reconstruction loss: 0.0370128/3811 - total loss: 0.0002 - classification loss: 0.0025 - dann loss: 2.0592 - reconstruction loss: 0.0368128/3811 - total loss: 0.0002 - classification loss: 0.0023 - dann loss: 2.0533 - reconstruction loss: 0.0370128/3811 - total loss: 0.0002 - classification loss: 0.0023 - dann loss: 2.0523 - reconstruction loss: 0.0371128/3811 - total loss: 0.0002 - classification loss: 0.0023 - dann loss: 2.0601 - reconstruction loss: 0.0372128/3811 - total loss: 0.0002 - classification loss: 0.0023 - dann loss: 2.0575 - reconstruction loss: 0.0371128/3811 - total loss: 0.0002 - classification loss: 0.0022 - dann loss: 2.0568 - reconstruction loss: 0.0371128/3811 - total loss: 0.0002 - classification loss: 0.0022 - dann loss: 2.0496 - reconstruction loss: 0.0372128/3811 - total loss: 0.0002 - classification loss: 0.0021 - dann loss: 2.0494 - reconstruction loss: 0.0371128/3811 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.0463 - reconstruction loss: 0.0371128/3811 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.0431 - reconstruction loss: 0.0370128/3811 - total loss: 0.0002 - classification loss: 0.0019 - dann loss: 2.0411 - reconstruction loss: 0.0371128/3811 - total loss: 0.0002 - classification loss: 0.0019 - dann loss: 2.0379 - reconstruction loss: 0.0372128/3811 - total loss: 0.0002 - classification loss: 0.0019 - dann loss: 2.0370 - reconstruction loss: 0.0372128/3811 - total loss: 0.0002 - classification loss: 0.0019 - dann loss: 2.0359 - reconstruction loss: 0.0373128/3811 - total loss: 0.0002 - classification loss: 0.0018 - dann loss: 2.0320 - reconstruction loss: 0.0372128/3811 - total loss: 0.0002 - classification loss: 0.0018 - dann loss: 2.0313 - reconstruction loss: 0.0373128/3811 - total loss: 0.0002 - classification loss: 0.0018 - dann loss: 2.0300 - reconstruction loss: 0.0372128/3811 - total loss: 0.0002 - classification loss: 0.0018 - dann loss: 2.0277 - reconstruction loss: 0.0372128/3811 - total loss: 0.0002 - classification loss: 0.0017 - dann loss: 2.0279 - reconstruction loss: 0.0372128/3811 - total loss: 0.0002 - classification loss: 0.0018 - dann loss: 2.0305 - reconstruction loss: 0.0371128/3811 - total loss: 0.0002 - classification loss: 0.0018 - dann loss: 2.0293 - reconstruction loss: 0.0371128/3811 - total loss: 0.0002 - classification loss: 0.0018 - dann loss: 2.0301 - reconstruction loss: 0.037198/3811 - total loss: 0.0002 - classification loss: 0.0018 - dann loss: 2.0326 - reconstruction loss: 0.03720/3811 - total loss: 0.0002 - classification loss: 0.0018 - dann loss: 2.0326 - reconstruction loss: 0.0372Epoch 49/150, Current strat Epoch 10/50
use_perm = False
switching perm
128/3811 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.0623 - reconstruction loss: 0.0345128/3811 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.0944 - reconstruction loss: 0.0353128/3811 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.0888 - reconstruction loss: 0.0355128/3811 - total loss: 0.0001 - classification loss: 0.0015 - dann loss: 2.0868 - reconstruction loss: 0.0359128/3811 - total loss: 0.0002 - classification loss: 0.0017 - dann loss: 2.0792 - reconstruction loss: 0.0364128/3811 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.0600 - reconstruction loss: 0.0368128/3811 - total loss: 0.0001 - classification loss: 0.0015 - dann loss: 2.0617 - reconstruction loss: 0.0369128/3811 - total loss: 0.0001 - classification loss: 0.0015 - dann loss: 2.0531 - reconstruction loss: 0.0369128/3811 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.0550 - reconstruction loss: 0.0368128/3811 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.0534 - reconstruction loss: 0.0366128/3811 - total loss: 0.0001 - classification loss: 0.0013 - dann loss: 2.0470 - reconstruction loss: 0.0365128/3811 - total loss: 0.0001 - classification loss: 0.0013 - dann loss: 2.0431 - reconstruction loss: 0.0366128/3811 - total loss: 0.0002 - classification loss: 0.0017 - dann loss: 2.0415 - reconstruction loss: 0.0367128/3811 - total loss: 0.0002 - classification loss: 0.0017 - dann loss: 2.0434 - reconstruction loss: 0.0367128/3811 - total loss: 0.0002 - classification loss: 0.0018 - dann loss: 2.0420 - reconstruction loss: 0.0369128/3811 - total loss: 0.0002 - classification loss: 0.0017 - dann loss: 2.0366 - reconstruction loss: 0.0368128/3811 - total loss: 0.0002 - classification loss: 0.0018 - dann loss: 2.0380 - reconstruction loss: 0.0368128/3811 - total loss: 0.0002 - classification loss: 0.0017 - dann loss: 2.0342 - reconstruction loss: 0.0367128/3811 - total loss: 0.0002 - classification loss: 0.0017 - dann loss: 2.0359 - reconstruction loss: 0.0369128/3811 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.0377 - reconstruction loss: 0.0370128/3811 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.0367 - reconstruction loss: 0.0370128/3811 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.0357 - reconstruction loss: 0.0370128/3811 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.0368 - reconstruction loss: 0.0370128/3811 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.0346 - reconstruction loss: 0.0371128/3811 - total loss: 0.0002 - classification loss: 0.0015 - dann loss: 2.0341 - reconstruction loss: 0.0371128/3811 - total loss: 0.0002 - classification loss: 0.0015 - dann loss: 2.0314 - reconstruction loss: 0.0371128/3811 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.0292 - reconstruction loss: 0.0371128/3811 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.0286 - reconstruction loss: 0.0372128/3811 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.0321 - reconstruction loss: 0.037198/3811 - total loss: 0.0002 - classification loss: 0.0015 - dann loss: 2.0320 - reconstruction loss: 0.03720/3811 - total loss: 0.0002 - classification loss: 0.0015 - dann loss: 2.0320 - reconstruction loss: 0.0372Epoch 50/150, Current strat Epoch 11/50
use_perm = False
switching perm
128/3811 - total loss: 0.0001 - classification loss: 0.0010 - dann loss: 2.0235 - reconstruction loss: 0.0387128/3811 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 1.9868 - reconstruction loss: 0.0377128/3811 - total loss: 0.0001 - classification loss: 0.0012 - dann loss: 1.9849 - reconstruction loss: 0.0378128/3811 - total loss: 0.0001 - classification loss: 0.0012 - dann loss: 1.9856 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0052 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0175 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0272 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0012 - dann loss: 2.0263 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0013 - dann loss: 2.0244 - reconstruction loss: 0.0375128/3811 - total loss: 0.0001 - classification loss: 0.0015 - dann loss: 2.0280 - reconstruction loss: 0.0375128/3811 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.0178 - reconstruction loss: 0.0375128/3811 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.0147 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.0168 - reconstruction loss: 0.0373128/3811 - total loss: 0.0002 - classification loss: 0.0015 - dann loss: 2.0236 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0015 - dann loss: 2.0211 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0015 - dann loss: 2.0281 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.0263 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.0352 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.0342 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.0376 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0013 - dann loss: 2.0349 - reconstruction loss: 0.0371128/3742 - total loss: 0.2572 - classification loss: 0.0131 - dann loss: 2.2838 - reconstruction loss: 0.0344128/3742 - total loss: 0.2575 - classification loss: 0.0134 - dann loss: 2.2853 - reconstruction loss: 0.0345128/3742 - total loss: 0.2569 - classification loss: 0.0134 - dann loss: 2.2777 - reconstruction loss: 0.0347128/3742 - total loss: 0.2573 - classification loss: 0.0133 - dann loss: 2.2816 - reconstruction loss: 0.0347128/3742 - total loss: 0.2575 - classification loss: 0.0135 - dann loss: 2.2830 - reconstruction loss: 0.0348128/3742 - total loss: 0.2569 - classification loss: 0.0135 - dann loss: 2.2765 - reconstruction loss: 0.0349128/3742 - total loss: 0.2566 - classification loss: 0.0135 - dann loss: 2.2733 - reconstruction loss: 0.0349128/3742 - total loss: 0.2568 - classification loss: 0.0133 - dann loss: 2.2735 - reconstruction loss: 0.0351128/3742 - total loss: 0.2568 - classification loss: 0.0137 - dann loss: 2.2744 - reconstruction loss: 0.0350128/3742 - total loss: 0.2571 - classification loss: 0.0137 - dann loss: 2.2779 - reconstruction loss: 0.0349128/3742 - total loss: 0.2570 - classification loss: 0.0136 - dann loss: 2.2771 - reconstruction loss: 0.0349128/3742 - total loss: 0.2569 - classification loss: 0.0133 - dann loss: 2.2767 - reconstruction loss: 0.0349128/3742 - total loss: 0.2569 - classification loss: 0.0135 - dann loss: 2.2768 - reconstruction loss: 0.0349128/3742 - total loss: 0.2567 - classification loss: 0.0133 - dann loss: 2.2748 - reconstruction loss: 0.0348128/3742 - total loss: 0.2566 - classification loss: 0.0133 - dann loss: 2.2739 - reconstruction loss: 0.0349128/3742 - total loss: 0.2568 - classification loss: 0.0135 - dann loss: 2.2756 - reconstruction loss: 0.0349128/3742 - total loss: 0.2563 - classification loss: 0.0133 - dann loss: 2.2715 - reconstruction loss: 0.0348128/3742 - total loss: 0.2562 - classification loss: 0.0136 - dann loss: 2.2698 - reconstruction loss: 0.0349128/3742 - total loss: 0.2563 - classification loss: 0.0135 - dann loss: 2.2702 - reconstruction loss: 0.0349128/3742 - total loss: 0.2562 - classification loss: 0.0136 - dann loss: 2.2704 - reconstruction loss: 0.0348128/3742 - total loss: 0.2563 - classification loss: 0.0136 - dann loss: 2.2713 - reconstruction loss: 0.0347128/3742 - total loss: 0.2561 - classification loss: 0.0137 - dann loss: 2.2693 - reconstruction loss: 0.034729/3742 - total loss: 0.2562 - classification loss: 0.0137 - dann loss: 2.2686 - reconstruction loss: 0.03490/3742 - total loss: 0.2562 - classification loss: 0.0137 - dann loss: 2.2686 - reconstruction loss: 0.0349Epoch 23/150, Current strat Epoch 23/100
use_perm = True
switching perm
128/3742 - total loss: 0.2580 - classification loss: 0.0122 - dann loss: 2.2908 - reconstruction loss: 0.0346128/3742 - total loss: 0.2525 - classification loss: 0.0105 - dann loss: 2.2379 - reconstruction loss: 0.0345128/3742 - total loss: 0.2540 - classification loss: 0.0097 - dann loss: 2.2546 - reconstruction loss: 0.0344128/3742 - total loss: 0.2567 - classification loss: 0.0092 - dann loss: 2.2812 - reconstruction loss: 0.0346128/3742 - total loss: 0.2559 - classification loss: 0.0095 - dann loss: 2.2718 - reconstruction loss: 0.0347128/3742 - total loss: 0.2565 - classification loss: 0.0103 - dann loss: 2.2755 - reconstruction loss: 0.0349128/3742 - total loss: 0.2555 - classification loss: 0.0104 - dann loss: 2.2680 - reconstruction loss: 0.0346128/3742 - total loss: 0.2561 - classification loss: 0.0108 - dann loss: 2.2722 - reconstruction loss: 0.0347128/3742 - total loss: 0.2560 - classification loss: 0.0112 - dann loss: 2.2710 - reconstruction loss: 0.0347128/3742 - total loss: 0.2560 - classification loss: 0.0110 - dann loss: 2.2730 - reconstruction loss: 0.0345128/3742 - total loss: 0.2559 - classification loss: 0.0121 - dann loss: 2.2702 - reconstruction loss: 0.0346128/3742 - total loss: 0.2570 - classification loss: 0.0125 - dann loss: 2.2809 - reconstruction loss: 0.0346128/3742 - total loss: 0.2568 - classification loss: 0.0125 - dann loss: 2.2774 - reconstruction loss: 0.0347128/3742 - total loss: 0.2563 - classification loss: 0.0123 - dann loss: 2.2722 - reconstruction loss: 0.0348128/3742 - total loss: 0.2565 - classification loss: 0.0122 - dann loss: 2.2740 - reconstruction loss: 0.0348128/3742 - total loss: 0.2564 - classification loss: 0.0119 - dann loss: 2.2738 - reconstruction loss: 0.0348128/3742 - total loss: 0.2563 - classification loss: 0.0118 - dann loss: 2.2734 - reconstruction loss: 0.0347128/3742 - total loss: 0.2561 - classification loss: 0.0121 - dann loss: 2.2705 - reconstruction loss: 0.0348128/3742 - total loss: 0.2559 - classification loss: 0.0119 - dann loss: 2.2678 - reconstruction loss: 0.0349128/3742 - total loss: 0.2556 - classification loss: 0.0117 - dann loss: 2.2658 - reconstruction loss: 0.0348128/3742 - total loss: 0.2558 - classification loss: 0.0118 - dann loss: 2.2675 - reconstruction loss: 0.0348128/3742 - total loss: 0.2557 - classification loss: 0.0118 - dann loss: 2.2666 - reconstruction loss: 0.0348128/3742 - total loss: 0.2552 - classification loss: 0.0117 - dann loss: 2.2621 - reconstruction loss: 0.0348128/3742 - total loss: 0.2554 - classification loss: 0.0116 - dann loss: 2.2636 - reconstruction loss: 0.0349128/3742 - total loss: 0.2553 - classification loss: 0.0114 - dann loss: 2.2635 - reconstruction loss: 0.0348128/3742 - total loss: 0.2554 - classification loss: 0.0118 - dann loss: 2.2637 - reconstruction loss: 0.0348128/3742 - total loss: 0.2554 - classification loss: 0.0118 - dann loss: 2.2636 - reconstruction loss: 0.0348128/3742 - total loss: 0.2553 - classification loss: 0.0118 - dann loss: 2.2633 - reconstruction loss: 0.0348128/3742 - total loss: 0.2554 - classification loss: 0.0118 - dann loss: 2.2641 - reconstruction loss: 0.034729/3742 - total loss: 0.2549 - classification loss: 0.0120 - dann loss: 2.2596 - reconstruction loss: 0.03470/3742 - total loss: 0.2549 - classification loss: 0.0120 - dann loss: 2.2596 - reconstruction loss: 0.0347Epoch 24/150, Current strat Epoch 24/100
use_perm = True
switching perm
128/3742 - total loss: 0.2560 - classification loss: 0.0144 - dann loss: 2.2718 - reconstruction loss: 0.0343128/3742 - total loss: 0.2556 - classification loss: 0.0121 - dann loss: 2.2756 - reconstruction loss: 0.0336128/3742 - total loss: 0.2563 - classification loss: 0.0184 - dann loss: 2.2733 - reconstruction loss: 0.0339128/3742 - total loss: 0.2575 - classification loss: 0.0193 - dann loss: 2.2827 - reconstruction loss: 0.0341128/3742 - total loss: 0.2562 - classification loss: 0.0171 - dann loss: 2.2722 - reconstruction loss: 0.0341128/3742 - total loss: 0.2563 - classification loss: 0.0158 - dann loss: 2.2741 - reconstruction loss: 0.0341128/3742 - total loss: 0.2569 - classification loss: 0.0150 - dann loss: 2.2820 - reconstruction loss: 0.0341128/3742 - total loss: 0.2564 - classification loss: 0.0142 - dann loss: 2.2770 - reconstruction loss: 0.0341128/3742 - total loss: 0.2566 - classification loss: 0.0136 - dann loss: 2.2798 - reconstruction loss: 0.0341128/3742 - total loss: 0.2554 - classification loss: 0.0130 - dann loss: 2.2676 - reconstruction loss: 0.0341128/3742 - total loss: 0.2557 - classification loss: 0.0124 - dann loss: 2.2704 - reconstruction loss: 0.0343128/3742 - total loss: 0.2552 - classification loss: 0.0121 - dann loss: 2.2669 - reconstruction loss: 0.0342128/3742 - total loss: 0.2552 - classification loss: 0.0121 - dann loss: 2.2663 - reconstruction loss: 0.0342128/3742 - total loss: 0.2553 - classification loss: 0.0119 - dann loss: 2.2664 - reconstruction loss: 0.0343128/3742 - total loss: 0.2550 - classification loss: 0.0116 - dann loss: 2.2630 - reconstruction loss: 0.0344128/3742 - total loss: 0.2545 - classification loss: 0.0118 - dann loss: 2.2582 - reconstruction loss: 0.0344128/3742 - total loss: 0.2541 - classification loss: 0.0117 - dann loss: 2.2544 - reconstruction loss: 0.0343128/3742 - total loss: 0.2537 - classification loss: 0.0114 - dann loss: 2.2507 - reconstruction loss: 0.0344128/3742 - total loss: 0.2538 - classification loss: 0.0115 - dann loss: 2.2513 - reconstruction loss: 0.0345128/3811 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.0348 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.0365 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.0372 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0013 - dann loss: 2.0345 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0013 - dann loss: 2.0321 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0013 - dann loss: 2.0288 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0013 - dann loss: 2.0329 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0013 - dann loss: 2.0316 - reconstruction loss: 0.037298/3811 - total loss: 0.0001 - classification loss: 0.0013 - dann loss: 2.0321 - reconstruction loss: 0.03720/3811 - total loss: 0.0001 - classification loss: 0.0013 - dann loss: 2.0321 - reconstruction loss: 0.0372Epoch 51/150, Current strat Epoch 12/50
use_perm = False
switching perm
128/3811 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.0340 - reconstruction loss: 0.0358128/3811 - total loss: 0.0001 - classification loss: 0.0013 - dann loss: 2.0055 - reconstruction loss: 0.0368128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0554 - reconstruction loss: 0.0376128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0750 - reconstruction loss: 0.0378128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0493 - reconstruction loss: 0.0376128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0571 - reconstruction loss: 0.0375128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0542 - reconstruction loss: 0.0378128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0505 - reconstruction loss: 0.0376128/3811 - total loss: 0.0001 - classification loss: 0.0012 - dann loss: 2.0557 - reconstruction loss: 0.0375128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0480 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0450 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0400 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0435 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0409 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0430 - reconstruction loss: 0.0375128/3811 - total loss: 0.0001 - classification loss: 0.0012 - dann loss: 2.0414 - reconstruction loss: 0.0375128/3811 - total loss: 0.0001 - classification loss: 0.0012 - dann loss: 2.0385 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0391 - reconstruction loss: 0.0375128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0424 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0418 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0012 - dann loss: 2.0409 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0012 - dann loss: 2.0380 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0012 - dann loss: 2.0390 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0364 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0352 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0315 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0318 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0302 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0308 - reconstruction loss: 0.037298/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0324 - reconstruction loss: 0.03720/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0324 - reconstruction loss: 0.0372Epoch 52/150, Current strat Epoch 13/50
use_perm = False
switching perm
128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 1.9759 - reconstruction loss: 0.0375128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 1.9858 - reconstruction loss: 0.0379128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 1.9883 - reconstruction loss: 0.0377128/3811 - total loss: 0.0001 - classification loss: 0.0010 - dann loss: 1.9844 - reconstruction loss: 0.0379128/3811 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 1.9943 - reconstruction loss: 0.0376128/3811 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 1.9939 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 1.9995 - reconstruction loss: 0.0375128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0121 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.0170 - reconstruction loss: 0.0377128/3811 - total loss: 0.0001 - classification loss: 0.0010 - dann loss: 2.0191 - reconstruction loss: 0.0377128/3811 - total loss: 0.0001 - classification loss: 0.0010 - dann loss: 2.0163 - reconstruction loss: 0.0376128/3811 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.0182 - reconstruction loss: 0.0377128/3811 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.0283 - reconstruction loss: 0.0376128/3811 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.0325 - reconstruction loss: 0.0377128/3811 - total loss: 0.0001 - classification loss: 0.0010 - dann loss: 2.0364 - reconstruction loss: 0.0376128/3811 - total loss: 0.0001 - classification loss: 0.0010 - dann loss: 2.0442 - reconstruction loss: 0.0375128/3811 - total loss: 0.0001 - classification loss: 0.0010 - dann loss: 2.0425 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0010 - dann loss: 2.0460 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0010 - dann loss: 2.0453 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0418 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0010 - dann loss: 2.0412 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0389 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0010 - dann loss: 2.0434 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0381 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0379 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0353 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0012 - dann loss: 2.0320 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0320 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0317 - reconstruction loss: 0.037298/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0321 - reconstruction loss: 0.03720/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0321 - reconstruction loss: 0.0372Epoch 53/150, Current strat Epoch 14/50
use_perm = False
switching perm
128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 1.9598 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0012 - dann loss: 2.0602 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0010 - dann loss: 2.0754 - reconstruction loss: 0.0366128/3811 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.0660 - reconstruction loss: 0.0366128/3811 - total loss: 0.0001 - classification loss: 0.0010 - dann loss: 2.0609 - reconstruction loss: 0.0366128/3811 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.0668 - reconstruction loss: 0.0366128/3811 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.0563 - reconstruction loss: 0.0368128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0513 - reconstruction loss: 0.0369128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0496 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0430 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0378 - reconstruction loss: 0.0368128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0386 - reconstruction loss: 0.0367128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0348 - reconstruction loss: 0.0366128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0328 - reconstruction loss: 0.0367128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0341 - reconstruction loss: 0.0368128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0373 - reconstruction loss: 0.0369128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0364 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0369 - reconstruction loss: 0.0369128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0356 - reconstruction loss: 0.0369128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0333 - reconstruction loss: 0.0369128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0361 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.0343 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.0347 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.0342 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0326 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0319 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0296 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0338 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0314 - reconstruction loss: 0.037298/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0322 - reconstruction loss: 0.03710/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0322 - reconstruction loss: 0.0371Epoch 54/150, Current strat Epoch 15/50
use_perm = False
switching perm
128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0348 - reconstruction loss: 0.0399128/3811 - total loss: 0.0001 - classification loss: 0.0010 - dann loss: 2.0584 - reconstruction loss: 0.0389128/3811 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.0291 - reconstruction loss: 0.0380128/3811 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.0025 - reconstruction loss: 0.0384128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0246 - reconstruction loss: 0.0383128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0188 - reconstruction loss: 0.0377128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0467 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0434 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0415 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0359 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0371 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0357 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0370 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0354 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0336 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0366 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0350 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0363 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0341 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0336 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0317 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0331 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0333 - reconstruction loss: 0.0369128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0310 - reconstruction loss: 0.0369128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0337 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0337 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0364 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0353 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0357 - reconstruction loss: 0.037298/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0309 - reconstruction loss: 0.03720/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0309 - reconstruction loss: 0.0372Epoch 55/150, Current strat Epoch 16/50
use_perm = False
switching perm
128/3811 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.0950 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0254 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0260 - reconstruction loss: 0.0365128/3811 - total loss: 0.0001 - classification loss: 0.0010 - dann loss: 2.0307 - reconstruction loss: 0.0368128/3811 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.0321 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0296 - reconstruction loss: 0.0368128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0258 - reconstruction loss: 0.0366128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0214 - reconstruction loss: 0.0369128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0209 - reconstruction loss: 0.0368128/3811 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.0237 - reconstruction loss: 0.0369128/3811 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.0215 - reconstruction loss: 0.0369128/3811 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.0250 - reconstruction loss: 0.0368128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0301 - reconstruction loss: 0.0369128/3811 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.0310 - reconstruction loss: 0.0368128/3811 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.0306 - reconstruction loss: 0.0367128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0275 - reconstruction loss: 0.0368128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0234 - reconstruction loss: 0.0369128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0282 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0315 - reconstruction loss: 0.0369128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0329 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0325 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0288 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0289 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0294 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0297 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0309 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0364 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0344 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0339 - reconstruction loss: 0.037198/3811 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.0315 - reconstruction loss: 0.03720/3811 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.0315 - reconstruction loss: 0.0372Epoch 56/150, Current strat Epoch 17/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0221 - reconstruction loss: 0.0369128/3811 - total loss: 0.0001 - classification loss: 0.0010 - dann loss: 2.0181 - reconstruction loss: 0.0376128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0014 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 1.9902 - reconstruction loss: 0.0378128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0015 - reconstruction loss: 0.0379128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 1.9917 - reconstruction loss: 0.0379128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 1.9905 - reconstruction loss: 0.0376128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 1.9971 - reconstruction loss: 0.0375128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0077 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0107 - reconstruction loss: 0.0375128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0175 - reconstruction loss: 0.0376128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0203 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0258 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0224 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0264 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0246 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0313 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0297 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0293 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0246 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0275 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0320 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0312 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0325 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0290 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0298 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0336 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0338 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0334 - reconstruction loss: 0.037298/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0316 - reconstruction loss: 0.03720/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0316 - reconstruction loss: 0.0372Epoch 57/150, Current strat Epoch 18/50
use_perm = False
switching perm
128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0200 - reconstruction loss: 0.0380128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0102 - reconstruction loss: 0.0376128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0374 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0605 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0372 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0218 - reconstruction loss: 0.0369128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0292 - reconstruction loss: 0.0369128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0346 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0366 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0339 - reconstruction loss: 0.0368128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0268 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0290 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0234 - reconstruction loss: 0.0368128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0206 - reconstruction loss: 0.0369128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0206 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0189 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0178 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0191 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0243 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0253 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0276 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0297 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0293 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0313 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0327 - reconstruction loss: 0.0373128/3742 - total loss: 0.2540 - classification loss: 0.0115 - dann loss: 2.2529 - reconstruction loss: 0.0345128/3742 - total loss: 0.2542 - classification loss: 0.0117 - dann loss: 2.2535 - reconstruction loss: 0.0346128/3742 - total loss: 0.2542 - classification loss: 0.0119 - dann loss: 2.2533 - reconstruction loss: 0.0346128/3742 - total loss: 0.2544 - classification loss: 0.0125 - dann loss: 2.2542 - reconstruction loss: 0.0346128/3742 - total loss: 0.2545 - classification loss: 0.0125 - dann loss: 2.2555 - reconstruction loss: 0.0347128/3742 - total loss: 0.2542 - classification loss: 0.0123 - dann loss: 2.2525 - reconstruction loss: 0.0347128/3742 - total loss: 0.2541 - classification loss: 0.0122 - dann loss: 2.2510 - reconstruction loss: 0.0347128/3742 - total loss: 0.2541 - classification loss: 0.0122 - dann loss: 2.2508 - reconstruction loss: 0.0347128/3742 - total loss: 0.2539 - classification loss: 0.0122 - dann loss: 2.2499 - reconstruction loss: 0.0346128/3742 - total loss: 0.2537 - classification loss: 0.0121 - dann loss: 2.2475 - reconstruction loss: 0.034729/3742 - total loss: 0.2538 - classification loss: 0.0124 - dann loss: 2.2474 - reconstruction loss: 0.03470/3742 - total loss: 0.2538 - classification loss: 0.0124 - dann loss: 2.2474 - reconstruction loss: 0.0347Epoch 25/150, Current strat Epoch 25/100
use_perm = True
switching perm
128/3742 - total loss: 0.2555 - classification loss: 0.0092 - dann loss: 2.2638 - reconstruction loss: 0.0353128/3742 - total loss: 0.2550 - classification loss: 0.0102 - dann loss: 2.2605 - reconstruction loss: 0.0350128/3742 - total loss: 0.2533 - classification loss: 0.0096 - dann loss: 2.2453 - reconstruction loss: 0.0348128/3742 - total loss: 0.2550 - classification loss: 0.0110 - dann loss: 2.2631 - reconstruction loss: 0.0345128/3742 - total loss: 0.2546 - classification loss: 0.0118 - dann loss: 2.2542 - reconstruction loss: 0.0350128/3742 - total loss: 0.2535 - classification loss: 0.0115 - dann loss: 2.2462 - reconstruction loss: 0.0347128/3742 - total loss: 0.2529 - classification loss: 0.0113 - dann loss: 2.2375 - reconstruction loss: 0.0350128/3742 - total loss: 0.2525 - classification loss: 0.0109 - dann loss: 2.2355 - reconstruction loss: 0.0348128/3742 - total loss: 0.2525 - classification loss: 0.0110 - dann loss: 2.2348 - reconstruction loss: 0.0349128/3742 - total loss: 0.2526 - classification loss: 0.0110 - dann loss: 2.2356 - reconstruction loss: 0.0349128/3742 - total loss: 0.2523 - classification loss: 0.0110 - dann loss: 2.2336 - reconstruction loss: 0.0348128/3742 - total loss: 0.2525 - classification loss: 0.0113 - dann loss: 2.2361 - reconstruction loss: 0.0348128/3742 - total loss: 0.2522 - classification loss: 0.0114 - dann loss: 2.2323 - reconstruction loss: 0.0347128/3742 - total loss: 0.2521 - classification loss: 0.0117 - dann loss: 2.2317 - reconstruction loss: 0.0347128/3742 - total loss: 0.2523 - classification loss: 0.0121 - dann loss: 2.2342 - reconstruction loss: 0.0346128/3742 - total loss: 0.2526 - classification loss: 0.0136 - dann loss: 2.2347 - reconstruction loss: 0.0347128/3742 - total loss: 0.2526 - classification loss: 0.0136 - dann loss: 2.2352 - reconstruction loss: 0.0347128/3742 - total loss: 0.2531 - classification loss: 0.0137 - dann loss: 2.2388 - reconstruction loss: 0.0348128/3742 - total loss: 0.2529 - classification loss: 0.0135 - dann loss: 2.2380 - reconstruction loss: 0.0347128/3742 - total loss: 0.2525 - classification loss: 0.0135 - dann loss: 2.2342 - reconstruction loss: 0.0346128/3742 - total loss: 0.2526 - classification loss: 0.0133 - dann loss: 2.2359 - reconstruction loss: 0.0346128/3742 - total loss: 0.2524 - classification loss: 0.0135 - dann loss: 2.2336 - reconstruction loss: 0.0346128/3742 - total loss: 0.2523 - classification loss: 0.0134 - dann loss: 2.2333 - reconstruction loss: 0.0346128/3742 - total loss: 0.2520 - classification loss: 0.0134 - dann loss: 2.2303 - reconstruction loss: 0.0346128/3742 - total loss: 0.2522 - classification loss: 0.0133 - dann loss: 2.2318 - reconstruction loss: 0.0346128/3742 - total loss: 0.2524 - classification loss: 0.0138 - dann loss: 2.2322 - reconstruction loss: 0.0347128/3742 - total loss: 0.2524 - classification loss: 0.0140 - dann loss: 2.2320 - reconstruction loss: 0.0348128/3742 - total loss: 0.2524 - classification loss: 0.0147 - dann loss: 2.2308 - reconstruction loss: 0.0348128/3742 - total loss: 0.2522 - classification loss: 0.0146 - dann loss: 2.2292 - reconstruction loss: 0.034729/3742 - total loss: 0.2516 - classification loss: 0.0144 - dann loss: 2.2245 - reconstruction loss: 0.03470/3742 - total loss: 0.2516 - classification loss: 0.0144 - dann loss: 2.2245 - reconstruction loss: 0.0347Epoch 26/150, Current strat Epoch 26/100
use_perm = True
switching perm
128/3742 - total loss: 0.2549 - classification loss: 0.0088 - dann loss: 2.2653 - reconstruction loss: 0.0344128/3742 - total loss: 0.2544 - classification loss: 0.0091 - dann loss: 2.2631 - reconstruction loss: 0.0340128/3742 - total loss: 0.2544 - classification loss: 0.0113 - dann loss: 2.2597 - reconstruction loss: 0.0341128/3742 - total loss: 0.2540 - classification loss: 0.0110 - dann loss: 2.2550 - reconstruction loss: 0.0342128/3742 - total loss: 0.2537 - classification loss: 0.0108 - dann loss: 2.2521 - reconstruction loss: 0.0342128/3742 - total loss: 0.2528 - classification loss: 0.0106 - dann loss: 2.2455 - reconstruction loss: 0.0340128/3742 - total loss: 0.2526 - classification loss: 0.0109 - dann loss: 2.2420 - reconstruction loss: 0.0341128/3742 - total loss: 0.2522 - classification loss: 0.0111 - dann loss: 2.2371 - reconstruction loss: 0.0342128/3742 - total loss: 0.2527 - classification loss: 0.0125 - dann loss: 2.2392 - reconstruction loss: 0.0344128/3742 - total loss: 0.2528 - classification loss: 0.0127 - dann loss: 2.2379 - reconstruction loss: 0.0347128/3742 - total loss: 0.2534 - classification loss: 0.0135 - dann loss: 2.2418 - reconstruction loss: 0.0349128/3742 - total loss: 0.2535 - classification loss: 0.0130 - dann loss: 2.2431 - reconstruction loss: 0.0349128/3742 - total loss: 0.2533 - classification loss: 0.0125 - dann loss: 2.2434 - reconstruction loss: 0.0347128/3742 - total loss: 0.2533 - classification loss: 0.0123 - dann loss: 2.2427 - reconstruction loss: 0.0347128/3742 - total loss: 0.2530 - classification loss: 0.0120 - dann loss: 2.2414 - reconstruction loss: 0.0346128/3742 - total loss: 0.2532 - classification loss: 0.0120 - dann loss: 2.2430 - reconstruction loss: 0.0346128/3742 - total loss: 0.2531 - classification loss: 0.0118 - dann loss: 2.2416 - reconstruction loss: 0.0347128/3742 - total loss: 0.2528 - classification loss: 0.0116 - dann loss: 2.2385 - reconstruction loss: 0.0347128/3742 - total loss: 0.2525 - classification loss: 0.0112 - dann loss: 2.2360 - reconstruction loss: 0.0347128/3742 - total loss: 0.2525 - classification loss: 0.0111 - dann loss: 2.2354 - reconstruction loss: 0.0348128/3742 - total loss: 0.2521 - classification loss: 0.0109 - dann loss: 2.2320 - reconstruction loss: 0.0347128/3742 - total loss: 0.2519 - classification loss: 0.0111 - dann loss: 2.2307 - reconstruction loss: 0.0347128/3742 - total loss: 0.2516 - classification loss: 0.0109 - dann loss: 2.2281 - reconstruction loss: 0.0346128/3742 - total loss: 0.2515 - classification loss: 0.0108 - dann loss: 2.2268 - reconstruction loss: 0.0346128/3742 - total loss: 0.2512 - classification loss: 0.0109 - dann loss: 2.2246 - reconstruction loss: 0.0346128/3742 - total loss: 0.2514 - classification loss: 0.0109 - dann loss: 2.2262 - reconstruction loss: 0.0346128/3742 - total loss: 0.2515 - classification loss: 0.0108 - dann loss: 2.2268 - reconstruction loss: 0.0347128/3742 - total loss: 0.2513 - classification loss: 0.0107 - dann loss: 2.2248 - reconstruction loss: 0.0347128/3742 - total loss: 0.2514 - classification loss: 0.0112 - dann loss: 2.2252 - reconstruction loss: 0.034729/3742 - total loss: 0.2514 - classification loss: 0.0131 - dann loss: 2.2235 - reconstruction loss: 0.03470/3742 - total loss: 0.2514 - classification loss: 0.0131 - dann loss: 2.2235 - reconstruction loss: 0.0347128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0336 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0344 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0322 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0317 - reconstruction loss: 0.037298/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0321 - reconstruction loss: 0.03710/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0321 - reconstruction loss: 0.0371Epoch 58/150, Current strat Epoch 19/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0022 - reconstruction loss: 0.0395128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0540 - reconstruction loss: 0.0388128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0552 - reconstruction loss: 0.0386128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0347 - reconstruction loss: 0.0383128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0360 - reconstruction loss: 0.0380128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0330 - reconstruction loss: 0.0376128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0274 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0263 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0171 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0104 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0236 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0274 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0248 - reconstruction loss: 0.0375128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0245 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0230 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0235 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0244 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0243 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0252 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0273 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0300 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0312 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0324 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0314 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0296 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0303 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0310 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0339 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0322 - reconstruction loss: 0.037198/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0319 - reconstruction loss: 0.03720/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0319 - reconstruction loss: 0.0372Epoch 59/150, Current strat Epoch 20/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 1.9964 - reconstruction loss: 0.0388128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0099 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0229 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0612 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0497 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0313 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0274 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0250 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0248 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0240 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0226 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0217 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0287 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0338 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0451 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0404 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0362 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0319 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0259 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0280 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0336 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0347 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0335 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0349 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0328 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0324 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0336 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0333 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0328 - reconstruction loss: 0.037298/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0318 - reconstruction loss: 0.03720/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0318 - reconstruction loss: 0.0372Epoch 60/150, Current strat Epoch 21/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.1038 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0343 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0653 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0408 - reconstruction loss: 0.0368128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0263 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0245 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0224 - reconstruction loss: 0.0368128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0381 - reconstruction loss: 0.0368128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0340 - reconstruction loss: 0.0368128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0291 - reconstruction loss: 0.0366128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0323 - reconstruction loss: 0.0368128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0295 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0220 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0230 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0297 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0315 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0335 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0278 - reconstruction loss: 0.0375128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0314 - reconstruction loss: 0.0375128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0298 - reconstruction loss: 0.0375128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0281 - reconstruction loss: 0.0375128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0247 - reconstruction loss: 0.0375128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0288 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0302 - reconstruction loss: 0.0374128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0327 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0308 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0332 - reconstruction loss: 0.0373128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0320 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0307 - reconstruction loss: 0.037298/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0324 - reconstruction loss: 0.03710/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0324 - reconstruction loss: 0.0371Epoch 61/150, Current strat Epoch 22/50
use_perm = False
switching perm
128/3811 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.0719 - reconstruction loss: 0.0375128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0632 - reconstruction loss: 0.0356128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0686 - reconstruction loss: 0.0356128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0226 - reconstruction loss: 0.0359128/3811 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.0223 - reconstruction loss: 0.0367128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0281 - reconstruction loss: 0.0365128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0197 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0077 - reconstruction loss: 0.0368128/3811 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.0153 - reconstruction loss: 0.0369128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0195 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0164 - reconstruction loss: 0.0368128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0198 - reconstruction loss: 0.0369128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0256 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0285 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0289 - reconstruction loss: 0.0369128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0289 - reconstruction loss: 0.0368128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0313 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0378 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0353 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0354 - reconstruction loss: 0.0370128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0359 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0353 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0338 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0377 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0403 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0395 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0361 - reconstruction loss: 0.0371128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0350 - reconstruction loss: 0.0372128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0344 - reconstruction loss: 0.037198/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0313 - reconstruction loss: 0.03720/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0313 - reconstruction loss: 0.0372Epoch 62/150, Current strat Epoch 23/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 1.9484 - reconstruction loss: 0.0376128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0254 - reconstruction loss: 0.0380128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 1.9938 - reconstruction loss: 0.0378128/3811 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.0159 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0188 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0264 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0230 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0242 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0203 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0186 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0098 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0126 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0158 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0202 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0202 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0214 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0179 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0206 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0218 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0219 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0228 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0264 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.0264 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0261 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0234 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0248 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0254 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0270 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0291 - reconstruction loss: 0.037298/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0329 - reconstruction loss: 0.03710/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0329 - reconstruction loss: 0.0371Epoch 63/150, Current strat Epoch 24/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0942 - reconstruction loss: 0.0379128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.1041 - reconstruction loss: 0.0362128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.1199 - reconstruction loss: 0.0360128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0773 - reconstruction loss: 0.0364128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0639 - reconstruction loss: 0.0367128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0573 - reconstruction loss: 0.0365128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0602 - reconstruction loss: 0.0367128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0608 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0529 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0448 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0442 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0440 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0386 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0455 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0476 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0447 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0460 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0444 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0450 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0386 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0388 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0339 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0305 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0288 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0272 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0298 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0304 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0311 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0315 - reconstruction loss: 0.037198/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0322 - reconstruction loss: 0.03720/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0322 - reconstruction loss: 0.0372Epoch 64/150, Current strat Epoch 25/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0355 - reconstruction loss: 0.0368128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0703 - reconstruction loss: 0.0367128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0252 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 1.9998 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0213 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0132 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0256 - reconstruction loss: 0.0367128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0206 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0175 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0180 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0150 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0102 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0137 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0157 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0135 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0156 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0159 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0188 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0202 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0225 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0215 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0253 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0241 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0257 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0286 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0260 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0288 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0335 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0349 - reconstruction loss: 0.0371Epoch 27/150, Current strat Epoch 27/100
use_perm = True
switching perm
128/3742 - total loss: 0.2512 - classification loss: 0.0085 - dann loss: 2.2258 - reconstruction loss: 0.0347128/3742 - total loss: 0.2515 - classification loss: 0.0078 - dann loss: 2.2400 - reconstruction loss: 0.0334128/3742 - total loss: 0.2495 - classification loss: 0.0096 - dann loss: 2.2135 - reconstruction loss: 0.0340128/3742 - total loss: 0.2484 - classification loss: 0.0095 - dann loss: 2.2039 - reconstruction loss: 0.0339128/3742 - total loss: 0.2487 - classification loss: 0.0091 - dann loss: 2.2051 - reconstruction loss: 0.0341128/3742 - total loss: 0.2494 - classification loss: 0.0087 - dann loss: 2.2127 - reconstruction loss: 0.0341128/3742 - total loss: 0.2501 - classification loss: 0.0088 - dann loss: 2.2163 - reconstruction loss: 0.0345128/3742 - total loss: 0.2500 - classification loss: 0.0089 - dann loss: 2.2123 - reconstruction loss: 0.0348128/3742 - total loss: 0.2493 - classification loss: 0.0089 - dann loss: 2.2076 - reconstruction loss: 0.0346128/3742 - total loss: 0.2498 - classification loss: 0.0089 - dann loss: 2.2109 - reconstruction loss: 0.0347128/3742 - total loss: 0.2489 - classification loss: 0.0086 - dann loss: 2.2029 - reconstruction loss: 0.0346128/3742 - total loss: 0.2488 - classification loss: 0.0087 - dann loss: 2.2026 - reconstruction loss: 0.0346128/3742 - total loss: 0.2493 - classification loss: 0.0091 - dann loss: 2.2069 - reconstruction loss: 0.0346128/3742 - total loss: 0.2497 - classification loss: 0.0096 - dann loss: 2.2099 - reconstruction loss: 0.0347128/3742 - total loss: 0.2494 - classification loss: 0.0094 - dann loss: 2.2065 - reconstruction loss: 0.0347128/3742 - total loss: 0.2489 - classification loss: 0.0092 - dann loss: 2.2029 - reconstruction loss: 0.0346128/3742 - total loss: 0.2486 - classification loss: 0.0093 - dann loss: 2.2000 - reconstruction loss: 0.0346128/3742 - total loss: 0.2485 - classification loss: 0.0098 - dann loss: 2.1985 - reconstruction loss: 0.0346128/3742 - total loss: 0.2488 - classification loss: 0.0102 - dann loss: 2.2004 - reconstruction loss: 0.0346128/3742 - total loss: 0.2489 - classification loss: 0.0100 - dann loss: 2.2019 - reconstruction loss: 0.0347128/3742 - total loss: 0.2487 - classification loss: 0.0100 - dann loss: 2.2001 - reconstruction loss: 0.0346128/3742 - total loss: 0.2485 - classification loss: 0.0100 - dann loss: 2.1979 - reconstruction loss: 0.0347128/3742 - total loss: 0.2486 - classification loss: 0.0101 - dann loss: 2.1984 - reconstruction loss: 0.0347128/3742 - total loss: 0.2485 - classification loss: 0.0101 - dann loss: 2.1980 - reconstruction loss: 0.0346128/3742 - total loss: 0.2486 - classification loss: 0.0101 - dann loss: 2.1994 - reconstruction loss: 0.0346128/3742 - total loss: 0.2488 - classification loss: 0.0105 - dann loss: 2.2000 - reconstruction loss: 0.0346128/3742 - total loss: 0.2487 - classification loss: 0.0105 - dann loss: 2.1990 - reconstruction loss: 0.0347128/3742 - total loss: 0.2484 - classification loss: 0.0105 - dann loss: 2.1963 - reconstruction loss: 0.0347128/3742 - total loss: 0.2483 - classification loss: 0.0105 - dann loss: 2.1949 - reconstruction loss: 0.034729/3742 - total loss: 0.2485 - classification loss: 0.0166 - dann loss: 2.1892 - reconstruction loss: 0.03490/3742 - total loss: 0.2485 - classification loss: 0.0166 - dann loss: 2.1892 - reconstruction loss: 0.0349Epoch 28/150, Current strat Epoch 28/100
use_perm = True
switching perm
128/3742 - total loss: 0.2436 - classification loss: 0.0080 - dann loss: 2.1627 - reconstruction loss: 0.0331128/3742 - total loss: 0.2479 - classification loss: 0.0099 - dann loss: 2.1996 - reconstruction loss: 0.0337128/3742 - total loss: 0.2501 - classification loss: 0.0149 - dann loss: 2.2083 - reconstruction loss: 0.0347128/3742 - total loss: 0.2513 - classification loss: 0.0146 - dann loss: 2.2202 - reconstruction loss: 0.0348128/3742 - total loss: 0.2524 - classification loss: 0.0141 - dann loss: 2.2294 - reconstruction loss: 0.0351128/3742 - total loss: 0.2511 - classification loss: 0.0129 - dann loss: 2.2204 - reconstruction loss: 0.0348128/3742 - total loss: 0.2510 - classification loss: 0.0124 - dann loss: 2.2176 - reconstruction loss: 0.0350128/3742 - total loss: 0.2499 - classification loss: 0.0118 - dann loss: 2.2069 - reconstruction loss: 0.0350128/3742 - total loss: 0.2506 - classification loss: 0.0194 - dann loss: 2.2058 - reconstruction loss: 0.0351128/3742 - total loss: 0.2507 - classification loss: 0.0227 - dann loss: 2.2056 - reconstruction loss: 0.0348128/3742 - total loss: 0.2504 - classification loss: 0.0220 - dann loss: 2.2037 - reconstruction loss: 0.0348128/3742 - total loss: 0.2498 - classification loss: 0.0208 - dann loss: 2.1993 - reconstruction loss: 0.0347128/3742 - total loss: 0.2498 - classification loss: 0.0205 - dann loss: 2.1997 - reconstruction loss: 0.0347128/3742 - total loss: 0.2495 - classification loss: 0.0200 - dann loss: 2.1979 - reconstruction loss: 0.0347128/3742 - total loss: 0.2495 - classification loss: 0.0195 - dann loss: 2.1976 - reconstruction loss: 0.0347128/3742 - total loss: 0.2502 - classification loss: 0.0224 - dann loss: 2.2007 - reconstruction loss: 0.0348128/3742 - total loss: 0.2503 - classification loss: 0.0219 - dann loss: 2.2028 - reconstruction loss: 0.0348128/3742 - total loss: 0.2502 - classification loss: 0.0214 - dann loss: 2.2018 - reconstruction loss: 0.0348128/3742 - total loss: 0.2507 - classification loss: 0.0252 - dann loss: 2.2029 - reconstruction loss: 0.0348128/3742 - total loss: 0.2507 - classification loss: 0.0251 - dann loss: 2.2035 - reconstruction loss: 0.0347128/3742 - total loss: 0.2507 - classification loss: 0.0248 - dann loss: 2.2050 - reconstruction loss: 0.0347128/3742 - total loss: 0.2505 - classification loss: 0.0241 - dann loss: 2.2031 - reconstruction loss: 0.0347128/3742 - total loss: 0.2503 - classification loss: 0.0236 - dann loss: 2.2019 - reconstruction loss: 0.0347128/3742 - total loss: 0.2504 - classification loss: 0.0232 - dann loss: 2.2034 - reconstruction loss: 0.0347128/3742 - total loss: 0.2503 - classification loss: 0.0227 - dann loss: 2.2020 - reconstruction loss: 0.0348128/3742 - total loss: 0.2501 - classification loss: 0.0229 - dann loss: 2.1999 - reconstruction loss: 0.0348128/3742 - total loss: 0.2503 - classification loss: 0.0229 - dann loss: 2.2014 - reconstruction loss: 0.0348128/3742 - total loss: 0.2502 - classification loss: 0.0236 - dann loss: 2.1996 - reconstruction loss: 0.0349128/3742 - total loss: 0.2499 - classification loss: 0.0235 - dann loss: 2.1968 - reconstruction loss: 0.034929/3742 - total loss: 0.2498 - classification loss: 0.0255 - dann loss: 2.1929 - reconstruction loss: 0.03500/3742 - total loss: 0.2498 - classification loss: 0.0255 - dann loss: 2.1929 - reconstruction loss: 0.0350Epoch 29/150, Current strat Epoch 29/100
use_perm = True
switching perm
128/3742 - total loss: 0.2578 - classification loss: 0.0247 - dann loss: 2.2600 - reconstruction loss: 0.0367128/3742 - total loss: 0.2549 - classification loss: 0.0203 - dann loss: 2.2402 - reconstruction loss: 0.0361128/3742 - total loss: 0.2537 - classification loss: 0.0185 - dann loss: 2.2369 - reconstruction loss: 0.0352128/3742 - total loss: 0.2547 - classification loss: 0.0187 - dann loss: 2.2492 - reconstruction loss: 0.0348128/3742 - total loss: 0.2551 - classification loss: 0.0177 - dann loss: 2.2515 - reconstruction loss: 0.0353128/3742 - total loss: 0.2534 - classification loss: 0.0163 - dann loss: 2.2360 - reconstruction loss: 0.0352128/3742 - total loss: 0.2526 - classification loss: 0.0159 - dann loss: 2.2299 - reconstruction loss: 0.0350128/3742 - total loss: 0.2526 - classification loss: 0.0156 - dann loss: 2.2310 - reconstruction loss: 0.0350128/3742 - total loss: 0.2532 - classification loss: 0.0179 - dann loss: 2.2340 - reconstruction loss: 0.0350128/3742 - total loss: 0.2529 - classification loss: 0.0179 - dann loss: 2.2310 - reconstruction loss: 0.0350128/3742 - total loss: 0.2524 - classification loss: 0.0184 - dann loss: 2.2263 - reconstruction loss: 0.034998/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0311 - reconstruction loss: 0.03720/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0311 - reconstruction loss: 0.0372Epoch 65/150, Current strat Epoch 26/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0517 - reconstruction loss: 0.0354128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0516 - reconstruction loss: 0.0363128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0758 - reconstruction loss: 0.0362128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0704 - reconstruction loss: 0.0367128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0801 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0662 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0509 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0569 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0585 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0446 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0439 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0380 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0397 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0361 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0369 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0358 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0367 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0387 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0362 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0345 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0302 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0262 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0281 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0258 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0288 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0296 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0293 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0292 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0324 - reconstruction loss: 0.037298/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0319 - reconstruction loss: 0.03720/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0319 - reconstruction loss: 0.0372Epoch 66/150, Current strat Epoch 27/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0774 - reconstruction loss: 0.0377128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0093 - reconstruction loss: 0.0368128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 1.9936 - reconstruction loss: 0.0377128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0294 - reconstruction loss: 0.0383128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0324 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0273 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0284 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0258 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0332 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0287 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0276 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0320 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0329 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0341 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0321 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0309 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0319 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0323 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0301 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0279 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0274 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0290 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0261 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0294 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0281 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0265 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0301 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0313 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0317 - reconstruction loss: 0.037198/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0321 - reconstruction loss: 0.03720/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0321 - reconstruction loss: 0.0372Epoch 67/150, Current strat Epoch 28/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0363 - reconstruction loss: 0.0366128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0098 - reconstruction loss: 0.0364128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0204 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0233 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0290 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0356 - reconstruction loss: 0.0368128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0434 - reconstruction loss: 0.0367128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0427 - reconstruction loss: 0.0364128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0381 - reconstruction loss: 0.0365128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0312 - reconstruction loss: 0.0367128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0186 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0288 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0359 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0361 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0473 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0419 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0432 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0424 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0410 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0417 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0366 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0363 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0351 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0374 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0401 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0395 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0336 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0360 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0330 - reconstruction loss: 0.037198/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0317 - reconstruction loss: 0.03720/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0317 - reconstruction loss: 0.0372Epoch 68/150, Current strat Epoch 29/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 1.9921 - reconstruction loss: 0.0368128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 1.9885 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0185 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0265 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0324 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0456 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0443 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0455 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0569 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0487 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0473 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0502 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0478 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0531 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0497 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0485 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0418 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0383 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0376 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0322 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0316 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0329 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0305 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0300 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0278 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0269 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0288 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0319 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0296 - reconstruction loss: 0.037298/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0327 - reconstruction loss: 0.03720/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0327 - reconstruction loss: 0.0372Epoch 69/150, Current strat Epoch 30/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0297 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 1.9828 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0138 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0274 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0319 - reconstruction loss: 0.0378128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0229 - reconstruction loss: 0.0379128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0158 - reconstruction loss: 0.0378128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0189 - reconstruction loss: 0.0377128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0257 - reconstruction loss: 0.0376128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0230 - reconstruction loss: 0.0376128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0389 - reconstruction loss: 0.0377128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0486 - reconstruction loss: 0.0377128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0512 - reconstruction loss: 0.0378128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0476 - reconstruction loss: 0.0377128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0451 - reconstruction loss: 0.0377128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0434 - reconstruction loss: 0.0378128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0459 - reconstruction loss: 0.0376128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0424 - reconstruction loss: 0.0376128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0433 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0422 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0397 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0386 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0396 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0389 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0359 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0353 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0334 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0324 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0354 - reconstruction loss: 0.037198/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0310 - reconstruction loss: 0.03720/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0310 - reconstruction loss: 0.0372Epoch 70/150, Current strat Epoch 31/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0104 - reconstruction loss: 0.0354128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 1.9939 - reconstruction loss: 0.0363128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0098 - reconstruction loss: 0.0363128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0174 - reconstruction loss: 0.0365128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0147 - reconstruction loss: 0.0367128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0229 - reconstruction loss: 0.0367128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0347 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0438 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0361 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0447 - reconstruction loss: 0.0368128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0421 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0446 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0383 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0396 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0369 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0361 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0342 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0319 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0344 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0365 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0345 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0316 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0323 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0314 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0341 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0334 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0328 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0288 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0290 - reconstruction loss: 0.037298/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0329 - reconstruction loss: 0.03720/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0329 - reconstruction loss: 0.0372Epoch 71/150, Current strat Epoch 32/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0840 - reconstruction loss: 0.0391128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0673 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0479 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0538 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0553 - reconstruction loss: 0.0379128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0546 - reconstruction loss: 0.0379128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0419 - reconstruction loss: 0.0378128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0505 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0460 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0478 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0402 - reconstruction loss: 0.0376128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0398 - reconstruction loss: 0.0376128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0372 - reconstruction loss: 0.0376128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0370 - reconstruction loss: 0.0376128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0400 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0369 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0366 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0280 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0316 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0338 - reconstruction loss: 0.0377128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0337 - reconstruction loss: 0.0376128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0361 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0344 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0314 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0289 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0327 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0313 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0325 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0299 - reconstruction loss: 0.037298/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0326 - reconstruction loss: 0.03710/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0326 - reconstruction loss: 0.0371Epoch 72/150, Current strat Epoch 33/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0390 - reconstruction loss: 0.0377128/3742 - total loss: 0.2525 - classification loss: 0.0187 - dann loss: 2.2266 - reconstruction loss: 0.0349128/3742 - total loss: 0.2522 - classification loss: 0.0184 - dann loss: 2.2251 - reconstruction loss: 0.0349128/3742 - total loss: 0.2519 - classification loss: 0.0180 - dann loss: 2.2218 - reconstruction loss: 0.0348128/3742 - total loss: 0.2515 - classification loss: 0.0178 - dann loss: 2.2185 - reconstruction loss: 0.0348128/3742 - total loss: 0.2517 - classification loss: 0.0178 - dann loss: 2.2214 - reconstruction loss: 0.0347128/3742 - total loss: 0.2519 - classification loss: 0.0192 - dann loss: 2.2214 - reconstruction loss: 0.0347128/3742 - total loss: 0.2514 - classification loss: 0.0188 - dann loss: 2.2170 - reconstruction loss: 0.0348128/3742 - total loss: 0.2512 - classification loss: 0.0195 - dann loss: 2.2157 - reconstruction loss: 0.0346128/3742 - total loss: 0.2514 - classification loss: 0.0194 - dann loss: 2.2173 - reconstruction loss: 0.0347128/3742 - total loss: 0.2511 - classification loss: 0.0205 - dann loss: 2.2124 - reconstruction loss: 0.0347128/3742 - total loss: 0.2513 - classification loss: 0.0202 - dann loss: 2.2149 - reconstruction loss: 0.0348128/3742 - total loss: 0.2514 - classification loss: 0.0201 - dann loss: 2.2148 - reconstruction loss: 0.0349128/3742 - total loss: 0.2513 - classification loss: 0.0200 - dann loss: 2.2137 - reconstruction loss: 0.0349128/3742 - total loss: 0.2514 - classification loss: 0.0202 - dann loss: 2.2154 - reconstruction loss: 0.0348128/3742 - total loss: 0.2514 - classification loss: 0.0209 - dann loss: 2.2138 - reconstruction loss: 0.0349128/3742 - total loss: 0.2516 - classification loss: 0.0209 - dann loss: 2.2150 - reconstruction loss: 0.0350128/3742 - total loss: 0.2515 - classification loss: 0.0205 - dann loss: 2.2143 - reconstruction loss: 0.0350128/3742 - total loss: 0.2511 - classification loss: 0.0208 - dann loss: 2.2115 - reconstruction loss: 0.034929/3742 - total loss: 0.2514 - classification loss: 0.0221 - dann loss: 2.2120 - reconstruction loss: 0.03490/3742 - total loss: 0.2514 - classification loss: 0.0221 - dann loss: 2.2120 - reconstruction loss: 0.0349Epoch 30/150, Current strat Epoch 30/100
use_perm = True
switching perm
128/3742 - total loss: 0.2529 - classification loss: 0.0156 - dann loss: 2.2497 - reconstruction loss: 0.0330128/3742 - total loss: 0.2519 - classification loss: 0.0203 - dann loss: 2.2341 - reconstruction loss: 0.0331128/3742 - total loss: 0.2545 - classification loss: 0.0171 - dann loss: 2.2527 - reconstruction loss: 0.0344128/3742 - total loss: 0.2542 - classification loss: 0.0159 - dann loss: 2.2503 - reconstruction loss: 0.0345128/3742 - total loss: 0.2546 - classification loss: 0.0174 - dann loss: 2.2504 - reconstruction loss: 0.0348128/3742 - total loss: 0.2543 - classification loss: 0.0169 - dann loss: 2.2475 - reconstruction loss: 0.0348128/3742 - total loss: 0.2537 - classification loss: 0.0161 - dann loss: 2.2422 - reconstruction loss: 0.0348128/3742 - total loss: 0.2527 - classification loss: 0.0157 - dann loss: 2.2333 - reconstruction loss: 0.0348128/3742 - total loss: 0.2531 - classification loss: 0.0154 - dann loss: 2.2374 - reconstruction loss: 0.0348128/3742 - total loss: 0.2527 - classification loss: 0.0150 - dann loss: 2.2366 - reconstruction loss: 0.0344128/3742 - total loss: 0.2535 - classification loss: 0.0150 - dann loss: 2.2450 - reconstruction loss: 0.0344128/3742 - total loss: 0.2531 - classification loss: 0.0144 - dann loss: 2.2398 - reconstruction loss: 0.0346128/3742 - total loss: 0.2533 - classification loss: 0.0168 - dann loss: 2.2390 - reconstruction loss: 0.0346128/3742 - total loss: 0.2534 - classification loss: 0.0169 - dann loss: 2.2398 - reconstruction loss: 0.0346128/3742 - total loss: 0.2537 - classification loss: 0.0173 - dann loss: 2.2425 - reconstruction loss: 0.0347128/3742 - total loss: 0.2538 - classification loss: 0.0173 - dann loss: 2.2436 - reconstruction loss: 0.0346128/3742 - total loss: 0.2538 - classification loss: 0.0170 - dann loss: 2.2442 - reconstruction loss: 0.0346128/3742 - total loss: 0.2537 - classification loss: 0.0164 - dann loss: 2.2434 - reconstruction loss: 0.0346128/3742 - total loss: 0.2535 - classification loss: 0.0162 - dann loss: 2.2425 - reconstruction loss: 0.0346128/3742 - total loss: 0.2539 - classification loss: 0.0164 - dann loss: 2.2442 - reconstruction loss: 0.0348128/3742 - total loss: 0.2540 - classification loss: 0.0164 - dann loss: 2.2454 - reconstruction loss: 0.0348128/3742 - total loss: 0.2537 - classification loss: 0.0160 - dann loss: 2.2424 - reconstruction loss: 0.0348128/3742 - total loss: 0.2539 - classification loss: 0.0164 - dann loss: 2.2429 - reconstruction loss: 0.0349128/3742 - total loss: 0.2538 - classification loss: 0.0162 - dann loss: 2.2422 - reconstruction loss: 0.0349128/3742 - total loss: 0.2533 - classification loss: 0.0159 - dann loss: 2.2378 - reconstruction loss: 0.0349128/3742 - total loss: 0.2532 - classification loss: 0.0161 - dann loss: 2.2358 - reconstruction loss: 0.0350128/3742 - total loss: 0.2529 - classification loss: 0.0157 - dann loss: 2.2345 - reconstruction loss: 0.0349128/3742 - total loss: 0.2528 - classification loss: 0.0155 - dann loss: 2.2328 - reconstruction loss: 0.0349128/3742 - total loss: 0.2528 - classification loss: 0.0153 - dann loss: 2.2333 - reconstruction loss: 0.034929/3742 - total loss: 0.2527 - classification loss: 0.0151 - dann loss: 2.2327 - reconstruction loss: 0.03480/3742 - total loss: 0.2527 - classification loss: 0.0151 - dann loss: 2.2327 - reconstruction loss: 0.0348Epoch 31/150, Current strat Epoch 31/100
use_perm = True
switching perm
128/3742 - total loss: 0.2566 - classification loss: 0.0178 - dann loss: 2.2868 - reconstruction loss: 0.0327128/3742 - total loss: 0.2543 - classification loss: 0.0128 - dann loss: 2.2603 - reconstruction loss: 0.0338128/3742 - total loss: 0.2536 - classification loss: 0.0145 - dann loss: 2.2493 - reconstruction loss: 0.0340128/3742 - total loss: 0.2526 - classification loss: 0.0133 - dann loss: 2.2364 - reconstruction loss: 0.0345128/3742 - total loss: 0.2534 - classification loss: 0.0121 - dann loss: 2.2424 - reconstruction loss: 0.0349128/3742 - total loss: 0.2547 - classification loss: 0.0125 - dann loss: 2.2544 - reconstruction loss: 0.0350128/3742 - total loss: 0.2539 - classification loss: 0.0132 - dann loss: 2.2477 - reconstruction loss: 0.0348128/3742 - total loss: 0.2549 - classification loss: 0.0138 - dann loss: 2.2566 - reconstruction loss: 0.0348128/3742 - total loss: 0.2549 - classification loss: 0.0135 - dann loss: 2.2584 - reconstruction loss: 0.0346128/3742 - total loss: 0.2550 - classification loss: 0.0146 - dann loss: 2.2587 - reconstruction loss: 0.0346128/3742 - total loss: 0.2544 - classification loss: 0.0145 - dann loss: 2.2526 - reconstruction loss: 0.0347128/3742 - total loss: 0.2544 - classification loss: 0.0143 - dann loss: 2.2517 - reconstruction loss: 0.0348128/3742 - total loss: 0.2545 - classification loss: 0.0149 - dann loss: 2.2514 - reconstruction loss: 0.0349128/3742 - total loss: 0.2546 - classification loss: 0.0146 - dann loss: 2.2522 - reconstruction loss: 0.0349128/3742 - total loss: 0.2544 - classification loss: 0.0143 - dann loss: 2.2503 - reconstruction loss: 0.0349128/3742 - total loss: 0.2547 - classification loss: 0.0141 - dann loss: 2.2531 - reconstruction loss: 0.0350128/3742 - total loss: 0.2552 - classification loss: 0.0149 - dann loss: 2.2569 - reconstruction loss: 0.0350128/3742 - total loss: 0.2550 - classification loss: 0.0149 - dann loss: 2.2559 - reconstruction loss: 0.0350128/3742 - total loss: 0.2552 - classification loss: 0.0152 - dann loss: 2.2573 - reconstruction loss: 0.0350128/3742 - total loss: 0.2554 - classification loss: 0.0152 - dann loss: 2.2603 - reconstruction loss: 0.0348128/3742 - total loss: 0.2557 - classification loss: 0.0149 - dann loss: 2.2635 - reconstruction loss: 0.0348128/3742 - total loss: 0.2555 - classification loss: 0.0146 - dann loss: 2.2615 - reconstruction loss: 0.0348128/3742 - total loss: 0.2554 - classification loss: 0.0147 - dann loss: 2.2605 - reconstruction loss: 0.0348128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0325 - reconstruction loss: 0.0379128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0441 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0562 - reconstruction loss: 0.0366128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0521 - reconstruction loss: 0.0365128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0564 - reconstruction loss: 0.0366128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0516 - reconstruction loss: 0.0368128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0521 - reconstruction loss: 0.0368128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0473 - reconstruction loss: 0.0368128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0414 - reconstruction loss: 0.0367128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0393 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0309 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0325 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0339 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0389 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0353 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0342 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0383 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0397 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0390 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0385 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0369 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0372 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0354 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0356 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0332 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0348 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0348 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0333 - reconstruction loss: 0.037198/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0316 - reconstruction loss: 0.03720/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0316 - reconstruction loss: 0.0372Epoch 73/150, Current strat Epoch 34/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0093 - reconstruction loss: 0.0365128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0485 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0413 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0307 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0325 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0292 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0262 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0236 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0233 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0245 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0293 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0316 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0345 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0344 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0357 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0291 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0274 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0265 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0254 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0290 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0355 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0370 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0356 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0358 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0409 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0391 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0396 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0372 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0341 - reconstruction loss: 0.037298/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0314 - reconstruction loss: 0.03720/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0314 - reconstruction loss: 0.0372Epoch 74/150, Current strat Epoch 35/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0799 - reconstruction loss: 0.0367128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.1005 - reconstruction loss: 0.0362128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0739 - reconstruction loss: 0.0367128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0406 - reconstruction loss: 0.0368128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0402 - reconstruction loss: 0.0366128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0335 - reconstruction loss: 0.0368128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0275 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0249 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0190 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0140 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0211 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0190 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0201 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0202 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0216 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0248 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0223 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0216 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0198 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0200 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0223 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0246 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0267 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0270 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0309 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0336 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0330 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0325 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0332 - reconstruction loss: 0.037198/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0317 - reconstruction loss: 0.03720/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0317 - reconstruction loss: 0.0372Epoch 75/150, Current strat Epoch 36/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0493 - reconstruction loss: 0.0392128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0170 - reconstruction loss: 0.0361128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0205 - reconstruction loss: 0.0368128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0174 - reconstruction loss: 0.0366128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0287 - reconstruction loss: 0.0365128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0408 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0422 - reconstruction loss: 0.0367128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0401 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0512 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0478 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0432 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0515 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0447 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0475 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0458 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0415 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0391 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0339 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0282 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0290 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0300 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0247 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0258 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0288 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0309 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0290 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0291 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0290 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0323 - reconstruction loss: 0.037298/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0319 - reconstruction loss: 0.03720/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0319 - reconstruction loss: 0.0372Epoch 76/150, Current strat Epoch 37/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.0853 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0524 - reconstruction loss: 0.0380128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0271 - reconstruction loss: 0.0376128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0262 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0111 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0186 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0283 - reconstruction loss: 0.0379128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0149 - reconstruction loss: 0.0379128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0139 - reconstruction loss: 0.0379128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0163 - reconstruction loss: 0.0377128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0193 - reconstruction loss: 0.0378128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0214 - reconstruction loss: 0.0380128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0154 - reconstruction loss: 0.0380128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0144 - reconstruction loss: 0.0378128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0156 - reconstruction loss: 0.0378128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0126 - reconstruction loss: 0.0377128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0135 - reconstruction loss: 0.0376128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0147 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0142 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0165 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0198 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0213 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0210 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0214 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0229 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0216 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0257 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0300 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0340 - reconstruction loss: 0.037298/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0314 - reconstruction loss: 0.03720/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0314 - reconstruction loss: 0.0372Epoch 77/150, Current strat Epoch 38/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0138 - reconstruction loss: 0.0368128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0633 - reconstruction loss: 0.0364128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0067 - reconstruction loss: 0.0359128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0059 - reconstruction loss: 0.0364128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0022 - reconstruction loss: 0.0368128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0000 - reconstruction loss: 0.0365128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0319 - reconstruction loss: 0.0368128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0266 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0177 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0295 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0270 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0251 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0260 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0289 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0291 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0339 - reconstruction loss: 0.0368128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0313 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0314 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0315 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0344 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0368 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0373 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0368 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0378 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0355 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0331 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0323 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0273 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0303 - reconstruction loss: 0.037298/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0325 - reconstruction loss: 0.03710/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0325 - reconstruction loss: 0.0371Epoch 78/150, Current strat Epoch 39/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0278 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0177 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0202 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0101 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0213 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0296 - reconstruction loss: 0.0367128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0233 - reconstruction loss: 0.0367128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0298 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0313 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0311 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0254 - reconstruction loss: 0.0368128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0166 - reconstruction loss: 0.0368128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0150 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0178 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0221 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0222 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0179 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0250 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0300 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0267 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0257 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0292 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0313 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0277 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0291 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0315 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0319 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0317 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0294 - reconstruction loss: 0.037198/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0328 - reconstruction loss: 0.03720/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0328 - reconstruction loss: 0.0372Epoch 79/150, Current strat Epoch 40/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 1.9937 - reconstruction loss: 0.0378128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.0019 - reconstruction loss: 0.0363128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0325 - reconstruction loss: 0.0362128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0339 - reconstruction loss: 0.0367128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0516 - reconstruction loss: 0.0367128/3742 - total loss: 0.2554 - classification loss: 0.0148 - dann loss: 2.2602 - reconstruction loss: 0.0349128/3742 - total loss: 0.2555 - classification loss: 0.0146 - dann loss: 2.2617 - reconstruction loss: 0.0348128/3742 - total loss: 0.2555 - classification loss: 0.0144 - dann loss: 2.2624 - reconstruction loss: 0.0348128/3742 - total loss: 0.2554 - classification loss: 0.0144 - dann loss: 2.2614 - reconstruction loss: 0.0348128/3742 - total loss: 0.2553 - classification loss: 0.0142 - dann loss: 2.2597 - reconstruction loss: 0.0348128/3742 - total loss: 0.2551 - classification loss: 0.0140 - dann loss: 2.2583 - reconstruction loss: 0.034829/3742 - total loss: 0.2555 - classification loss: 0.0181 - dann loss: 2.2590 - reconstruction loss: 0.03480/3742 - total loss: 0.2555 - classification loss: 0.0181 - dann loss: 2.2590 - reconstruction loss: 0.0348Epoch 32/150, Current strat Epoch 32/100
use_perm = True
switching perm
128/3742 - total loss: 0.2591 - classification loss: 0.0214 - dann loss: 2.2833 - reconstruction loss: 0.0358128/3742 - total loss: 0.2568 - classification loss: 0.0177 - dann loss: 2.2741 - reconstruction loss: 0.0345128/3742 - total loss: 0.2571 - classification loss: 0.0199 - dann loss: 2.2744 - reconstruction loss: 0.0346128/3742 - total loss: 0.2566 - classification loss: 0.0198 - dann loss: 2.2684 - reconstruction loss: 0.0347128/3742 - total loss: 0.2574 - classification loss: 0.0215 - dann loss: 2.2760 - reconstruction loss: 0.0346128/3742 - total loss: 0.2560 - classification loss: 0.0202 - dann loss: 2.2652 - reconstruction loss: 0.0343128/3742 - total loss: 0.2560 - classification loss: 0.0202 - dann loss: 2.2643 - reconstruction loss: 0.0345128/3742 - total loss: 0.2551 - classification loss: 0.0189 - dann loss: 2.2575 - reconstruction loss: 0.0343128/3742 - total loss: 0.2547 - classification loss: 0.0184 - dann loss: 2.2548 - reconstruction loss: 0.0343128/3742 - total loss: 0.2541 - classification loss: 0.0173 - dann loss: 2.2486 - reconstruction loss: 0.0343128/3742 - total loss: 0.2544 - classification loss: 0.0168 - dann loss: 2.2525 - reconstruction loss: 0.0344128/3742 - total loss: 0.2540 - classification loss: 0.0168 - dann loss: 2.2478 - reconstruction loss: 0.0344128/3742 - total loss: 0.2541 - classification loss: 0.0166 - dann loss: 2.2481 - reconstruction loss: 0.0345128/3742 - total loss: 0.2538 - classification loss: 0.0160 - dann loss: 2.2456 - reconstruction loss: 0.0345128/3742 - total loss: 0.2542 - classification loss: 0.0155 - dann loss: 2.2491 - reconstruction loss: 0.0347128/3742 - total loss: 0.2539 - classification loss: 0.0153 - dann loss: 2.2469 - reconstruction loss: 0.0346128/3742 - total loss: 0.2536 - classification loss: 0.0150 - dann loss: 2.2447 - reconstruction loss: 0.0346128/3742 - total loss: 0.2533 - classification loss: 0.0149 - dann loss: 2.2415 - reconstruction loss: 0.0345128/3742 - total loss: 0.2532 - classification loss: 0.0156 - dann loss: 2.2406 - reconstruction loss: 0.0345128/3742 - total loss: 0.2533 - classification loss: 0.0159 - dann loss: 2.2395 - reconstruction loss: 0.0346128/3742 - total loss: 0.2531 - classification loss: 0.0158 - dann loss: 2.2379 - reconstruction loss: 0.0347128/3742 - total loss: 0.2535 - classification loss: 0.0161 - dann loss: 2.2410 - reconstruction loss: 0.0347128/3742 - total loss: 0.2532 - classification loss: 0.0157 - dann loss: 2.2385 - reconstruction loss: 0.0347128/3742 - total loss: 0.2530 - classification loss: 0.0156 - dann loss: 2.2371 - reconstruction loss: 0.0347128/3742 - total loss: 0.2532 - classification loss: 0.0186 - dann loss: 2.2359 - reconstruction loss: 0.0347128/3742 - total loss: 0.2530 - classification loss: 0.0182 - dann loss: 2.2339 - reconstruction loss: 0.0347128/3742 - total loss: 0.2525 - classification loss: 0.0178 - dann loss: 2.2300 - reconstruction loss: 0.0347128/3742 - total loss: 0.2522 - classification loss: 0.0177 - dann loss: 2.2266 - reconstruction loss: 0.0347128/3742 - total loss: 0.2519 - classification loss: 0.0175 - dann loss: 2.2235 - reconstruction loss: 0.034729/3742 - total loss: 0.2528 - classification loss: 0.0215 - dann loss: 2.2260 - reconstruction loss: 0.03510/3742 - total loss: 0.2528 - classification loss: 0.0215 - dann loss: 2.2260 - reconstruction loss: 0.0351Epoch 33/150, Current strat Epoch 33/100
use_perm = True
switching perm
128/3742 - total loss: 0.2504 - classification loss: 0.0228 - dann loss: 2.2082 - reconstruction loss: 0.0341128/3742 - total loss: 0.2519 - classification loss: 0.0163 - dann loss: 2.2270 - reconstruction loss: 0.0345128/3742 - total loss: 0.2493 - classification loss: 0.0145 - dann loss: 2.2049 - reconstruction loss: 0.0342128/3742 - total loss: 0.2517 - classification loss: 0.0217 - dann loss: 2.2215 - reconstruction loss: 0.0343128/3742 - total loss: 0.2505 - classification loss: 0.0200 - dann loss: 2.2120 - reconstruction loss: 0.0342128/3742 - total loss: 0.2516 - classification loss: 0.0205 - dann loss: 2.2212 - reconstruction loss: 0.0342128/3742 - total loss: 0.2508 - classification loss: 0.0197 - dann loss: 2.2129 - reconstruction loss: 0.0344128/3742 - total loss: 0.2513 - classification loss: 0.0196 - dann loss: 2.2184 - reconstruction loss: 0.0344128/3742 - total loss: 0.2507 - classification loss: 0.0187 - dann loss: 2.2129 - reconstruction loss: 0.0344128/3742 - total loss: 0.2507 - classification loss: 0.0175 - dann loss: 2.2128 - reconstruction loss: 0.0346128/3742 - total loss: 0.2501 - classification loss: 0.0166 - dann loss: 2.2076 - reconstruction loss: 0.0346128/3742 - total loss: 0.2500 - classification loss: 0.0161 - dann loss: 2.2067 - reconstruction loss: 0.0347128/3742 - total loss: 0.2502 - classification loss: 0.0156 - dann loss: 2.2080 - reconstruction loss: 0.0348128/3742 - total loss: 0.2501 - classification loss: 0.0155 - dann loss: 2.2079 - reconstruction loss: 0.0347128/3742 - total loss: 0.2501 - classification loss: 0.0152 - dann loss: 2.2085 - reconstruction loss: 0.0347128/3742 - total loss: 0.2497 - classification loss: 0.0145 - dann loss: 2.2051 - reconstruction loss: 0.0347128/3742 - total loss: 0.2495 - classification loss: 0.0143 - dann loss: 2.2027 - reconstruction loss: 0.0347128/3742 - total loss: 0.2494 - classification loss: 0.0148 - dann loss: 2.2014 - reconstruction loss: 0.0347128/3742 - total loss: 0.2494 - classification loss: 0.0148 - dann loss: 2.2009 - reconstruction loss: 0.0347128/3742 - total loss: 0.2497 - classification loss: 0.0149 - dann loss: 2.2033 - reconstruction loss: 0.0348128/3742 - total loss: 0.2495 - classification loss: 0.0145 - dann loss: 2.2017 - reconstruction loss: 0.0348128/3742 - total loss: 0.2495 - classification loss: 0.0146 - dann loss: 2.2021 - reconstruction loss: 0.0347128/3742 - total loss: 0.2492 - classification loss: 0.0145 - dann loss: 2.2000 - reconstruction loss: 0.0347128/3742 - total loss: 0.2492 - classification loss: 0.0144 - dann loss: 2.1996 - reconstruction loss: 0.0347128/3742 - total loss: 0.2489 - classification loss: 0.0144 - dann loss: 2.1969 - reconstruction loss: 0.0348128/3742 - total loss: 0.2488 - classification loss: 0.0143 - dann loss: 2.1957 - reconstruction loss: 0.0348128/3742 - total loss: 0.2490 - classification loss: 0.0141 - dann loss: 2.1976 - reconstruction loss: 0.0347128/3742 - total loss: 0.2490 - classification loss: 0.0144 - dann loss: 2.1973 - reconstruction loss: 0.0348128/3742 - total loss: 0.2486 - classification loss: 0.0141 - dann loss: 2.1943 - reconstruction loss: 0.034829/3742 - total loss: 0.2485 - classification loss: 0.0144 - dann loss: 2.1926 - reconstruction loss: 0.03480/3742 - total loss: 0.2485 - classification loss: 0.0144 - dann loss: 2.1926 - reconstruction loss: 0.0348Epoch 34/150, Current strat Epoch 34/100
use_perm = True
switching perm
128/3742 - total loss: 0.2511 - classification loss: 0.0101 - dann loss: 2.2129 - reconstruction loss: 0.0360128/3742 - total loss: 0.2493 - classification loss: 0.0119 - dann loss: 2.1925 - reconstruction loss: 0.0361128/3742 - total loss: 0.2477 - classification loss: 0.0118 - dann loss: 2.1848 - reconstruction loss: 0.0350128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0372 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0330 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0360 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0364 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0431 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0452 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0492 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0449 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0415 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0450 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0385 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0399 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0376 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0356 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0333 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0332 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0312 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0291 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0269 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0267 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0292 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0270 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0294 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0299 - reconstruction loss: 0.037198/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0326 - reconstruction loss: 0.03720/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0326 - reconstruction loss: 0.0372Epoch 80/150, Current strat Epoch 41/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.1045 - reconstruction loss: 0.0392128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0563 - reconstruction loss: 0.0391128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0240 - reconstruction loss: 0.0389128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0116 - reconstruction loss: 0.0390128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0310 - reconstruction loss: 0.0383128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0184 - reconstruction loss: 0.0377128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0164 - reconstruction loss: 0.0378128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0251 - reconstruction loss: 0.0378128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0254 - reconstruction loss: 0.0377128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0405 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0354 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0407 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0376 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0363 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0334 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0329 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0338 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0330 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0324 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0279 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0264 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0275 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0261 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0269 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0293 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0283 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0312 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0322 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0305 - reconstruction loss: 0.037298/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0325 - reconstruction loss: 0.03720/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0325 - reconstruction loss: 0.0372Epoch 81/150, Current strat Epoch 42/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 1.9681 - reconstruction loss: 0.0399128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 1.9739 - reconstruction loss: 0.0391128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 1.9880 - reconstruction loss: 0.0385128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0048 - reconstruction loss: 0.0382128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0302 - reconstruction loss: 0.0381128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0372 - reconstruction loss: 0.0381128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0353 - reconstruction loss: 0.0380128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0312 - reconstruction loss: 0.0379128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0282 - reconstruction loss: 0.0378128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0292 - reconstruction loss: 0.0376128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0337 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0364 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0261 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0250 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0277 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0290 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0295 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0323 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0334 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0303 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0331 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0342 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0326 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0308 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0297 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0322 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0330 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0299 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0305 - reconstruction loss: 0.037298/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0325 - reconstruction loss: 0.03720/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0325 - reconstruction loss: 0.0372Epoch 82/150, Current strat Epoch 43/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0743 - reconstruction loss: 0.0368128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0141 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0227 - reconstruction loss: 0.0365128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0166 - reconstruction loss: 0.0363128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0188 - reconstruction loss: 0.0363128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0103 - reconstruction loss: 0.0364128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0200 - reconstruction loss: 0.0366128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0133 - reconstruction loss: 0.0364128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0272 - reconstruction loss: 0.0365128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0290 - reconstruction loss: 0.0367128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0356 - reconstruction loss: 0.0366128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0313 - reconstruction loss: 0.0366128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0303 - reconstruction loss: 0.0367128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0258 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0299 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0316 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0272 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0270 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0326 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0314 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0337 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0358 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0335 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0361 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0343 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0338 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0341 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0322 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0290 - reconstruction loss: 0.037298/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0329 - reconstruction loss: 0.03720/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0329 - reconstruction loss: 0.0372Epoch 83/150, Current strat Epoch 44/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 1.9995 - reconstruction loss: 0.0348128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0559 - reconstruction loss: 0.0361128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0528 - reconstruction loss: 0.0365128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0342 - reconstruction loss: 0.0368128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0548 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0644 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0531 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0509 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0461 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0531 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0480 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0417 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0396 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0393 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0460 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0446 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0439 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0363 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0368 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0365 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0353 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0345 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0309 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0340 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0355 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0340 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0316 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0333 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.0310 - reconstruction loss: 0.037298/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0323 - reconstruction loss: 0.03720/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0323 - reconstruction loss: 0.0372Epoch 84/150, Current strat Epoch 45/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0000 - dann loss: 2.0104 - reconstruction loss: 0.0351128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0471 - reconstruction loss: 0.0355128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0640 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0423 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0357 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0453 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0501 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0406 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0419 - reconstruction loss: 0.0376128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0497 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0453 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0435 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0416 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0446 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0457 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0421 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0443 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0408 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0421 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0424 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0385 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0385 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0370 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0339 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0341 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0352 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0356 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0338 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0306 - reconstruction loss: 0.037298/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0324 - reconstruction loss: 0.03720/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0324 - reconstruction loss: 0.0372Epoch 85/150, Current strat Epoch 46/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1121 - reconstruction loss: 0.0379128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1141 - reconstruction loss: 0.0382128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0993 - reconstruction loss: 0.0378128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0837 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0845 - reconstruction loss: 0.0377128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0958 - reconstruction loss: 0.0376128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0806 - reconstruction loss: 0.0377128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0658 - reconstruction loss: 0.0379128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0622 - reconstruction loss: 0.0376128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0597 - reconstruction loss: 0.0376128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0542 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0465 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0515 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0502 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0468 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0475 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0454 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0449 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0474 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0471 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0408 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0396 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0385 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0412 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0409 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0385 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0390 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0341 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0323 - reconstruction loss: 0.037298/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0319 - reconstruction loss: 0.03720/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0319 - reconstruction loss: 0.0372Epoch 86/150, Current strat Epoch 47/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0170 - reconstruction loss: 0.0385128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0563 - reconstruction loss: 0.0359128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0427 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0294 - reconstruction loss: 0.0366128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0456 - reconstruction loss: 0.0368128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0443 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0357 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0345 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0279 - reconstruction loss: 0.0371128/3742 - total loss: 0.2496 - classification loss: 0.0118 - dann loss: 2.2041 - reconstruction loss: 0.0350128/3742 - total loss: 0.2514 - classification loss: 0.0111 - dann loss: 2.2229 - reconstruction loss: 0.0349128/3742 - total loss: 0.2507 - classification loss: 0.0122 - dann loss: 2.2168 - reconstruction loss: 0.0347128/3742 - total loss: 0.2506 - classification loss: 0.0128 - dann loss: 2.2150 - reconstruction loss: 0.0348128/3742 - total loss: 0.2505 - classification loss: 0.0130 - dann loss: 2.2131 - reconstruction loss: 0.0349128/3742 - total loss: 0.2509 - classification loss: 0.0122 - dann loss: 2.2160 - reconstruction loss: 0.0351128/3742 - total loss: 0.2509 - classification loss: 0.0125 - dann loss: 2.2155 - reconstruction loss: 0.0351128/3742 - total loss: 0.2505 - classification loss: 0.0122 - dann loss: 2.2115 - reconstruction loss: 0.0351128/3742 - total loss: 0.2504 - classification loss: 0.0118 - dann loss: 2.2106 - reconstruction loss: 0.0352128/3742 - total loss: 0.2502 - classification loss: 0.0116 - dann loss: 2.2102 - reconstruction loss: 0.0351128/3742 - total loss: 0.2501 - classification loss: 0.0113 - dann loss: 2.2090 - reconstruction loss: 0.0350128/3742 - total loss: 0.2501 - classification loss: 0.0115 - dann loss: 2.2100 - reconstruction loss: 0.0350128/3742 - total loss: 0.2501 - classification loss: 0.0115 - dann loss: 2.2109 - reconstruction loss: 0.0348128/3742 - total loss: 0.2506 - classification loss: 0.0114 - dann loss: 2.2155 - reconstruction loss: 0.0349128/3742 - total loss: 0.2508 - classification loss: 0.0115 - dann loss: 2.2171 - reconstruction loss: 0.0349128/3742 - total loss: 0.2505 - classification loss: 0.0114 - dann loss: 2.2141 - reconstruction loss: 0.0349128/3742 - total loss: 0.2502 - classification loss: 0.0111 - dann loss: 2.2111 - reconstruction loss: 0.0349128/3742 - total loss: 0.2501 - classification loss: 0.0109 - dann loss: 2.2107 - reconstruction loss: 0.0349128/3742 - total loss: 0.2501 - classification loss: 0.0108 - dann loss: 2.2115 - reconstruction loss: 0.0349128/3742 - total loss: 0.2506 - classification loss: 0.0122 - dann loss: 2.2155 - reconstruction loss: 0.0348128/3742 - total loss: 0.2506 - classification loss: 0.0119 - dann loss: 2.2153 - reconstruction loss: 0.0349128/3742 - total loss: 0.2506 - classification loss: 0.0117 - dann loss: 2.2153 - reconstruction loss: 0.0349128/3742 - total loss: 0.2505 - classification loss: 0.0115 - dann loss: 2.2148 - reconstruction loss: 0.0348128/3742 - total loss: 0.2502 - classification loss: 0.0117 - dann loss: 2.2122 - reconstruction loss: 0.0348128/3742 - total loss: 0.2501 - classification loss: 0.0118 - dann loss: 2.2103 - reconstruction loss: 0.0348128/3742 - total loss: 0.2499 - classification loss: 0.0118 - dann loss: 2.2086 - reconstruction loss: 0.034829/3742 - total loss: 0.2497 - classification loss: 0.0147 - dann loss: 2.2044 - reconstruction loss: 0.03480/3742 - total loss: 0.2497 - classification loss: 0.0147 - dann loss: 2.2044 - reconstruction loss: 0.0348Epoch 35/150, Current strat Epoch 35/100
use_perm = True
switching perm
128/3742 - total loss: 0.2529 - classification loss: 0.0073 - dann loss: 2.2381 - reconstruction loss: 0.0354128/3742 - total loss: 0.2524 - classification loss: 0.0069 - dann loss: 2.2405 - reconstruction loss: 0.0346128/3742 - total loss: 0.2500 - classification loss: 0.0064 - dann loss: 2.2124 - reconstruction loss: 0.0351128/3742 - total loss: 0.2506 - classification loss: 0.0062 - dann loss: 2.2191 - reconstruction loss: 0.0351128/3742 - total loss: 0.2504 - classification loss: 0.0065 - dann loss: 2.2172 - reconstruction loss: 0.0351128/3742 - total loss: 0.2508 - classification loss: 0.0066 - dann loss: 2.2229 - reconstruction loss: 0.0348128/3742 - total loss: 0.2518 - classification loss: 0.0075 - dann loss: 2.2324 - reconstruction loss: 0.0347128/3742 - total loss: 0.2519 - classification loss: 0.0077 - dann loss: 2.2346 - reconstruction loss: 0.0346128/3742 - total loss: 0.2517 - classification loss: 0.0079 - dann loss: 2.2328 - reconstruction loss: 0.0345128/3742 - total loss: 0.2519 - classification loss: 0.0080 - dann loss: 2.2348 - reconstruction loss: 0.0345128/3742 - total loss: 0.2521 - classification loss: 0.0077 - dann loss: 2.2382 - reconstruction loss: 0.0344128/3742 - total loss: 0.2522 - classification loss: 0.0089 - dann loss: 2.2355 - reconstruction loss: 0.0347128/3742 - total loss: 0.2525 - classification loss: 0.0093 - dann loss: 2.2388 - reconstruction loss: 0.0346128/3742 - total loss: 0.2531 - classification loss: 0.0096 - dann loss: 2.2432 - reconstruction loss: 0.0347128/3742 - total loss: 0.2532 - classification loss: 0.0097 - dann loss: 2.2437 - reconstruction loss: 0.0348128/3742 - total loss: 0.2533 - classification loss: 0.0094 - dann loss: 2.2448 - reconstruction loss: 0.0348128/3742 - total loss: 0.2533 - classification loss: 0.0096 - dann loss: 2.2453 - reconstruction loss: 0.0347128/3742 - total loss: 0.2530 - classification loss: 0.0101 - dann loss: 2.2417 - reconstruction loss: 0.0348128/3742 - total loss: 0.2529 - classification loss: 0.0101 - dann loss: 2.2399 - reconstruction loss: 0.0349128/3742 - total loss: 0.2530 - classification loss: 0.0100 - dann loss: 2.2416 - reconstruction loss: 0.0348128/3742 - total loss: 0.2529 - classification loss: 0.0111 - dann loss: 2.2391 - reconstruction loss: 0.0349128/3742 - total loss: 0.2529 - classification loss: 0.0111 - dann loss: 2.2390 - reconstruction loss: 0.0349128/3742 - total loss: 0.2527 - classification loss: 0.0109 - dann loss: 2.2366 - reconstruction loss: 0.0349128/3742 - total loss: 0.2524 - classification loss: 0.0108 - dann loss: 2.2348 - reconstruction loss: 0.0348128/3742 - total loss: 0.2525 - classification loss: 0.0112 - dann loss: 2.2351 - reconstruction loss: 0.0348128/3742 - total loss: 0.2526 - classification loss: 0.0113 - dann loss: 2.2360 - reconstruction loss: 0.0348128/3742 - total loss: 0.2528 - classification loss: 0.0116 - dann loss: 2.2382 - reconstruction loss: 0.0348128/3742 - total loss: 0.2527 - classification loss: 0.0114 - dann loss: 2.2372 - reconstruction loss: 0.0348128/3742 - total loss: 0.2526 - classification loss: 0.0114 - dann loss: 2.2365 - reconstruction loss: 0.034829/3742 - total loss: 0.2522 - classification loss: 0.0128 - dann loss: 2.2311 - reconstruction loss: 0.03480/3742 - total loss: 0.2522 - classification loss: 0.0128 - dann loss: 2.2311 - reconstruction loss: 0.0348Epoch 36/150, Current strat Epoch 36/100
use_perm = True
switching perm
128/3742 - total loss: 0.2501 - classification loss: 0.0084 - dann loss: 2.2196 - reconstruction loss: 0.0342128/3742 - total loss: 0.2553 - classification loss: 0.0075 - dann loss: 2.2785 - reconstruction loss: 0.0334128/3742 - total loss: 0.2539 - classification loss: 0.0099 - dann loss: 2.2594 - reconstruction loss: 0.0337128/3742 - total loss: 0.2537 - classification loss: 0.0108 - dann loss: 2.2499 - reconstruction loss: 0.0345128/3742 - total loss: 0.2535 - classification loss: 0.0115 - dann loss: 2.2475 - reconstruction loss: 0.0345128/3742 - total loss: 0.2540 - classification loss: 0.0106 - dann loss: 2.2531 - reconstruction loss: 0.0346128/3742 - total loss: 0.2533 - classification loss: 0.0104 - dann loss: 2.2443 - reconstruction loss: 0.0347128/3742 - total loss: 0.2532 - classification loss: 0.0099 - dann loss: 2.2432 - reconstruction loss: 0.0348128/3742 - total loss: 0.2530 - classification loss: 0.0095 - dann loss: 2.2426 - reconstruction loss: 0.0347128/3742 - total loss: 0.2528 - classification loss: 0.0091 - dann loss: 2.2421 - reconstruction loss: 0.0346128/3742 - total loss: 0.2528 - classification loss: 0.0089 - dann loss: 2.2426 - reconstruction loss: 0.0346128/3742 - total loss: 0.2524 - classification loss: 0.0086 - dann loss: 2.2391 - reconstruction loss: 0.0345128/3742 - total loss: 0.2522 - classification loss: 0.0084 - dann loss: 2.2379 - reconstruction loss: 0.0345128/3742 - total loss: 0.2524 - classification loss: 0.0091 - dann loss: 2.2397 - reconstruction loss: 0.0344128/3742 - total loss: 0.2521 - classification loss: 0.0089 - dann loss: 2.2362 - reconstruction loss: 0.0345128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0279 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0233 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0288 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0231 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0245 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0284 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0322 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0354 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0357 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0347 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0331 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0319 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0334 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0321 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0304 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0303 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0333 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0379 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0339 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0359 - reconstruction loss: 0.037298/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0309 - reconstruction loss: 0.03710/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0309 - reconstruction loss: 0.0371Epoch 87/150, Current strat Epoch 48/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 1.9433 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0373 - reconstruction loss: 0.0385128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0116 - reconstruction loss: 0.0381128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0049 - reconstruction loss: 0.0382128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 1.9969 - reconstruction loss: 0.0383128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0002 - reconstruction loss: 0.0382128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0123 - reconstruction loss: 0.0380128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0147 - reconstruction loss: 0.0381128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0197 - reconstruction loss: 0.0379128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0305 - reconstruction loss: 0.0379128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0285 - reconstruction loss: 0.0378128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0335 - reconstruction loss: 0.0378128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0322 - reconstruction loss: 0.0377128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0312 - reconstruction loss: 0.0376128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0315 - reconstruction loss: 0.0377128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0252 - reconstruction loss: 0.0376128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0249 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0278 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0300 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0295 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0296 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0294 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0346 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0339 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0334 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0323 - reconstruction loss: 0.0370128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0300 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0287 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0319 - reconstruction loss: 0.037198/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0320 - reconstruction loss: 0.03720/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0320 - reconstruction loss: 0.0372Epoch 88/150, Current strat Epoch 49/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0328 - reconstruction loss: 0.0357128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0130 - reconstruction loss: 0.0366128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0168 - reconstruction loss: 0.0369128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0196 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0398 - reconstruction loss: 0.0377128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0364 - reconstruction loss: 0.0376128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0271 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0274 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0272 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0324 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0313 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0312 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0280 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0225 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0264 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0263 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0281 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0259 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0238 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0231 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0262 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0273 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0293 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0287 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0274 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0268 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0274 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0296 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0302 - reconstruction loss: 0.037198/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0326 - reconstruction loss: 0.03720/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0326 - reconstruction loss: 0.0372Epoch 89/150, Current strat Epoch 50/50
use_perm = False
switching perm
128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0896 - reconstruction loss: 0.0379128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0524 - reconstruction loss: 0.0383128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0337 - reconstruction loss: 0.0385128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0275 - reconstruction loss: 0.0382128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0251 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0310 - reconstruction loss: 0.0371128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0399 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0321 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0347 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0270 - reconstruction loss: 0.0375128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0249 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0193 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0150 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0158 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0167 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0181 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0204 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0259 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0266 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0205 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0224 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0234 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0257 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0271 - reconstruction loss: 0.0374128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0280 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0282 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0315 - reconstruction loss: 0.0373128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0284 - reconstruction loss: 0.0372128/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0309 - reconstruction loss: 0.037298/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0323 - reconstruction loss: 0.03710/3811 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.0323 - reconstruction loss: 0.0371['Secretory N' 'Suprabasal N' 'Secretory' ... 'Serous' 'Macrophage'
 'Basal']
Save adata_pred to /data/analysis/data_becavin/scmusketeers/data/Deprez-Lung-query-batch-0.2-pred.h5ad
128/3742 - total loss: 0.2517 - classification loss: 0.0091 - dann loss: 2.2329 - reconstruction loss: 0.0344128/3742 - total loss: 0.2518 - classification loss: 0.0089 - dann loss: 2.2327 - reconstruction loss: 0.0345128/3742 - total loss: 0.2517 - classification loss: 0.0091 - dann loss: 2.2317 - reconstruction loss: 0.0345128/3742 - total loss: 0.2517 - classification loss: 0.0091 - dann loss: 2.2309 - reconstruction loss: 0.0346128/3742 - total loss: 0.2514 - classification loss: 0.0092 - dann loss: 2.2284 - reconstruction loss: 0.0346128/3742 - total loss: 0.2516 - classification loss: 0.0091 - dann loss: 2.2305 - reconstruction loss: 0.0346128/3742 - total loss: 0.2519 - classification loss: 0.0091 - dann loss: 2.2331 - reconstruction loss: 0.0346128/3742 - total loss: 0.2518 - classification loss: 0.0091 - dann loss: 2.2324 - reconstruction loss: 0.0346128/3742 - total loss: 0.2518 - classification loss: 0.0091 - dann loss: 2.2315 - reconstruction loss: 0.0346128/3742 - total loss: 0.2514 - classification loss: 0.0090 - dann loss: 2.2280 - reconstruction loss: 0.0347128/3742 - total loss: 0.2512 - classification loss: 0.0089 - dann loss: 2.2257 - reconstruction loss: 0.0346128/3742 - total loss: 0.2510 - classification loss: 0.0088 - dann loss: 2.2246 - reconstruction loss: 0.0346128/3742 - total loss: 0.2511 - classification loss: 0.0087 - dann loss: 2.2248 - reconstruction loss: 0.0346128/3742 - total loss: 0.2513 - classification loss: 0.0089 - dann loss: 2.2261 - reconstruction loss: 0.034729/3742 - total loss: 0.2514 - classification loss: 0.0092 - dann loss: 2.2272 - reconstruction loss: 0.03470/3742 - total loss: 0.2514 - classification loss: 0.0092 - dann loss: 2.2272 - reconstruction loss: 0.0347Epoch 37/150, Current strat Epoch 37/100
use_perm = True
switching perm
128/3742 - total loss: 0.2538 - classification loss: 0.0151 - dann loss: 2.2389 - reconstruction loss: 0.0355128/3742 - total loss: 0.2532 - classification loss: 0.0116 - dann loss: 2.2320 - reconstruction loss: 0.0360128/3742 - total loss: 0.2516 - classification loss: 0.0110 - dann loss: 2.2169 - reconstruction loss: 0.0360128/3742 - total loss: 0.2523 - classification loss: 0.0102 - dann loss: 2.2268 - reconstruction loss: 0.0358128/3742 - total loss: 0.2510 - classification loss: 0.0097 - dann loss: 2.2150 - reconstruction loss: 0.0356128/3742 - total loss: 0.2498 - classification loss: 0.0093 - dann loss: 2.2053 - reconstruction loss: 0.0354128/3742 - total loss: 0.2498 - classification loss: 0.0093 - dann loss: 2.2036 - reconstruction loss: 0.0357128/3742 - total loss: 0.2501 - classification loss: 0.0095 - dann loss: 2.2081 - reconstruction loss: 0.0355128/3742 - total loss: 0.2497 - classification loss: 0.0091 - dann loss: 2.2046 - reconstruction loss: 0.0354128/3742 - total loss: 0.2504 - classification loss: 0.0094 - dann loss: 2.2115 - reconstruction loss: 0.0353128/3742 - total loss: 0.2503 - classification loss: 0.0090 - dann loss: 2.2104 - reconstruction loss: 0.0354128/3742 - total loss: 0.2497 - classification loss: 0.0088 - dann loss: 2.2065 - reconstruction loss: 0.0352128/3742 - total loss: 0.2499 - classification loss: 0.0092 - dann loss: 2.2107 - reconstruction loss: 0.0349128/3742 - total loss: 0.2501 - classification loss: 0.0092 - dann loss: 2.2122 - reconstruction loss: 0.0349128/3742 - total loss: 0.2502 - classification loss: 0.0094 - dann loss: 2.2128 - reconstruction loss: 0.0349128/3742 - total loss: 0.2503 - classification loss: 0.0093 - dann loss: 2.2154 - reconstruction loss: 0.0348128/3742 - total loss: 0.2504 - classification loss: 0.0092 - dann loss: 2.2169 - reconstruction loss: 0.0348128/3742 - total loss: 0.2505 - classification loss: 0.0094 - dann loss: 2.2177 - reconstruction loss: 0.0348128/3742 - total loss: 0.2504 - classification loss: 0.0092 - dann loss: 2.2168 - reconstruction loss: 0.0347128/3742 - total loss: 0.2502 - classification loss: 0.0091 - dann loss: 2.2155 - reconstruction loss: 0.0347128/3742 - total loss: 0.2502 - classification loss: 0.0089 - dann loss: 2.2157 - reconstruction loss: 0.0347128/3742 - total loss: 0.2502 - classification loss: 0.0087 - dann loss: 2.2163 - reconstruction loss: 0.0346128/3742 - total loss: 0.2499 - classification loss: 0.0087 - dann loss: 2.2136 - reconstruction loss: 0.0345128/3742 - total loss: 0.2500 - classification loss: 0.0090 - dann loss: 2.2138 - reconstruction loss: 0.0346128/3742 - total loss: 0.2499 - classification loss: 0.0089 - dann loss: 2.2132 - reconstruction loss: 0.0346128/3742 - total loss: 0.2498 - classification loss: 0.0089 - dann loss: 2.2116 - reconstruction loss: 0.0346128/3742 - total loss: 0.2496 - classification loss: 0.0087 - dann loss: 2.2095 - reconstruction loss: 0.0347128/3742 - total loss: 0.2496 - classification loss: 0.0088 - dann loss: 2.2100 - reconstruction loss: 0.0347128/3742 - total loss: 0.2492 - classification loss: 0.0087 - dann loss: 2.2057 - reconstruction loss: 0.034729/3742 - total loss: 0.2490 - classification loss: 0.0089 - dann loss: 2.2026 - reconstruction loss: 0.03480/3742 - total loss: 0.2490 - classification loss: 0.0089 - dann loss: 2.2026 - reconstruction loss: 0.0348Epoch 38/150, Current strat Epoch 38/100
use_perm = True
switching perm
128/3742 - total loss: 0.2436 - classification loss: 0.0044 - dann loss: 2.1399 - reconstruction loss: 0.0365128/3742 - total loss: 0.2462 - classification loss: 0.0049 - dann loss: 2.1722 - reconstruction loss: 0.0356128/3742 - total loss: 0.2476 - classification loss: 0.0065 - dann loss: 2.1904 - reconstruction loss: 0.0349128/3742 - total loss: 0.2472 - classification loss: 0.0062 - dann loss: 2.1860 - reconstruction loss: 0.0349128/3742 - total loss: 0.2468 - classification loss: 0.0058 - dann loss: 2.1817 - reconstruction loss: 0.0350128/3742 - total loss: 0.2469 - classification loss: 0.0062 - dann loss: 2.1848 - reconstruction loss: 0.0347128/3742 - total loss: 0.2472 - classification loss: 0.0069 - dann loss: 2.1856 - reconstruction loss: 0.0350128/3742 - total loss: 0.2475 - classification loss: 0.0072 - dann loss: 2.1879 - reconstruction loss: 0.0350128/3742 - total loss: 0.2482 - classification loss: 0.0072 - dann loss: 2.1964 - reconstruction loss: 0.0348128/3742 - total loss: 0.2489 - classification loss: 0.0072 - dann loss: 2.2049 - reconstruction loss: 0.0347128/3742 - total loss: 0.2488 - classification loss: 0.0074 - dann loss: 2.2034 - reconstruction loss: 0.0347128/3742 - total loss: 0.2489 - classification loss: 0.0072 - dann loss: 2.2038 - reconstruction loss: 0.0348128/3742 - total loss: 0.2488 - classification loss: 0.0071 - dann loss: 2.2036 - reconstruction loss: 0.0347128/3742 - total loss: 0.2486 - classification loss: 0.0070 - dann loss: 2.2014 - reconstruction loss: 0.0347128/3742 - total loss: 0.2484 - classification loss: 0.0077 - dann loss: 2.1986 - reconstruction loss: 0.0348128/3742 - total loss: 0.2481 - classification loss: 0.0076 - dann loss: 2.1958 - reconstruction loss: 0.0346128/3742 - total loss: 0.2476 - classification loss: 0.0075 - dann loss: 2.1926 - reconstruction loss: 0.0345128/3742 - total loss: 0.2477 - classification loss: 0.0075 - dann loss: 2.1934 - reconstruction loss: 0.0346128/3742 - total loss: 0.2477 - classification loss: 0.0077 - dann loss: 2.1924 - reconstruction loss: 0.0346128/3742 - total loss: 0.2477 - classification loss: 0.0077 - dann loss: 2.1926 - reconstruction loss: 0.0346128/3742 - total loss: 0.2474 - classification loss: 0.0077 - dann loss: 2.1899 - reconstruction loss: 0.0346128/3742 - total loss: 0.2475 - classification loss: 0.0075 - dann loss: 2.1905 - reconstruction loss: 0.0346128/3742 - total loss: 0.2474 - classification loss: 0.0075 - dann loss: 2.1901 - reconstruction loss: 0.0346128/3742 - total loss: 0.2477 - classification loss: 0.0074 - dann loss: 2.1925 - reconstruction loss: 0.0346128/3742 - total loss: 0.2478 - classification loss: 0.0074 - dann loss: 2.1930 - reconstruction loss: 0.0346128/3742 - total loss: 0.2481 - classification loss: 0.0074 - dann loss: 2.1964 - reconstruction loss: 0.0347128/3742 - total loss: 0.2485 - classification loss: 0.0075 - dann loss: 2.1994 - reconstruction loss: 0.0347128/3742 - total loss: 0.2484 - classification loss: 0.0074 - dann loss: 2.1986 - reconstruction loss: 0.0347128/3742 - total loss: 0.2482 - classification loss: 0.0075 - dann loss: 2.1974 - reconstruction loss: 0.034729/3742 - total loss: 0.2482 - classification loss: 0.0076 - dann loss: 2.1959 - reconstruction loss: 0.03480/3742 - total loss: 0.2482 - classification loss: 0.0076 - dann loss: 2.1959 - reconstruction loss: 0.0348Epoch 39/150, Current strat Epoch 39/100
use_perm = True
switching perm
128/3742 - total loss: 0.2483 - classification loss: 0.0099 - dann loss: 2.1982 - reconstruction loss: 0.0343128/3742 - total loss: 0.2479 - classification loss: 0.0085 - dann loss: 2.1966 - reconstruction loss: 0.0343128/3742 - total loss: 0.2490 - classification loss: 0.0085 - dann loss: 2.2070 - reconstruction loss: 0.0343128/3742 - total loss: 0.2497 - classification loss: 0.0076 - dann loss: 2.2146 - reconstruction loss: 0.0344128/3742 - total loss: 0.2495 - classification loss: 0.0074 - dann loss: 2.2127 - reconstruction loss: 0.0343128/3742 - total loss: 0.2495 - classification loss: 0.0074 - dann loss: 2.2125 - reconstruction loss: 0.0344128/3742 - total loss: 0.2488 - classification loss: 0.0074 - dann loss: 2.2073 - reconstruction loss: 0.0342128/3742 - total loss: 0.2490 - classification loss: 0.0073 - dann loss: 2.2086 - reconstruction loss: 0.0343128/3742 - total loss: 0.2486 - classification loss: 0.0075 - dann loss: 2.2044 - reconstruction loss: 0.0343128/3742 - total loss: 0.2486 - classification loss: 0.0074 - dann loss: 2.2036 - reconstruction loss: 0.0344128/3742 - total loss: 0.2488 - classification loss: 0.0071 - dann loss: 2.2055 - reconstruction loss: 0.0345128/3742 - total loss: 0.2487 - classification loss: 0.0071 - dann loss: 2.2052 - reconstruction loss: 0.0343128/3742 - total loss: 0.2488 - classification loss: 0.0071 - dann loss: 2.2064 - reconstruction loss: 0.0343128/3742 - total loss: 0.2488 - classification loss: 0.0070 - dann loss: 2.2072 - reconstruction loss: 0.0342128/3742 - total loss: 0.2481 - classification loss: 0.0069 - dann loss: 2.2000 - reconstruction loss: 0.0343128/3742 - total loss: 0.2484 - classification loss: 0.0069 - dann loss: 2.2018 - reconstruction loss: 0.0344128/3742 - total loss: 0.2484 - classification loss: 0.0070 - dann loss: 2.2016 - reconstruction loss: 0.0345128/3742 - total loss: 0.2483 - classification loss: 0.0070 - dann loss: 2.2009 - reconstruction loss: 0.0344128/3742 - total loss: 0.2482 - classification loss: 0.0070 - dann loss: 2.1996 - reconstruction loss: 0.0345128/3742 - total loss: 0.2484 - classification loss: 0.0072 - dann loss: 2.2007 - reconstruction loss: 0.0346128/3742 - total loss: 0.2485 - classification loss: 0.0073 - dann loss: 2.2018 - reconstruction loss: 0.0345128/3742 - total loss: 0.2485 - classification loss: 0.0073 - dann loss: 2.2017 - reconstruction loss: 0.0345128/3742 - total loss: 0.2485 - classification loss: 0.0072 - dann loss: 2.2010 - reconstruction loss: 0.0346128/3742 - total loss: 0.2483 - classification loss: 0.0070 - dann loss: 2.1991 - reconstruction loss: 0.0346128/3742 - total loss: 0.2483 - classification loss: 0.0070 - dann loss: 2.1988 - reconstruction loss: 0.0346128/3742 - total loss: 0.2483 - classification loss: 0.0075 - dann loss: 2.1991 - reconstruction loss: 0.0346128/3742 - total loss: 0.2479 - classification loss: 0.0074 - dann loss: 2.1949 - reconstruction loss: 0.0346128/3742 - total loss: 0.2475 - classification loss: 0.0074 - dann loss: 2.1909 - reconstruction loss: 0.0346128/3742 - total loss: 0.2475 - classification loss: 0.0073 - dann loss: 2.1910 - reconstruction loss: 0.034629/3742 - total loss: 0.2484 - classification loss: 0.0153 - dann loss: 2.1924 - reconstruction loss: 0.03450/3742 - total loss: 0.2484 - classification loss: 0.0153 - dann loss: 2.1924 - reconstruction loss: 0.0345Epoch 40/150, Current strat Epoch 40/100
use_perm = True
switching perm
128/3742 - total loss: 0.2436 - classification loss: 0.0045 - dann loss: 2.1657 - reconstruction loss: 0.0332128/3742 - total loss: 0.2437 - classification loss: 0.0050 - dann loss: 2.1581 - reconstruction loss: 0.0343128/3742 - total loss: 0.2480 - classification loss: 0.0260 - dann loss: 2.1753 - reconstruction loss: 0.0348128/3742 - total loss: 0.2458 - classification loss: 0.0213 - dann loss: 2.1638 - reconstruction loss: 0.0342128/3742 - total loss: 0.2448 - classification loss: 0.0201 - dann loss: 2.1520 - reconstruction loss: 0.0345128/3742 - total loss: 0.2454 - classification loss: 0.0212 - dann loss: 2.1582 - reconstruction loss: 0.0343128/3742 - total loss: 0.2465 - classification loss: 0.0189 - dann loss: 2.1733 - reconstruction loss: 0.0341128/3742 - total loss: 0.2471 - classification loss: 0.0221 - dann loss: 2.1741 - reconstruction loss: 0.0343128/3742 - total loss: 0.2474 - classification loss: 0.0340 - dann loss: 2.1662 - reconstruction loss: 0.0343128/3742 - total loss: 0.2481 - classification loss: 0.0317 - dann loss: 2.1740 - reconstruction loss: 0.0344128/3742 - total loss: 0.2477 - classification loss: 0.0297 - dann loss: 2.1714 - reconstruction loss: 0.0345128/3742 - total loss: 0.2484 - classification loss: 0.0280 - dann loss: 2.1793 - reconstruction loss: 0.0346128/3742 - total loss: 0.2484 - classification loss: 0.0264 - dann loss: 2.1811 - reconstruction loss: 0.0346128/3742 - total loss: 0.2482 - classification loss: 0.0250 - dann loss: 2.1800 - reconstruction loss: 0.0346128/3742 - total loss: 0.2478 - classification loss: 0.0235 - dann loss: 2.1776 - reconstruction loss: 0.0346128/3742 - total loss: 0.2474 - classification loss: 0.0230 - dann loss: 2.1745 - reconstruction loss: 0.0346128/3742 - total loss: 0.2477 - classification loss: 0.0224 - dann loss: 2.1785 - reconstruction loss: 0.0345128/3742 - total loss: 0.2473 - classification loss: 0.0214 - dann loss: 2.1761 - reconstruction loss: 0.0345128/3742 - total loss: 0.2475 - classification loss: 0.0205 - dann loss: 2.1785 - reconstruction loss: 0.0345128/3742 - total loss: 0.2472 - classification loss: 0.0196 - dann loss: 2.1768 - reconstruction loss: 0.0344128/3742 - total loss: 0.2471 - classification loss: 0.0191 - dann loss: 2.1770 - reconstruction loss: 0.0344128/3742 - total loss: 0.2477 - classification loss: 0.0251 - dann loss: 2.1765 - reconstruction loss: 0.0345128/3742 - total loss: 0.2477 - classification loss: 0.0241 - dann loss: 2.1767 - reconstruction loss: 0.0345128/3742 - total loss: 0.2479 - classification loss: 0.0256 - dann loss: 2.1772 - reconstruction loss: 0.0345128/3742 - total loss: 0.2477 - classification loss: 0.0247 - dann loss: 2.1765 - reconstruction loss: 0.0344128/3742 - total loss: 0.2476 - classification loss: 0.0239 - dann loss: 2.1761 - reconstruction loss: 0.0345128/3742 - total loss: 0.2478 - classification loss: 0.0232 - dann loss: 2.1788 - reconstruction loss: 0.0345128/3742 - total loss: 0.2476 - classification loss: 0.0225 - dann loss: 2.1767 - reconstruction loss: 0.0345128/3742 - total loss: 0.2474 - classification loss: 0.0221 - dann loss: 2.1762 - reconstruction loss: 0.034529/3742 - total loss: 0.2477 - classification loss: 0.0218 - dann loss: 2.1787 - reconstruction loss: 0.03450/3742 - total loss: 0.2477 - classification loss: 0.0218 - dann loss: 2.1787 - reconstruction loss: 0.0345Epoch 41/150, Current strat Epoch 41/100
use_perm = True
switching perm
128/3742 - total loss: 0.2536 - classification loss: 0.0658 - dann loss: 2.2041 - reconstruction loss: 0.0332128/3742 - total loss: 0.2494 - classification loss: 0.0344 - dann loss: 2.1872 - reconstruction loss: 0.0340128/3742 - total loss: 0.2482 - classification loss: 0.0242 - dann loss: 2.1878 - reconstruction loss: 0.0338128/3742 - total loss: 0.2479 - classification loss: 0.0203 - dann loss: 2.1903 - reconstruction loss: 0.0335128/3742 - total loss: 0.2468 - classification loss: 0.0183 - dann loss: 2.1799 - reconstruction loss: 0.0337128/3742 - total loss: 0.2471 - classification loss: 0.0163 - dann loss: 2.1847 - reconstruction loss: 0.0338128/3742 - total loss: 0.2468 - classification loss: 0.0145 - dann loss: 2.1820 - reconstruction loss: 0.0340128/3742 - total loss: 0.2470 - classification loss: 0.0130 - dann loss: 2.1859 - reconstruction loss: 0.0339128/3742 - total loss: 0.2476 - classification loss: 0.0122 - dann loss: 2.1915 - reconstruction loss: 0.0340128/3742 - total loss: 0.2473 - classification loss: 0.0113 - dann loss: 2.1895 - reconstruction loss: 0.0340128/3742 - total loss: 0.2478 - classification loss: 0.0108 - dann loss: 2.1951 - reconstruction loss: 0.0340128/3742 - total loss: 0.2478 - classification loss: 0.0104 - dann loss: 2.1960 - reconstruction loss: 0.0340128/3742 - total loss: 0.2480 - classification loss: 0.0136 - dann loss: 2.1936 - reconstruction loss: 0.0342128/3742 - total loss: 0.2476 - classification loss: 0.0131 - dann loss: 2.1889 - reconstruction loss: 0.0343128/3742 - total loss: 0.2477 - classification loss: 0.0125 - dann loss: 2.1898 - reconstruction loss: 0.0343128/3742 - total loss: 0.2470 - classification loss: 0.0120 - dann loss: 2.1839 - reconstruction loss: 0.0342128/3742 - total loss: 0.2465 - classification loss: 0.0115 - dann loss: 2.1790 - reconstruction loss: 0.0343128/3742 - total loss: 0.2463 - classification loss: 0.0116 - dann loss: 2.1763 - reconstruction loss: 0.0344128/3742 - total loss: 0.2459 - classification loss: 0.0113 - dann loss: 2.1731 - reconstruction loss: 0.0344128/3742 - total loss: 0.2464 - classification loss: 0.0110 - dann loss: 2.1780 - reconstruction loss: 0.0344128/3742 - total loss: 0.2467 - classification loss: 0.0107 - dann loss: 2.1810 - reconstruction loss: 0.0344128/3742 - total loss: 0.2465 - classification loss: 0.0105 - dann loss: 2.1781 - reconstruction loss: 0.0345128/3742 - total loss: 0.2468 - classification loss: 0.0107 - dann loss: 2.1800 - reconstruction loss: 0.0346128/3742 - total loss: 0.2465 - classification loss: 0.0106 - dann loss: 2.1776 - reconstruction loss: 0.0346128/3742 - total loss: 0.2466 - classification loss: 0.0103 - dann loss: 2.1791 - reconstruction loss: 0.0346128/3742 - total loss: 0.2467 - classification loss: 0.0101 - dann loss: 2.1800 - reconstruction loss: 0.0346128/3742 - total loss: 0.2468 - classification loss: 0.0101 - dann loss: 2.1819 - reconstruction loss: 0.0345128/3742 - total loss: 0.2466 - classification loss: 0.0100 - dann loss: 2.1802 - reconstruction loss: 0.0345128/3742 - total loss: 0.2467 - classification loss: 0.0101 - dann loss: 2.1809 - reconstruction loss: 0.034529/3742 - total loss: 0.2475 - classification loss: 0.0169 - dann loss: 2.1810 - reconstruction loss: 0.03460/3742 - total loss: 0.2475 - classification loss: 0.0169 - dann loss: 2.1810 - reconstruction loss: 0.0346Epoch 42/150, Current strat Epoch 42/100
use_perm = True
switching perm
128/3742 - total loss: 0.2471 - classification loss: 0.0051 - dann loss: 2.1927 - reconstruction loss: 0.0341128/3742 - total loss: 0.2475 - classification loss: 0.0039 - dann loss: 2.1921 - reconstruction loss: 0.0349128/3742 - total loss: 0.2483 - classification loss: 0.0043 - dann loss: 2.2012 - reconstruction loss: 0.0347128/3742 - total loss: 0.2475 - classification loss: 0.0045 - dann loss: 2.1951 - reconstruction loss: 0.0344128/3742 - total loss: 0.2481 - classification loss: 0.0043 - dann loss: 2.2013 - reconstruction loss: 0.0344128/3742 - total loss: 0.2470 - classification loss: 0.0043 - dann loss: 2.1903 - reconstruction loss: 0.0345128/3742 - total loss: 0.2478 - classification loss: 0.0051 - dann loss: 2.1965 - reconstruction loss: 0.0345128/3742 - total loss: 0.2473 - classification loss: 0.0072 - dann loss: 2.1892 - reconstruction loss: 0.0345128/3742 - total loss: 0.2472 - classification loss: 0.0077 - dann loss: 2.1878 - reconstruction loss: 0.0346128/3742 - total loss: 0.2469 - classification loss: 0.0074 - dann loss: 2.1832 - reconstruction loss: 0.0348128/3742 - total loss: 0.2470 - classification loss: 0.0082 - dann loss: 2.1847 - reconstruction loss: 0.0347128/3742 - total loss: 0.2467 - classification loss: 0.0083 - dann loss: 2.1814 - reconstruction loss: 0.0347128/3742 - total loss: 0.2466 - classification loss: 0.0084 - dann loss: 2.1800 - reconstruction loss: 0.0347128/3742 - total loss: 0.2468 - classification loss: 0.0112 - dann loss: 2.1777 - reconstruction loss: 0.0349128/3742 - total loss: 0.2467 - classification loss: 0.0109 - dann loss: 2.1778 - reconstruction loss: 0.0348128/3742 - total loss: 0.2471 - classification loss: 0.0107 - dann loss: 2.1810 - reconstruction loss: 0.0349128/3742 - total loss: 0.2471 - classification loss: 0.0105 - dann loss: 2.1817 - reconstruction loss: 0.0349128/3742 - total loss: 0.2471 - classification loss: 0.0105 - dann loss: 2.1800 - reconstruction loss: 0.0350128/3742 - total loss: 0.2466 - classification loss: 0.0107 - dann loss: 2.1759 - reconstruction loss: 0.0349128/3742 - total loss: 0.2459 - classification loss: 0.0104 - dann loss: 2.1702 - reconstruction loss: 0.0348128/3742 - total loss: 0.2459 - classification loss: 0.0102 - dann loss: 2.1706 - reconstruction loss: 0.0348128/3742 - total loss: 0.2462 - classification loss: 0.0115 - dann loss: 2.1724 - reconstruction loss: 0.0347128/3742 - total loss: 0.2463 - classification loss: 0.0112 - dann loss: 2.1741 - reconstruction loss: 0.0347128/3742 - total loss: 0.2463 - classification loss: 0.0110 - dann loss: 2.1746 - reconstruction loss: 0.0347128/3742 - total loss: 0.2460 - classification loss: 0.0112 - dann loss: 2.1716 - reconstruction loss: 0.0347128/3742 - total loss: 0.2464 - classification loss: 0.0112 - dann loss: 2.1751 - reconstruction loss: 0.0347128/3742 - total loss: 0.2463 - classification loss: 0.0111 - dann loss: 2.1743 - reconstruction loss: 0.0347128/3742 - total loss: 0.2462 - classification loss: 0.0112 - dann loss: 2.1729 - reconstruction loss: 0.0347128/3742 - total loss: 0.2463 - classification loss: 0.0110 - dann loss: 2.1745 - reconstruction loss: 0.034629/3742 - total loss: 0.2464 - classification loss: 0.0110 - dann loss: 2.1764 - reconstruction loss: 0.03460/3742 - total loss: 0.2464 - classification loss: 0.0110 - dann loss: 2.1764 - reconstruction loss: 0.0346Epoch 43/150, Current strat Epoch 43/100
use_perm = True
switching perm
128/3742 - total loss: 0.2505 - classification loss: 0.0044 - dann loss: 2.2190 - reconstruction loss: 0.0352128/3742 - total loss: 0.2501 - classification loss: 0.0036 - dann loss: 2.2236 - reconstruction loss: 0.0343128/3742 - total loss: 0.2492 - classification loss: 0.0038 - dann loss: 2.2141 - reconstruction loss: 0.0342128/3742 - total loss: 0.2483 - classification loss: 0.0038 - dann loss: 2.2003 - reconstruction loss: 0.0349128/3742 - total loss: 0.2485 - classification loss: 0.0045 - dann loss: 2.2033 - reconstruction loss: 0.0346128/3742 - total loss: 0.2488 - classification loss: 0.0043 - dann loss: 2.2048 - reconstruction loss: 0.0349128/3742 - total loss: 0.2484 - classification loss: 0.0043 - dann loss: 2.2026 - reconstruction loss: 0.0346128/3742 - total loss: 0.2488 - classification loss: 0.0041 - dann loss: 2.2066 - reconstruction loss: 0.0346128/3742 - total loss: 0.2488 - classification loss: 0.0041 - dann loss: 2.2076 - reconstruction loss: 0.0345128/3742 - total loss: 0.2487 - classification loss: 0.0048 - dann loss: 2.2066 - reconstruction loss: 0.0345128/3742 - total loss: 0.2485 - classification loss: 0.0048 - dann loss: 2.2046 - reconstruction loss: 0.0345128/3742 - total loss: 0.2492 - classification loss: 0.0055 - dann loss: 2.2097 - reconstruction loss: 0.0346128/3742 - total loss: 0.2502 - classification loss: 0.0056 - dann loss: 2.2178 - reconstruction loss: 0.0348128/3742 - total loss: 0.2504 - classification loss: 0.0061 - dann loss: 2.2191 - reconstruction loss: 0.0348128/3742 - total loss: 0.2499 - classification loss: 0.0062 - dann loss: 2.2136 - reconstruction loss: 0.0349128/3742 - total loss: 0.2501 - classification loss: 0.0066 - dann loss: 2.2171 - reconstruction loss: 0.0347128/3742 - total loss: 0.2501 - classification loss: 0.0067 - dann loss: 2.2173 - reconstruction loss: 0.0347128/3742 - total loss: 0.2508 - classification loss: 0.0075 - dann loss: 2.2225 - reconstruction loss: 0.0347128/3742 - total loss: 0.2510 - classification loss: 0.0075 - dann loss: 2.2245 - reconstruction loss: 0.0348128/3742 - total loss: 0.2510 - classification loss: 0.0079 - dann loss: 2.2242 - reconstruction loss: 0.0347128/3742 - total loss: 0.2510 - classification loss: 0.0078 - dann loss: 2.2242 - reconstruction loss: 0.0347128/3742 - total loss: 0.2510 - classification loss: 0.0079 - dann loss: 2.2251 - reconstruction loss: 0.0347128/3742 - total loss: 0.2508 - classification loss: 0.0081 - dann loss: 2.2228 - reconstruction loss: 0.0347128/3742 - total loss: 0.2505 - classification loss: 0.0079 - dann loss: 2.2193 - reconstruction loss: 0.0347128/3742 - total loss: 0.2507 - classification loss: 0.0079 - dann loss: 2.2212 - reconstruction loss: 0.0347128/3742 - total loss: 0.2505 - classification loss: 0.0077 - dann loss: 2.2196 - reconstruction loss: 0.0347128/3742 - total loss: 0.2503 - classification loss: 0.0077 - dann loss: 2.2181 - reconstruction loss: 0.0347128/3742 - total loss: 0.2506 - classification loss: 0.0082 - dann loss: 2.2207 - reconstruction loss: 0.0347128/3742 - total loss: 0.2507 - classification loss: 0.0083 - dann loss: 2.2213 - reconstruction loss: 0.034629/3742 - total loss: 0.2506 - classification loss: 0.0086 - dann loss: 2.2209 - reconstruction loss: 0.03450/3742 - total loss: 0.2506 - classification loss: 0.0086 - dann loss: 2.2209 - reconstruction loss: 0.0345Epoch 44/150, Current strat Epoch 44/100
use_perm = True
switching perm
128/3742 - total loss: 0.2500 - classification loss: 0.0075 - dann loss: 2.2357 - reconstruction loss: 0.0321128/3742 - total loss: 0.2522 - classification loss: 0.0098 - dann loss: 2.2436 - reconstruction loss: 0.0336128/3742 - total loss: 0.2515 - classification loss: 0.0092 - dann loss: 2.2399 - reconstruction loss: 0.0333128/3742 - total loss: 0.2515 - classification loss: 0.0094 - dann loss: 2.2382 - reconstruction loss: 0.0334128/3742 - total loss: 0.2513 - classification loss: 0.0084 - dann loss: 2.2319 - reconstruction loss: 0.0341128/3742 - total loss: 0.2514 - classification loss: 0.0094 - dann loss: 2.2319 - reconstruction loss: 0.0341128/3742 - total loss: 0.2512 - classification loss: 0.0092 - dann loss: 2.2287 - reconstruction loss: 0.0342128/3742 - total loss: 0.2520 - classification loss: 0.0112 - dann loss: 2.2342 - reconstruction loss: 0.0343128/3742 - total loss: 0.2521 - classification loss: 0.0112 - dann loss: 2.2336 - reconstruction loss: 0.0345128/3742 - total loss: 0.2517 - classification loss: 0.0105 - dann loss: 2.2303 - reconstruction loss: 0.0345128/3742 - total loss: 0.2515 - classification loss: 0.0108 - dann loss: 2.2297 - reconstruction loss: 0.0342128/3742 - total loss: 0.2513 - classification loss: 0.0105 - dann loss: 2.2284 - reconstruction loss: 0.0342128/3742 - total loss: 0.2520 - classification loss: 0.0105 - dann loss: 2.2345 - reconstruction loss: 0.0344128/3742 - total loss: 0.2519 - classification loss: 0.0103 - dann loss: 2.2348 - reconstruction loss: 0.0343128/3742 - total loss: 0.2519 - classification loss: 0.0112 - dann loss: 2.2327 - reconstruction loss: 0.0344128/3742 - total loss: 0.2516 - classification loss: 0.0117 - dann loss: 2.2294 - reconstruction loss: 0.0344128/3742 - total loss: 0.2518 - classification loss: 0.0117 - dann loss: 2.2295 - reconstruction loss: 0.0346128/3742 - total loss: 0.2517 - classification loss: 0.0119 - dann loss: 2.2279 - reconstruction loss: 0.0347128/3742 - total loss: 0.2518 - classification loss: 0.0116 - dann loss: 2.2288 - reconstruction loss: 0.0347128/3742 - total loss: 0.2517 - classification loss: 0.0116 - dann loss: 2.2280 - reconstruction loss: 0.0346128/3742 - total loss: 0.2515 - classification loss: 0.0114 - dann loss: 2.2266 - reconstruction loss: 0.0347128/3742 - total loss: 0.2516 - classification loss: 0.0113 - dann loss: 2.2278 - reconstruction loss: 0.0347128/3742 - total loss: 0.2520 - classification loss: 0.0114 - dann loss: 2.2313 - reconstruction loss: 0.0346128/3742 - total loss: 0.2521 - classification loss: 0.0113 - dann loss: 2.2324 - reconstruction loss: 0.0346128/3742 - total loss: 0.2521 - classification loss: 0.0112 - dann loss: 2.2326 - reconstruction loss: 0.0346128/3742 - total loss: 0.2521 - classification loss: 0.0112 - dann loss: 2.2325 - reconstruction loss: 0.0346128/3742 - total loss: 0.2521 - classification loss: 0.0110 - dann loss: 2.2328 - reconstruction loss: 0.0346128/3742 - total loss: 0.2520 - classification loss: 0.0109 - dann loss: 2.2327 - reconstruction loss: 0.0346128/3742 - total loss: 0.2518 - classification loss: 0.0109 - dann loss: 2.2302 - reconstruction loss: 0.034629/3742 - total loss: 0.2517 - classification loss: 0.0107 - dann loss: 2.2284 - reconstruction loss: 0.03470/3742 - total loss: 0.2517 - classification loss: 0.0107 - dann loss: 2.2284 - reconstruction loss: 0.0347Epoch 45/150, Current strat Epoch 45/100
use_perm = True
switching perm
128/3742 - total loss: 0.2549 - classification loss: 0.0136 - dann loss: 2.2685 - reconstruction loss: 0.0333128/3742 - total loss: 0.2518 - classification loss: 0.0145 - dann loss: 2.2306 - reconstruction loss: 0.0342128/3742 - total loss: 0.2533 - classification loss: 0.0143 - dann loss: 2.2377 - reconstruction loss: 0.0351128/3742 - total loss: 0.2518 - classification loss: 0.0120 - dann loss: 2.2260 - reconstruction loss: 0.0350128/3742 - total loss: 0.2504 - classification loss: 0.0106 - dann loss: 2.2152 - reconstruction loss: 0.0347128/3742 - total loss: 0.2493 - classification loss: 0.0096 - dann loss: 2.2057 - reconstruction loss: 0.0348128/3742 - total loss: 0.2496 - classification loss: 0.0098 - dann loss: 2.2100 - reconstruction loss: 0.0346128/3742 - total loss: 0.2492 - classification loss: 0.0092 - dann loss: 2.2070 - reconstruction loss: 0.0345128/3742 - total loss: 0.2489 - classification loss: 0.0091 - dann loss: 2.2046 - reconstruction loss: 0.0345128/3742 - total loss: 0.2490 - classification loss: 0.0094 - dann loss: 2.2030 - reconstruction loss: 0.0347128/3742 - total loss: 0.2492 - classification loss: 0.0099 - dann loss: 2.2044 - reconstruction loss: 0.0347128/3742 - total loss: 0.2493 - classification loss: 0.0105 - dann loss: 2.2055 - reconstruction loss: 0.0346128/3742 - total loss: 0.2496 - classification loss: 0.0104 - dann loss: 2.2082 - reconstruction loss: 0.0347128/3742 - total loss: 0.2496 - classification loss: 0.0102 - dann loss: 2.2095 - reconstruction loss: 0.0346128/3742 - total loss: 0.2503 - classification loss: 0.0121 - dann loss: 2.2136 - reconstruction loss: 0.0347128/3742 - total loss: 0.2500 - classification loss: 0.0118 - dann loss: 2.2109 - reconstruction loss: 0.0347128/3742 - total loss: 0.2501 - classification loss: 0.0116 - dann loss: 2.2119 - reconstruction loss: 0.0347128/3742 - total loss: 0.2500 - classification loss: 0.0121 - dann loss: 2.2105 - reconstruction loss: 0.0347128/3742 - total loss: 0.2504 - classification loss: 0.0137 - dann loss: 2.2121 - reconstruction loss: 0.0347128/3742 - total loss: 0.2506 - classification loss: 0.0137 - dann loss: 2.2144 - reconstruction loss: 0.0348128/3742 - total loss: 0.2504 - classification loss: 0.0134 - dann loss: 2.2126 - reconstruction loss: 0.0348128/3742 - total loss: 0.2501 - classification loss: 0.0130 - dann loss: 2.2105 - reconstruction loss: 0.0347128/3742 - total loss: 0.2501 - classification loss: 0.0127 - dann loss: 2.2112 - reconstruction loss: 0.0346128/3742 - total loss: 0.2500 - classification loss: 0.0127 - dann loss: 2.2112 - reconstruction loss: 0.0345128/3742 - total loss: 0.2497 - classification loss: 0.0124 - dann loss: 2.2084 - reconstruction loss: 0.0345128/3742 - total loss: 0.2495 - classification loss: 0.0122 - dann loss: 2.2065 - reconstruction loss: 0.0345128/3742 - total loss: 0.2492 - classification loss: 0.0119 - dann loss: 2.2036 - reconstruction loss: 0.0345128/3742 - total loss: 0.2491 - classification loss: 0.0118 - dann loss: 2.2025 - reconstruction loss: 0.0346128/3742 - total loss: 0.2490 - classification loss: 0.0118 - dann loss: 2.2010 - reconstruction loss: 0.034729/3742 - total loss: 0.2494 - classification loss: 0.0186 - dann loss: 2.1989 - reconstruction loss: 0.03450/3742 - total loss: 0.2494 - classification loss: 0.0186 - dann loss: 2.1989 - reconstruction loss: 0.0345Epoch 46/150, Current strat Epoch 46/100
use_perm = True
switching perm
128/3742 - total loss: 0.2477 - classification loss: 0.0046 - dann loss: 2.2097 - reconstruction loss: 0.0328128/3742 - total loss: 0.2479 - classification loss: 0.0050 - dann loss: 2.2053 - reconstruction loss: 0.0336128/3742 - total loss: 0.2436 - classification loss: 0.0053 - dann loss: 2.1629 - reconstruction loss: 0.0335128/3742 - total loss: 0.2461 - classification loss: 0.0072 - dann loss: 2.1868 - reconstruction loss: 0.0334128/3742 - total loss: 0.2466 - classification loss: 0.0066 - dann loss: 2.1894 - reconstruction loss: 0.0338128/3742 - total loss: 0.2451 - classification loss: 0.0062 - dann loss: 2.1737 - reconstruction loss: 0.0339128/3742 - total loss: 0.2458 - classification loss: 0.0069 - dann loss: 2.1779 - reconstruction loss: 0.0342128/3742 - total loss: 0.2460 - classification loss: 0.0066 - dann loss: 2.1798 - reconstruction loss: 0.0342128/3742 - total loss: 0.2451 - classification loss: 0.0070 - dann loss: 2.1698 - reconstruction loss: 0.0343128/3742 - total loss: 0.2458 - classification loss: 0.0073 - dann loss: 2.1758 - reconstruction loss: 0.0344128/3742 - total loss: 0.2462 - classification loss: 0.0076 - dann loss: 2.1799 - reconstruction loss: 0.0343128/3742 - total loss: 0.2463 - classification loss: 0.0096 - dann loss: 2.1789 - reconstruction loss: 0.0343128/3742 - total loss: 0.2466 - classification loss: 0.0135 - dann loss: 2.1783 - reconstruction loss: 0.0343128/3742 - total loss: 0.2471 - classification loss: 0.0130 - dann loss: 2.1824 - reconstruction loss: 0.0344128/3742 - total loss: 0.2469 - classification loss: 0.0127 - dann loss: 2.1815 - reconstruction loss: 0.0344128/3742 - total loss: 0.2468 - classification loss: 0.0122 - dann loss: 2.1802 - reconstruction loss: 0.0344128/3742 - total loss: 0.2467 - classification loss: 0.0126 - dann loss: 2.1788 - reconstruction loss: 0.0344128/3742 - total loss: 0.2466 - classification loss: 0.0126 - dann loss: 2.1772 - reconstruction loss: 0.0345128/3742 - total loss: 0.2462 - classification loss: 0.0127 - dann loss: 2.1727 - reconstruction loss: 0.0345128/3742 - total loss: 0.2461 - classification loss: 0.0142 - dann loss: 2.1704 - reconstruction loss: 0.0346128/3742 - total loss: 0.2460 - classification loss: 0.0137 - dann loss: 2.1702 - reconstruction loss: 0.0345128/3742 - total loss: 0.2458 - classification loss: 0.0133 - dann loss: 2.1689 - reconstruction loss: 0.0345128/3742 - total loss: 0.2456 - classification loss: 0.0129 - dann loss: 2.1672 - reconstruction loss: 0.0345128/3742 - total loss: 0.2456 - classification loss: 0.0132 - dann loss: 2.1670 - reconstruction loss: 0.0345128/3742 - total loss: 0.2456 - classification loss: 0.0130 - dann loss: 2.1665 - reconstruction loss: 0.0346128/3742 - total loss: 0.2454 - classification loss: 0.0128 - dann loss: 2.1647 - reconstruction loss: 0.0346128/3742 - total loss: 0.2454 - classification loss: 0.0127 - dann loss: 2.1644 - reconstruction loss: 0.0346128/3742 - total loss: 0.2455 - classification loss: 0.0125 - dann loss: 2.1653 - reconstruction loss: 0.0346128/3742 - total loss: 0.2454 - classification loss: 0.0122 - dann loss: 2.1653 - reconstruction loss: 0.034629/3742 - total loss: 0.2451 - classification loss: 0.0125 - dann loss: 2.1617 - reconstruction loss: 0.03470/3742 - total loss: 0.2451 - classification loss: 0.0125 - dann loss: 2.1617 - reconstruction loss: 0.0347Epoch 47/150, Current strat Epoch 47/100
use_perm = True
switching perm
128/3742 - total loss: 0.2362 - classification loss: 0.0034 - dann loss: 2.0877 - reconstruction loss: 0.0339128/3742 - total loss: 0.2408 - classification loss: 0.0042 - dann loss: 2.1268 - reconstruction loss: 0.0346128/3742 - total loss: 0.2403 - classification loss: 0.0056 - dann loss: 2.1242 - reconstruction loss: 0.0342128/3742 - total loss: 0.2406 - classification loss: 0.0060 - dann loss: 2.1204 - reconstruction loss: 0.0350128/3742 - total loss: 0.2409 - classification loss: 0.0055 - dann loss: 2.1239 - reconstruction loss: 0.0350128/3742 - total loss: 0.2408 - classification loss: 0.0056 - dann loss: 2.1216 - reconstruction loss: 0.0351128/3742 - total loss: 0.2415 - classification loss: 0.0068 - dann loss: 2.1294 - reconstruction loss: 0.0348128/3742 - total loss: 0.2419 - classification loss: 0.0071 - dann loss: 2.1363 - reconstruction loss: 0.0345128/3742 - total loss: 0.2411 - classification loss: 0.0067 - dann loss: 2.1267 - reconstruction loss: 0.0347128/3742 - total loss: 0.2416 - classification loss: 0.0069 - dann loss: 2.1316 - reconstruction loss: 0.0347128/3742 - total loss: 0.2415 - classification loss: 0.0067 - dann loss: 2.1301 - reconstruction loss: 0.0347128/3742 - total loss: 0.2422 - classification loss: 0.0065 - dann loss: 2.1387 - reconstruction loss: 0.0346128/3742 - total loss: 0.2422 - classification loss: 0.0062 - dann loss: 2.1385 - reconstruction loss: 0.0347128/3742 - total loss: 0.2429 - classification loss: 0.0061 - dann loss: 2.1451 - reconstruction loss: 0.0348128/3742 - total loss: 0.2427 - classification loss: 0.0059 - dann loss: 2.1440 - reconstruction loss: 0.0347128/3742 - total loss: 0.2428 - classification loss: 0.0060 - dann loss: 2.1440 - reconstruction loss: 0.0347128/3742 - total loss: 0.2427 - classification loss: 0.0062 - dann loss: 2.1429 - reconstruction loss: 0.0347128/3742 - total loss: 0.2425 - classification loss: 0.0061 - dann loss: 2.1416 - reconstruction loss: 0.0346128/3742 - total loss: 0.2427 - classification loss: 0.0060 - dann loss: 2.1436 - reconstruction loss: 0.0346128/3742 - total loss: 0.2426 - classification loss: 0.0059 - dann loss: 2.1427 - reconstruction loss: 0.0346128/3742 - total loss: 0.2423 - classification loss: 0.0058 - dann loss: 2.1410 - reconstruction loss: 0.0345128/3742 - total loss: 0.2424 - classification loss: 0.0058 - dann loss: 2.1415 - reconstruction loss: 0.0345128/3742 - total loss: 0.2426 - classification loss: 0.0057 - dann loss: 2.1438 - reconstruction loss: 0.0345128/3742 - total loss: 0.2424 - classification loss: 0.0056 - dann loss: 2.1422 - reconstruction loss: 0.0345128/3742 - total loss: 0.2423 - classification loss: 0.0055 - dann loss: 2.1412 - reconstruction loss: 0.0345128/3742 - total loss: 0.2422 - classification loss: 0.0055 - dann loss: 2.1395 - reconstruction loss: 0.0346128/3742 - total loss: 0.2424 - classification loss: 0.0054 - dann loss: 2.1419 - reconstruction loss: 0.0345128/3742 - total loss: 0.2425 - classification loss: 0.0054 - dann loss: 2.1436 - reconstruction loss: 0.0345128/3742 - total loss: 0.2424 - classification loss: 0.0054 - dann loss: 2.1427 - reconstruction loss: 0.034529/3742 - total loss: 0.2422 - classification loss: 0.0055 - dann loss: 2.1394 - reconstruction loss: 0.03470/3742 - total loss: 0.2422 - classification loss: 0.0055 - dann loss: 2.1394 - reconstruction loss: 0.0347Epoch 48/150, Current strat Epoch 48/100
use_perm = True
switching perm
128/3742 - total loss: 0.2446 - classification loss: 0.0059 - dann loss: 2.1659 - reconstruction loss: 0.0342128/3742 - total loss: 0.2469 - classification loss: 0.0058 - dann loss: 2.1831 - reconstruction loss: 0.0350128/3742 - total loss: 0.2457 - classification loss: 0.0053 - dann loss: 2.1725 - reconstruction loss: 0.0349128/3742 - total loss: 0.2435 - classification loss: 0.0053 - dann loss: 2.1507 - reconstruction loss: 0.0349128/3742 - total loss: 0.2444 - classification loss: 0.0082 - dann loss: 2.1551 - reconstruction loss: 0.0350128/3742 - total loss: 0.2443 - classification loss: 0.0074 - dann loss: 2.1573 - reconstruction loss: 0.0348128/3742 - total loss: 0.2443 - classification loss: 0.0073 - dann loss: 2.1594 - reconstruction loss: 0.0346128/3742 - total loss: 0.2451 - classification loss: 0.0069 - dann loss: 2.1659 - reconstruction loss: 0.0348128/3742 - total loss: 0.2438 - classification loss: 0.0070 - dann loss: 2.1541 - reconstruction loss: 0.0347128/3742 - total loss: 0.2445 - classification loss: 0.0069 - dann loss: 2.1608 - reconstruction loss: 0.0347128/3742 - total loss: 0.2440 - classification loss: 0.0065 - dann loss: 2.1568 - reconstruction loss: 0.0346128/3742 - total loss: 0.2440 - classification loss: 0.0065 - dann loss: 2.1570 - reconstruction loss: 0.0345128/3742 - total loss: 0.2451 - classification loss: 0.0064 - dann loss: 2.1693 - reconstruction loss: 0.0344128/3742 - total loss: 0.2452 - classification loss: 0.0062 - dann loss: 2.1714 - reconstruction loss: 0.0343128/3742 - total loss: 0.2455 - classification loss: 0.0063 - dann loss: 2.1742 - reconstruction loss: 0.0343128/3742 - total loss: 0.2452 - classification loss: 0.0062 - dann loss: 2.1709 - reconstruction loss: 0.0344128/3742 - total loss: 0.2448 - classification loss: 0.0060 - dann loss: 2.1670 - reconstruction loss: 0.0343128/3742 - total loss: 0.2453 - classification loss: 0.0059 - dann loss: 2.1717 - reconstruction loss: 0.0344128/3742 - total loss: 0.2455 - classification loss: 0.0058 - dann loss: 2.1737 - reconstruction loss: 0.0344128/3742 - total loss: 0.2455 - classification loss: 0.0057 - dann loss: 2.1736 - reconstruction loss: 0.0344128/3742 - total loss: 0.2456 - classification loss: 0.0056 - dann loss: 2.1745 - reconstruction loss: 0.0345128/3742 - total loss: 0.2460 - classification loss: 0.0056 - dann loss: 2.1783 - reconstruction loss: 0.0345128/3742 - total loss: 0.2461 - classification loss: 0.0056 - dann loss: 2.1794 - reconstruction loss: 0.0345128/3742 - total loss: 0.2462 - classification loss: 0.0057 - dann loss: 2.1800 - reconstruction loss: 0.0345128/3742 - total loss: 0.2466 - classification loss: 0.0058 - dann loss: 2.1832 - reconstruction loss: 0.0346128/3742 - total loss: 0.2468 - classification loss: 0.0059 - dann loss: 2.1857 - reconstruction loss: 0.0346128/3742 - total loss: 0.2470 - classification loss: 0.0061 - dann loss: 2.1874 - reconstruction loss: 0.0346128/3742 - total loss: 0.2472 - classification loss: 0.0061 - dann loss: 2.1901 - reconstruction loss: 0.0345128/3742 - total loss: 0.2474 - classification loss: 0.0067 - dann loss: 2.1908 - reconstruction loss: 0.034529/3742 - total loss: 0.2475 - classification loss: 0.0073 - dann loss: 2.1918 - reconstruction loss: 0.03450/3742 - total loss: 0.2475 - classification loss: 0.0073 - dann loss: 2.1918 - reconstruction loss: 0.0345Epoch 49/150, Current strat Epoch 49/100
use_perm = True
switching perm
128/3742 - total loss: 0.2471 - classification loss: 0.0036 - dann loss: 2.2045 - reconstruction loss: 0.0328128/3742 - total loss: 0.2505 - classification loss: 0.0075 - dann loss: 2.2236 - reconstruction loss: 0.0342128/3742 - total loss: 0.2513 - classification loss: 0.0068 - dann loss: 2.2339 - reconstruction loss: 0.0341128/3742 - total loss: 0.2527 - classification loss: 0.0070 - dann loss: 2.2384 - reconstruction loss: 0.0351128/3742 - total loss: 0.2538 - classification loss: 0.0065 - dann loss: 2.2508 - reconstruction loss: 0.0351128/3742 - total loss: 0.2541 - classification loss: 0.0062 - dann loss: 2.2540 - reconstruction loss: 0.0351128/3742 - total loss: 0.2537 - classification loss: 0.0066 - dann loss: 2.2502 - reconstruction loss: 0.0351128/3742 - total loss: 0.2532 - classification loss: 0.0064 - dann loss: 2.2455 - reconstruction loss: 0.0351128/3742 - total loss: 0.2521 - classification loss: 0.0073 - dann loss: 2.2324 - reconstruction loss: 0.0351128/3742 - total loss: 0.2523 - classification loss: 0.0075 - dann loss: 2.2327 - reconstruction loss: 0.0353128/3742 - total loss: 0.2525 - classification loss: 0.0074 - dann loss: 2.2367 - reconstruction loss: 0.0352128/3742 - total loss: 0.2522 - classification loss: 0.0072 - dann loss: 2.2346 - reconstruction loss: 0.0350128/3742 - total loss: 0.2528 - classification loss: 0.0070 - dann loss: 2.2415 - reconstruction loss: 0.0350128/3742 - total loss: 0.2531 - classification loss: 0.0069 - dann loss: 2.2458 - reconstruction loss: 0.0348128/3742 - total loss: 0.2532 - classification loss: 0.0069 - dann loss: 2.2459 - reconstruction loss: 0.0350128/3742 - total loss: 0.2535 - classification loss: 0.0067 - dann loss: 2.2481 - reconstruction loss: 0.0350128/3742 - total loss: 0.2534 - classification loss: 0.0065 - dann loss: 2.2470 - reconstruction loss: 0.0350128/3742 - total loss: 0.2535 - classification loss: 0.0068 - dann loss: 2.2479 - reconstruction loss: 0.0350128/3742 - total loss: 0.2534 - classification loss: 0.0069 - dann loss: 2.2472 - reconstruction loss: 0.0349128/3742 - total loss: 0.2532 - classification loss: 0.0068 - dann loss: 2.2458 - reconstruction loss: 0.0349128/3742 - total loss: 0.2531 - classification loss: 0.0071 - dann loss: 2.2449 - reconstruction loss: 0.0349128/3742 - total loss: 0.2533 - classification loss: 0.0073 - dann loss: 2.2467 - reconstruction loss: 0.0349128/3742 - total loss: 0.2533 - classification loss: 0.0073 - dann loss: 2.2466 - reconstruction loss: 0.0348128/3742 - total loss: 0.2533 - classification loss: 0.0075 - dann loss: 2.2462 - reconstruction loss: 0.0349128/3742 - total loss: 0.2532 - classification loss: 0.0078 - dann loss: 2.2453 - reconstruction loss: 0.0348128/3742 - total loss: 0.2531 - classification loss: 0.0078 - dann loss: 2.2448 - reconstruction loss: 0.0348128/3742 - total loss: 0.2531 - classification loss: 0.0079 - dann loss: 2.2444 - reconstruction loss: 0.0348128/3742 - total loss: 0.2530 - classification loss: 0.0078 - dann loss: 2.2445 - reconstruction loss: 0.0348128/3742 - total loss: 0.2529 - classification loss: 0.0077 - dann loss: 2.2431 - reconstruction loss: 0.034729/3742 - total loss: 0.2526 - classification loss: 0.0076 - dann loss: 2.2419 - reconstruction loss: 0.03460/3742 - total loss: 0.2526 - classification loss: 0.0076 - dann loss: 2.2419 - reconstruction loss: 0.0346Epoch 50/150, Current strat Epoch 50/100
use_perm = True
switching perm
128/3742 - total loss: 0.2494 - classification loss: 0.0046 - dann loss: 2.2168 - reconstruction loss: 0.0340128/3742 - total loss: 0.2542 - classification loss: 0.0047 - dann loss: 2.2525 - reconstruction loss: 0.0356128/3742 - total loss: 0.2549 - classification loss: 0.0048 - dann loss: 2.2641 - reconstruction loss: 0.0350128/3742 - total loss: 0.2527 - classification loss: 0.0052 - dann loss: 2.2438 - reconstruction loss: 0.0348128/3742 - total loss: 0.2527 - classification loss: 0.0053 - dann loss: 2.2398 - reconstruction loss: 0.0352128/3742 - total loss: 0.2534 - classification loss: 0.0055 - dann loss: 2.2471 - reconstruction loss: 0.0351128/3742 - total loss: 0.2525 - classification loss: 0.0051 - dann loss: 2.2412 - reconstruction loss: 0.0348128/3742 - total loss: 0.2524 - classification loss: 0.0050 - dann loss: 2.2400 - reconstruction loss: 0.0349128/3742 - total loss: 0.2514 - classification loss: 0.0054 - dann loss: 2.2284 - reconstruction loss: 0.0350128/3742 - total loss: 0.2516 - classification loss: 0.0054 - dann loss: 2.2309 - reconstruction loss: 0.0350128/3742 - total loss: 0.2517 - classification loss: 0.0056 - dann loss: 2.2328 - reconstruction loss: 0.0348128/3742 - total loss: 0.2514 - classification loss: 0.0061 - dann loss: 2.2305 - reconstruction loss: 0.0347128/3742 - total loss: 0.2512 - classification loss: 0.0059 - dann loss: 2.2293 - reconstruction loss: 0.0346128/3742 - total loss: 0.2508 - classification loss: 0.0058 - dann loss: 2.2258 - reconstruction loss: 0.0346128/3742 - total loss: 0.2507 - classification loss: 0.0058 - dann loss: 2.2250 - reconstruction loss: 0.0345128/3742 - total loss: 0.2509 - classification loss: 0.0057 - dann loss: 2.2279 - reconstruction loss: 0.0344128/3742 - total loss: 0.2507 - classification loss: 0.0059 - dann loss: 2.2257 - reconstruction loss: 0.0344128/3742 - total loss: 0.2509 - classification loss: 0.0062 - dann loss: 2.2276 - reconstruction loss: 0.0344128/3742 - total loss: 0.2505 - classification loss: 0.0062 - dann loss: 2.2236 - reconstruction loss: 0.0344128/3742 - total loss: 0.2507 - classification loss: 0.0063 - dann loss: 2.2251 - reconstruction loss: 0.0345128/3742 - total loss: 0.2508 - classification loss: 0.0062 - dann loss: 2.2262 - reconstruction loss: 0.0345128/3742 - total loss: 0.2508 - classification loss: 0.0063 - dann loss: 2.2260 - reconstruction loss: 0.0345128/3742 - total loss: 0.2504 - classification loss: 0.0063 - dann loss: 2.2218 - reconstruction loss: 0.0345128/3742 - total loss: 0.2503 - classification loss: 0.0066 - dann loss: 2.2199 - reconstruction loss: 0.0345128/3742 - total loss: 0.2500 - classification loss: 0.0066 - dann loss: 2.2180 - reconstruction loss: 0.0345128/3742 - total loss: 0.2501 - classification loss: 0.0068 - dann loss: 2.2176 - reconstruction loss: 0.0346128/3742 - total loss: 0.2502 - classification loss: 0.0070 - dann loss: 2.2174 - reconstruction loss: 0.0347128/3742 - total loss: 0.2502 - classification loss: 0.0071 - dann loss: 2.2176 - reconstruction loss: 0.0347128/3742 - total loss: 0.2504 - classification loss: 0.0073 - dann loss: 2.2197 - reconstruction loss: 0.034729/3742 - total loss: 0.2502 - classification loss: 0.0079 - dann loss: 2.2161 - reconstruction loss: 0.03470/3742 - total loss: 0.2502 - classification loss: 0.0079 - dann loss: 2.2161 - reconstruction loss: 0.0347Epoch 51/150, Current strat Epoch 51/100
use_perm = True
switching perm
128/3742 - total loss: 0.2457 - classification loss: 0.0092 - dann loss: 2.1671 - reconstruction loss: 0.0350128/3742 - total loss: 0.2458 - classification loss: 0.0134 - dann loss: 2.1650 - reconstruction loss: 0.0350128/3742 - total loss: 0.2468 - classification loss: 0.0103 - dann loss: 2.1795 - reconstruction loss: 0.0347128/3742 - total loss: 0.2478 - classification loss: 0.0100 - dann loss: 2.1889 - reconstruction loss: 0.0349128/3742 - total loss: 0.2484 - classification loss: 0.0105 - dann loss: 2.1944 - reconstruction loss: 0.0348128/3742 - total loss: 0.2478 - classification loss: 0.0102 - dann loss: 2.1897 - reconstruction loss: 0.0348128/3742 - total loss: 0.2475 - classification loss: 0.0095 - dann loss: 2.1880 - reconstruction loss: 0.0347128/3742 - total loss: 0.2481 - classification loss: 0.0103 - dann loss: 2.1927 - reconstruction loss: 0.0347128/3742 - total loss: 0.2479 - classification loss: 0.0097 - dann loss: 2.1923 - reconstruction loss: 0.0347128/3742 - total loss: 0.2475 - classification loss: 0.0090 - dann loss: 2.1886 - reconstruction loss: 0.0346128/3742 - total loss: 0.2474 - classification loss: 0.0085 - dann loss: 2.1889 - reconstruction loss: 0.0346128/3742 - total loss: 0.2476 - classification loss: 0.0085 - dann loss: 2.1898 - reconstruction loss: 0.0347128/3742 - total loss: 0.2473 - classification loss: 0.0081 - dann loss: 2.1873 - reconstruction loss: 0.0347128/3742 - total loss: 0.2472 - classification loss: 0.0079 - dann loss: 2.1860 - reconstruction loss: 0.0347128/3742 - total loss: 0.2469 - classification loss: 0.0077 - dann loss: 2.1838 - reconstruction loss: 0.0347128/3742 - total loss: 0.2468 - classification loss: 0.0075 - dann loss: 2.1836 - reconstruction loss: 0.0346128/3742 - total loss: 0.2464 - classification loss: 0.0073 - dann loss: 2.1799 - reconstruction loss: 0.0346128/3742 - total loss: 0.2467 - classification loss: 0.0071 - dann loss: 2.1832 - reconstruction loss: 0.0346128/3742 - total loss: 0.2468 - classification loss: 0.0070 - dann loss: 2.1839 - reconstruction loss: 0.0346128/3742 - total loss: 0.2464 - classification loss: 0.0070 - dann loss: 2.1806 - reconstruction loss: 0.0346128/3742 - total loss: 0.2467 - classification loss: 0.0075 - dann loss: 2.1831 - reconstruction loss: 0.0345128/3742 - total loss: 0.2467 - classification loss: 0.0075 - dann loss: 2.1831 - reconstruction loss: 0.0345128/3742 - total loss: 0.2469 - classification loss: 0.0077 - dann loss: 2.1851 - reconstruction loss: 0.0345128/3742 - total loss: 0.2468 - classification loss: 0.0080 - dann loss: 2.1833 - reconstruction loss: 0.0346128/3742 - total loss: 0.2465 - classification loss: 0.0081 - dann loss: 2.1795 - reconstruction loss: 0.0347128/3742 - total loss: 0.2463 - classification loss: 0.0080 - dann loss: 2.1770 - reconstruction loss: 0.0347128/3742 - total loss: 0.2463 - classification loss: 0.0079 - dann loss: 2.1770 - reconstruction loss: 0.0347128/3742 - total loss: 0.2462 - classification loss: 0.0078 - dann loss: 2.1763 - reconstruction loss: 0.0347128/3742 - total loss: 0.2462 - classification loss: 0.0079 - dann loss: 2.1772 - reconstruction loss: 0.034729/3742 - total loss: 0.2462 - classification loss: 0.0078 - dann loss: 2.1766 - reconstruction loss: 0.03460/3742 - total loss: 0.2462 - classification loss: 0.0078 - dann loss: 2.1766 - reconstruction loss: 0.0346Epoch 52/150, Current strat Epoch 52/100
use_perm = True
switching perm
128/3742 - total loss: 0.2570 - classification loss: 0.0067 - dann loss: 2.2825 - reconstruction loss: 0.0351128/3742 - total loss: 0.2521 - classification loss: 0.0079 - dann loss: 2.2362 - reconstruction loss: 0.0346128/3742 - total loss: 0.2484 - classification loss: 0.0071 - dann loss: 2.2021 - reconstruction loss: 0.0343128/3742 - total loss: 0.2481 - classification loss: 0.0081 - dann loss: 2.1961 - reconstruction loss: 0.0346128/3742 - total loss: 0.2474 - classification loss: 0.0081 - dann loss: 2.1883 - reconstruction loss: 0.0347128/3742 - total loss: 0.2478 - classification loss: 0.0079 - dann loss: 2.1900 - reconstruction loss: 0.0350128/3742 - total loss: 0.2484 - classification loss: 0.0078 - dann loss: 2.1973 - reconstruction loss: 0.0349128/3742 - total loss: 0.2489 - classification loss: 0.0077 - dann loss: 2.2020 - reconstruction loss: 0.0349128/3742 - total loss: 0.2484 - classification loss: 0.0081 - dann loss: 2.1988 - reconstruction loss: 0.0347128/3742 - total loss: 0.2480 - classification loss: 0.0077 - dann loss: 2.1961 - reconstruction loss: 0.0345128/3742 - total loss: 0.2476 - classification loss: 0.0074 - dann loss: 2.1929 - reconstruction loss: 0.0345128/3742 - total loss: 0.2475 - classification loss: 0.0075 - dann loss: 2.1925 - reconstruction loss: 0.0343128/3742 - total loss: 0.2479 - classification loss: 0.0077 - dann loss: 2.1956 - reconstruction loss: 0.0344128/3742 - total loss: 0.2477 - classification loss: 0.0076 - dann loss: 2.1940 - reconstruction loss: 0.0345128/3742 - total loss: 0.2479 - classification loss: 0.0075 - dann loss: 2.1962 - reconstruction loss: 0.0345128/3742 - total loss: 0.2477 - classification loss: 0.0077 - dann loss: 2.1933 - reconstruction loss: 0.0345128/3742 - total loss: 0.2480 - classification loss: 0.0101 - dann loss: 2.1931 - reconstruction loss: 0.0346128/3742 - total loss: 0.2481 - classification loss: 0.0100 - dann loss: 2.1942 - reconstruction loss: 0.0346128/3742 - total loss: 0.2478 - classification loss: 0.0101 - dann loss: 2.1913 - reconstruction loss: 0.0346128/3742 - total loss: 0.2477 - classification loss: 0.0104 - dann loss: 2.1895 - reconstruction loss: 0.0346128/3742 - total loss: 0.2476 - classification loss: 0.0104 - dann loss: 2.1895 - reconstruction loss: 0.0346128/3742 - total loss: 0.2476 - classification loss: 0.0102 - dann loss: 2.1897 - reconstruction loss: 0.0345128/3742 - total loss: 0.2477 - classification loss: 0.0101 - dann loss: 2.1905 - reconstruction loss: 0.0345128/3742 - total loss: 0.2479 - classification loss: 0.0099 - dann loss: 2.1922 - reconstruction loss: 0.0346128/3742 - total loss: 0.2477 - classification loss: 0.0105 - dann loss: 2.1902 - reconstruction loss: 0.0345128/3742 - total loss: 0.2482 - classification loss: 0.0105 - dann loss: 2.1947 - reconstruction loss: 0.0346128/3742 - total loss: 0.2481 - classification loss: 0.0104 - dann loss: 2.1945 - reconstruction loss: 0.0345128/3742 - total loss: 0.2481 - classification loss: 0.0104 - dann loss: 2.1935 - reconstruction loss: 0.0346128/3742 - total loss: 0.2483 - classification loss: 0.0105 - dann loss: 2.1956 - reconstruction loss: 0.034729/3742 - total loss: 0.2486 - classification loss: 0.0107 - dann loss: 2.1990 - reconstruction loss: 0.03450/3742 - total loss: 0.2486 - classification loss: 0.0107 - dann loss: 2.1990 - reconstruction loss: 0.0345Epoch 53/150, Current strat Epoch 53/100
use_perm = True
switching perm
128/3742 - total loss: 0.2581 - classification loss: 0.0080 - dann loss: 2.2911 - reconstruction loss: 0.0352128/3742 - total loss: 0.2530 - classification loss: 0.0060 - dann loss: 2.2425 - reconstruction loss: 0.0351128/3742 - total loss: 0.2529 - classification loss: 0.0074 - dann loss: 2.2422 - reconstruction loss: 0.0350128/3742 - total loss: 0.2521 - classification loss: 0.0066 - dann loss: 2.2351 - reconstruction loss: 0.0349128/3742 - total loss: 0.2504 - classification loss: 0.0062 - dann loss: 2.2191 - reconstruction loss: 0.0348128/3742 - total loss: 0.2505 - classification loss: 0.0066 - dann loss: 2.2205 - reconstruction loss: 0.0347128/3742 - total loss: 0.2505 - classification loss: 0.0070 - dann loss: 2.2201 - reconstruction loss: 0.0347128/3742 - total loss: 0.2504 - classification loss: 0.0080 - dann loss: 2.2176 - reconstruction loss: 0.0348128/3742 - total loss: 0.2497 - classification loss: 0.0076 - dann loss: 2.2108 - reconstruction loss: 0.0348128/3742 - total loss: 0.2498 - classification loss: 0.0075 - dann loss: 2.2116 - reconstruction loss: 0.0349128/3742 - total loss: 0.2498 - classification loss: 0.0075 - dann loss: 2.2126 - reconstruction loss: 0.0348128/3742 - total loss: 0.2499 - classification loss: 0.0083 - dann loss: 2.2139 - reconstruction loss: 0.0345128/3742 - total loss: 0.2503 - classification loss: 0.0083 - dann loss: 2.2181 - reconstruction loss: 0.0345128/3742 - total loss: 0.2495 - classification loss: 0.0081 - dann loss: 2.2097 - reconstruction loss: 0.0346128/3742 - total loss: 0.2491 - classification loss: 0.0080 - dann loss: 2.2070 - reconstruction loss: 0.0345128/3742 - total loss: 0.2494 - classification loss: 0.0082 - dann loss: 2.2102 - reconstruction loss: 0.0344128/3742 - total loss: 0.2493 - classification loss: 0.0082 - dann loss: 2.2091 - reconstruction loss: 0.0345128/3742 - total loss: 0.2493 - classification loss: 0.0080 - dann loss: 2.2096 - reconstruction loss: 0.0344128/3742 - total loss: 0.2492 - classification loss: 0.0078 - dann loss: 2.2085 - reconstruction loss: 0.0345128/3742 - total loss: 0.2491 - classification loss: 0.0077 - dann loss: 2.2073 - reconstruction loss: 0.0344128/3742 - total loss: 0.2489 - classification loss: 0.0077 - dann loss: 2.2058 - reconstruction loss: 0.0345128/3742 - total loss: 0.2491 - classification loss: 0.0078 - dann loss: 2.2075 - reconstruction loss: 0.0345128/3742 - total loss: 0.2490 - classification loss: 0.0078 - dann loss: 2.2063 - reconstruction loss: 0.0345128/3742 - total loss: 0.2487 - classification loss: 0.0077 - dann loss: 2.2031 - reconstruction loss: 0.0346128/3742 - total loss: 0.2487 - classification loss: 0.0076 - dann loss: 2.2031 - reconstruction loss: 0.0346128/3742 - total loss: 0.2485 - classification loss: 0.0075 - dann loss: 2.2011 - reconstruction loss: 0.0346128/3742 - total loss: 0.2484 - classification loss: 0.0075 - dann loss: 2.2004 - reconstruction loss: 0.0346128/3742 - total loss: 0.2485 - classification loss: 0.0075 - dann loss: 2.2011 - reconstruction loss: 0.0346128/3742 - total loss: 0.2484 - classification loss: 0.0076 - dann loss: 2.1998 - reconstruction loss: 0.034629/3742 - total loss: 0.2483 - classification loss: 0.0078 - dann loss: 2.1980 - reconstruction loss: 0.03470/3742 - total loss: 0.2483 - classification loss: 0.0078 - dann loss: 2.1980 - reconstruction loss: 0.0347Epoch 54/150, Current strat Epoch 54/100
use_perm = True
switching perm
128/3742 - total loss: 0.2376 - classification loss: 0.0027 - dann loss: 2.0983 - reconstruction loss: 0.0344128/3742 - total loss: 0.2410 - classification loss: 0.0054 - dann loss: 2.1281 - reconstruction loss: 0.0345128/3742 - total loss: 0.2427 - classification loss: 0.0055 - dann loss: 2.1480 - reconstruction loss: 0.0341128/3742 - total loss: 0.2452 - classification loss: 0.0054 - dann loss: 2.1731 - reconstruction loss: 0.0341128/3742 - total loss: 0.2458 - classification loss: 0.0078 - dann loss: 2.1751 - reconstruction loss: 0.0344128/3742 - total loss: 0.2448 - classification loss: 0.0071 - dann loss: 2.1660 - reconstruction loss: 0.0343128/3742 - total loss: 0.2448 - classification loss: 0.0070 - dann loss: 2.1648 - reconstruction loss: 0.0346128/3742 - total loss: 0.2453 - classification loss: 0.0065 - dann loss: 2.1702 - reconstruction loss: 0.0345128/3742 - total loss: 0.2447 - classification loss: 0.0069 - dann loss: 2.1641 - reconstruction loss: 0.0345128/3742 - total loss: 0.2447 - classification loss: 0.0068 - dann loss: 2.1642 - reconstruction loss: 0.0346128/3742 - total loss: 0.2456 - classification loss: 0.0066 - dann loss: 2.1713 - reconstruction loss: 0.0347128/3742 - total loss: 0.2456 - classification loss: 0.0073 - dann loss: 2.1714 - reconstruction loss: 0.0346128/3742 - total loss: 0.2457 - classification loss: 0.0071 - dann loss: 2.1726 - reconstruction loss: 0.0346128/3742 - total loss: 0.2454 - classification loss: 0.0070 - dann loss: 2.1697 - reconstruction loss: 0.0347128/3742 - total loss: 0.2457 - classification loss: 0.0067 - dann loss: 2.1731 - reconstruction loss: 0.0347128/3742 - total loss: 0.2453 - classification loss: 0.0066 - dann loss: 2.1689 - reconstruction loss: 0.0346128/3742 - total loss: 0.2455 - classification loss: 0.0064 - dann loss: 2.1711 - reconstruction loss: 0.0347128/3742 - total loss: 0.2457 - classification loss: 0.0063 - dann loss: 2.1729 - reconstruction loss: 0.0347128/3742 - total loss: 0.2454 - classification loss: 0.0062 - dann loss: 2.1706 - reconstruction loss: 0.0346128/3742 - total loss: 0.2455 - classification loss: 0.0064 - dann loss: 2.1719 - reconstruction loss: 0.0346128/3742 - total loss: 0.2451 - classification loss: 0.0063 - dann loss: 2.1685 - reconstruction loss: 0.0345128/3742 - total loss: 0.2450 - classification loss: 0.0062 - dann loss: 2.1677 - reconstruction loss: 0.0345128/3742 - total loss: 0.2450 - classification loss: 0.0061 - dann loss: 2.1675 - reconstruction loss: 0.0346128/3742 - total loss: 0.2450 - classification loss: 0.0069 - dann loss: 2.1661 - reconstruction loss: 0.0347128/3742 - total loss: 0.2448 - classification loss: 0.0067 - dann loss: 2.1647 - reconstruction loss: 0.0346128/3742 - total loss: 0.2449 - classification loss: 0.0066 - dann loss: 2.1655 - reconstruction loss: 0.0346128/3742 - total loss: 0.2451 - classification loss: 0.0066 - dann loss: 2.1675 - reconstruction loss: 0.0346128/3742 - total loss: 0.2451 - classification loss: 0.0065 - dann loss: 2.1672 - reconstruction loss: 0.0346128/3742 - total loss: 0.2450 - classification loss: 0.0065 - dann loss: 2.1672 - reconstruction loss: 0.034529/3742 - total loss: 0.2449 - classification loss: 0.0064 - dann loss: 2.1677 - reconstruction loss: 0.03440/3742 - total loss: 0.2449 - classification loss: 0.0064 - dann loss: 2.1677 - reconstruction loss: 0.0344Early stopping at epoch 34, restoring model parameters from this epoch
adam
Epoch 55/150, Current strat Epoch 1/50
use_perm = False
switching perm
128/3742 - total loss: 0.0030 - classification loss: 0.0300 - dann loss: 2.2495 - reconstruction loss: 0.0343128/3742 - total loss: 0.0032 - classification loss: 0.0323 - dann loss: 2.2075 - reconstruction loss: 0.0361128/3742 - total loss: 0.0033 - classification loss: 0.0335 - dann loss: 2.2104 - reconstruction loss: 0.0365128/3742 - total loss: 0.0031 - classification loss: 0.0308 - dann loss: 2.2115 - reconstruction loss: 0.0367128/3742 - total loss: 0.0031 - classification loss: 0.0315 - dann loss: 2.2072 - reconstruction loss: 0.0363128/3742 - total loss: 0.0028 - classification loss: 0.0284 - dann loss: 2.2175 - reconstruction loss: 0.0361128/3742 - total loss: 0.0027 - classification loss: 0.0265 - dann loss: 2.2101 - reconstruction loss: 0.0361128/3742 - total loss: 0.0026 - classification loss: 0.0261 - dann loss: 2.2150 - reconstruction loss: 0.0360128/3742 - total loss: 0.0025 - classification loss: 0.0250 - dann loss: 2.2112 - reconstruction loss: 0.0359128/3742 - total loss: 0.0024 - classification loss: 0.0240 - dann loss: 2.2098 - reconstruction loss: 0.0358128/3742 - total loss: 0.0024 - classification loss: 0.0240 - dann loss: 2.2113 - reconstruction loss: 0.0359128/3742 - total loss: 0.0024 - classification loss: 0.0235 - dann loss: 2.2139 - reconstruction loss: 0.0358128/3742 - total loss: 0.0023 - classification loss: 0.0230 - dann loss: 2.2120 - reconstruction loss: 0.0360128/3742 - total loss: 0.0022 - classification loss: 0.0220 - dann loss: 2.2142 - reconstruction loss: 0.0359128/3742 - total loss: 0.0022 - classification loss: 0.0223 - dann loss: 2.2146 - reconstruction loss: 0.0359128/3742 - total loss: 0.0022 - classification loss: 0.0217 - dann loss: 2.2145 - reconstruction loss: 0.0357128/3742 - total loss: 0.0022 - classification loss: 0.0216 - dann loss: 2.2120 - reconstruction loss: 0.0358128/3742 - total loss: 0.0021 - classification loss: 0.0212 - dann loss: 2.2103 - reconstruction loss: 0.0358128/3742 - total loss: 0.0021 - classification loss: 0.0208 - dann loss: 2.2112 - reconstruction loss: 0.0359128/3742 - total loss: 0.0020 - classification loss: 0.0201 - dann loss: 2.2135 - reconstruction loss: 0.0359128/3742 - total loss: 0.0020 - classification loss: 0.0204 - dann loss: 2.2114 - reconstruction loss: 0.0358128/3742 - total loss: 0.0021 - classification loss: 0.0205 - dann loss: 2.2114 - reconstruction loss: 0.0358128/3742 - total loss: 0.0020 - classification loss: 0.0199 - dann loss: 2.2095 - reconstruction loss: 0.0358128/3742 - total loss: 0.0020 - classification loss: 0.0196 - dann loss: 2.2085 - reconstruction loss: 0.0357128/3742 - total loss: 0.0019 - classification loss: 0.0190 - dann loss: 2.2096 - reconstruction loss: 0.0357128/3742 - total loss: 0.0019 - classification loss: 0.0187 - dann loss: 2.2086 - reconstruction loss: 0.0357128/3742 - total loss: 0.0018 - classification loss: 0.0184 - dann loss: 2.2088 - reconstruction loss: 0.0356128/3742 - total loss: 0.0018 - classification loss: 0.0180 - dann loss: 2.2107 - reconstruction loss: 0.0356128/3742 - total loss: 0.0018 - classification loss: 0.0177 - dann loss: 2.2118 - reconstruction loss: 0.035629/3742 - total loss: 0.0017 - classification loss: 0.0173 - dann loss: 2.2111 - reconstruction loss: 0.03550/3742 - total loss: 0.0017 - classification loss: 0.0173 - dann loss: 2.2111 - reconstruction loss: 0.0355Epoch 56/150, Current strat Epoch 2/50
use_perm = False
switching perm
128/3742 - total loss: 0.0004 - classification loss: 0.0037 - dann loss: 2.2159 - reconstruction loss: 0.0360128/3742 - total loss: 0.0005 - classification loss: 0.0049 - dann loss: 2.2026 - reconstruction loss: 0.0367128/3742 - total loss: 0.0004 - classification loss: 0.0045 - dann loss: 2.2044 - reconstruction loss: 0.0357128/3742 - total loss: 0.0005 - classification loss: 0.0046 - dann loss: 2.2109 - reconstruction loss: 0.0357128/3742 - total loss: 0.0005 - classification loss: 0.0051 - dann loss: 2.2138 - reconstruction loss: 0.0358128/3742 - total loss: 0.0006 - classification loss: 0.0057 - dann loss: 2.2092 - reconstruction loss: 0.0359128/3742 - total loss: 0.0006 - classification loss: 0.0057 - dann loss: 2.2081 - reconstruction loss: 0.0356128/3742 - total loss: 0.0006 - classification loss: 0.0057 - dann loss: 2.2066 - reconstruction loss: 0.0356128/3742 - total loss: 0.0006 - classification loss: 0.0057 - dann loss: 2.2060 - reconstruction loss: 0.0356128/3742 - total loss: 0.0006 - classification loss: 0.0055 - dann loss: 2.2066 - reconstruction loss: 0.0357128/3742 - total loss: 0.0005 - classification loss: 0.0055 - dann loss: 2.2103 - reconstruction loss: 0.0356128/3742 - total loss: 0.0005 - classification loss: 0.0055 - dann loss: 2.2103 - reconstruction loss: 0.0355128/3742 - total loss: 0.0005 - classification loss: 0.0053 - dann loss: 2.2055 - reconstruction loss: 0.0355128/3742 - total loss: 0.0005 - classification loss: 0.0054 - dann loss: 2.2045 - reconstruction loss: 0.0357128/3742 - total loss: 0.0005 - classification loss: 0.0054 - dann loss: 2.2069 - reconstruction loss: 0.0357128/3742 - total loss: 0.0005 - classification loss: 0.0054 - dann loss: 2.2123 - reconstruction loss: 0.0356128/3742 - total loss: 0.0006 - classification loss: 0.0057 - dann loss: 2.2127 - reconstruction loss: 0.0356128/3742 - total loss: 0.0006 - classification loss: 0.0058 - dann loss: 2.2159 - reconstruction loss: 0.0356128/3742 - total loss: 0.0006 - classification loss: 0.0056 - dann loss: 2.2155 - reconstruction loss: 0.0356128/3742 - total loss: 0.0005 - classification loss: 0.0055 - dann loss: 2.2201 - reconstruction loss: 0.0355128/3742 - total loss: 0.0006 - classification loss: 0.0055 - dann loss: 2.2228 - reconstruction loss: 0.0355128/3742 - total loss: 0.0005 - classification loss: 0.0054 - dann loss: 2.2190 - reconstruction loss: 0.0355128/3742 - total loss: 0.0006 - classification loss: 0.0055 - dann loss: 2.2204 - reconstruction loss: 0.0355128/3742 - total loss: 0.0005 - classification loss: 0.0055 - dann loss: 2.2174 - reconstruction loss: 0.0355128/3742 - total loss: 0.0005 - classification loss: 0.0055 - dann loss: 2.2166 - reconstruction loss: 0.0355128/3742 - total loss: 0.0005 - classification loss: 0.0054 - dann loss: 2.2124 - reconstruction loss: 0.0355128/3742 - total loss: 0.0005 - classification loss: 0.0054 - dann loss: 2.2109 - reconstruction loss: 0.0355128/3742 - total loss: 0.0005 - classification loss: 0.0054 - dann loss: 2.2104 - reconstruction loss: 0.0355128/3742 - total loss: 0.0006 - classification loss: 0.0055 - dann loss: 2.2103 - reconstruction loss: 0.035529/3742 - total loss: 0.0005 - classification loss: 0.0055 - dann loss: 2.2160 - reconstruction loss: 0.03570/3742 - total loss: 0.0005 - classification loss: 0.0055 - dann loss: 2.2160 - reconstruction loss: 0.0357Epoch 57/150, Current strat Epoch 3/50
use_perm = False
switching perm
128/3742 - total loss: 0.0005 - classification loss: 0.0050 - dann loss: 2.2372 - reconstruction loss: 0.0352128/3742 - total loss: 0.0005 - classification loss: 0.0047 - dann loss: 2.2392 - reconstruction loss: 0.0350128/3742 - total loss: 0.0004 - classification loss: 0.0044 - dann loss: 2.2229 - reconstruction loss: 0.0355128/3742 - total loss: 0.0004 - classification loss: 0.0038 - dann loss: 2.2247 - reconstruction loss: 0.0358128/3742 - total loss: 0.0004 - classification loss: 0.0039 - dann loss: 2.2263 - reconstruction loss: 0.0356128/3742 - total loss: 0.0004 - classification loss: 0.0038 - dann loss: 2.2196 - reconstruction loss: 0.0355128/3742 - total loss: 0.0004 - classification loss: 0.0039 - dann loss: 2.2155 - reconstruction loss: 0.0352128/3742 - total loss: 0.0004 - classification loss: 0.0039 - dann loss: 2.2111 - reconstruction loss: 0.0356128/3742 - total loss: 0.0004 - classification loss: 0.0038 - dann loss: 2.2170 - reconstruction loss: 0.0356128/3742 - total loss: 0.0004 - classification loss: 0.0038 - dann loss: 2.2114 - reconstruction loss: 0.0356128/3742 - total loss: 0.0004 - classification loss: 0.0037 - dann loss: 2.2084 - reconstruction loss: 0.0358128/3742 - total loss: 0.0004 - classification loss: 0.0039 - dann loss: 2.2068 - reconstruction loss: 0.0358128/3742 - total loss: 0.0004 - classification loss: 0.0038 - dann loss: 2.2091 - reconstruction loss: 0.0357128/3742 - total loss: 0.0004 - classification loss: 0.0038 - dann loss: 2.2083 - reconstruction loss: 0.0357128/3742 - total loss: 0.0004 - classification loss: 0.0038 - dann loss: 2.2042 - reconstruction loss: 0.0357128/3742 - total loss: 0.0004 - classification loss: 0.0037 - dann loss: 2.2028 - reconstruction loss: 0.0357128/3742 - total loss: 0.0004 - classification loss: 0.0036 - dann loss: 2.2021 - reconstruction loss: 0.0357128/3742 - total loss: 0.0004 - classification loss: 0.0037 - dann loss: 2.2025 - reconstruction loss: 0.0356128/3742 - total loss: 0.0004 - classification loss: 0.0037 - dann loss: 2.2022 - reconstruction loss: 0.0356128/3742 - total loss: 0.0004 - classification loss: 0.0038 - dann loss: 2.2025 - reconstruction loss: 0.0356128/3742 - total loss: 0.0004 - classification loss: 0.0037 - dann loss: 2.2025 - reconstruction loss: 0.0355128/3742 - total loss: 0.0004 - classification loss: 0.0037 - dann loss: 2.2035 - reconstruction loss: 0.0355128/3742 - total loss: 0.0004 - classification loss: 0.0037 - dann loss: 2.2054 - reconstruction loss: 0.0355128/3742 - total loss: 0.0004 - classification loss: 0.0036 - dann loss: 2.2089 - reconstruction loss: 0.0356128/3742 - total loss: 0.0004 - classification loss: 0.0036 - dann loss: 2.2088 - reconstruction loss: 0.0355128/3742 - total loss: 0.0004 - classification loss: 0.0036 - dann loss: 2.2088 - reconstruction loss: 0.0355128/3742 - total loss: 0.0004 - classification loss: 0.0036 - dann loss: 2.2103 - reconstruction loss: 0.0355128/3742 - total loss: 0.0004 - classification loss: 0.0036 - dann loss: 2.2118 - reconstruction loss: 0.0356128/3742 - total loss: 0.0004 - classification loss: 0.0036 - dann loss: 2.2112 - reconstruction loss: 0.035629/3742 - total loss: 0.0004 - classification loss: 0.0037 - dann loss: 2.2131 - reconstruction loss: 0.03550/3742 - total loss: 0.0004 - classification loss: 0.0037 - dann loss: 2.2131 - reconstruction loss: 0.0355Epoch 58/150, Current strat Epoch 4/50
use_perm = False
switching perm
128/3742 - total loss: 0.0002 - classification loss: 0.0018 - dann loss: 2.1556 - reconstruction loss: 0.0348128/3742 - total loss: 0.0002 - classification loss: 0.0022 - dann loss: 2.2178 - reconstruction loss: 0.0353128/3742 - total loss: 0.0002 - classification loss: 0.0024 - dann loss: 2.2181 - reconstruction loss: 0.0351128/3742 - total loss: 0.0002 - classification loss: 0.0022 - dann loss: 2.2120 - reconstruction loss: 0.0354128/3742 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.2068 - reconstruction loss: 0.0354128/3742 - total loss: 0.0003 - classification loss: 0.0027 - dann loss: 2.2150 - reconstruction loss: 0.0353128/3742 - total loss: 0.0003 - classification loss: 0.0026 - dann loss: 2.2125 - reconstruction loss: 0.0354128/3742 - total loss: 0.0002 - classification loss: 0.0025 - dann loss: 2.2108 - reconstruction loss: 0.0354128/3742 - total loss: 0.0003 - classification loss: 0.0025 - dann loss: 2.2114 - reconstruction loss: 0.0354128/3742 - total loss: 0.0002 - classification loss: 0.0024 - dann loss: 2.2128 - reconstruction loss: 0.0355128/3742 - total loss: 0.0003 - classification loss: 0.0026 - dann loss: 2.2062 - reconstruction loss: 0.0355128/3742 - total loss: 0.0003 - classification loss: 0.0026 - dann loss: 2.2081 - reconstruction loss: 0.0356128/3742 - total loss: 0.0002 - classification loss: 0.0025 - dann loss: 2.2081 - reconstruction loss: 0.0356128/3742 - total loss: 0.0002 - classification loss: 0.0025 - dann loss: 2.2065 - reconstruction loss: 0.0355128/3742 - total loss: 0.0002 - classification loss: 0.0025 - dann loss: 2.2039 - reconstruction loss: 0.0356128/3742 - total loss: 0.0002 - classification loss: 0.0025 - dann loss: 2.2058 - reconstruction loss: 0.0356128/3742 - total loss: 0.0002 - classification loss: 0.0024 - dann loss: 2.2043 - reconstruction loss: 0.0356128/3742 - total loss: 0.0002 - classification loss: 0.0024 - dann loss: 2.2056 - reconstruction loss: 0.0355128/3742 - total loss: 0.0002 - classification loss: 0.0024 - dann loss: 2.2058 - reconstruction loss: 0.0354128/3742 - total loss: 0.0002 - classification loss: 0.0024 - dann loss: 2.2055 - reconstruction loss: 0.0355128/3742 - total loss: 0.0002 - classification loss: 0.0024 - dann loss: 2.2066 - reconstruction loss: 0.0355128/3742 - total loss: 0.0002 - classification loss: 0.0024 - dann loss: 2.2082 - reconstruction loss: 0.0356128/3742 - total loss: 0.0002 - classification loss: 0.0024 - dann loss: 2.2095 - reconstruction loss: 0.0356128/3742 - total loss: 0.0002 - classification loss: 0.0024 - dann loss: 2.2085 - reconstruction loss: 0.0357128/3742 - total loss: 0.0002 - classification loss: 0.0024 - dann loss: 2.2093 - reconstruction loss: 0.0357128/3742 - total loss: 0.0002 - classification loss: 0.0024 - dann loss: 2.2096 - reconstruction loss: 0.0356128/3742 - total loss: 0.0002 - classification loss: 0.0024 - dann loss: 2.2105 - reconstruction loss: 0.0356128/3742 - total loss: 0.0002 - classification loss: 0.0024 - dann loss: 2.2110 - reconstruction loss: 0.0356128/3742 - total loss: 0.0002 - classification loss: 0.0024 - dann loss: 2.2101 - reconstruction loss: 0.035629/3742 - total loss: 0.0003 - classification loss: 0.0026 - dann loss: 2.2166 - reconstruction loss: 0.03560/3742 - total loss: 0.0003 - classification loss: 0.0026 - dann loss: 2.2166 - reconstruction loss: 0.0356Epoch 59/150, Current strat Epoch 5/50
use_perm = False
switching perm
128/3742 - total loss: 0.0002 - classification loss: 0.0018 - dann loss: 2.2202 - reconstruction loss: 0.0340128/3742 - total loss: 0.0002 - classification loss: 0.0024 - dann loss: 2.2004 - reconstruction loss: 0.0341128/3742 - total loss: 0.0002 - classification loss: 0.0022 - dann loss: 2.2046 - reconstruction loss: 0.0351128/3742 - total loss: 0.0002 - classification loss: 0.0023 - dann loss: 2.1982 - reconstruction loss: 0.0351128/3742 - total loss: 0.0002 - classification loss: 0.0021 - dann loss: 2.1841 - reconstruction loss: 0.0352128/3742 - total loss: 0.0002 - classification loss: 0.0019 - dann loss: 2.1866 - reconstruction loss: 0.0353128/3742 - total loss: 0.0002 - classification loss: 0.0021 - dann loss: 2.1952 - reconstruction loss: 0.0351128/3742 - total loss: 0.0002 - classification loss: 0.0021 - dann loss: 2.1935 - reconstruction loss: 0.0354128/3742 - total loss: 0.0002 - classification loss: 0.0021 - dann loss: 2.1934 - reconstruction loss: 0.0355128/3742 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.1933 - reconstruction loss: 0.0356128/3742 - total loss: 0.0002 - classification loss: 0.0019 - dann loss: 2.2003 - reconstruction loss: 0.0355128/3742 - total loss: 0.0002 - classification loss: 0.0019 - dann loss: 2.1991 - reconstruction loss: 0.0356128/3742 - total loss: 0.0002 - classification loss: 0.0019 - dann loss: 2.1994 - reconstruction loss: 0.0357128/3742 - total loss: 0.0002 - classification loss: 0.0019 - dann loss: 2.2035 - reconstruction loss: 0.0356128/3742 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.2024 - reconstruction loss: 0.0356128/3742 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.2033 - reconstruction loss: 0.0357128/3742 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.2045 - reconstruction loss: 0.0357128/3742 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.2080 - reconstruction loss: 0.0357128/3742 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.2089 - reconstruction loss: 0.0357128/3742 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.2104 - reconstruction loss: 0.0357128/3742 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.2143 - reconstruction loss: 0.0357128/3742 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.2143 - reconstruction loss: 0.0357128/3742 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.2148 - reconstruction loss: 0.0357128/3742 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.2125 - reconstruction loss: 0.0357128/3742 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.2122 - reconstruction loss: 0.0357128/3742 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.2099 - reconstruction loss: 0.0357128/3742 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.2112 - reconstruction loss: 0.0356128/3742 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.2110 - reconstruction loss: 0.0356128/3742 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.2113 - reconstruction loss: 0.035629/3742 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.2127 - reconstruction loss: 0.03560/3742 - total loss: 0.0002 - classification loss: 0.0020 - dann loss: 2.2127 - reconstruction loss: 0.0356Epoch 60/150, Current strat Epoch 6/50
use_perm = False
switching perm
128/3742 - total loss: 0.0002 - classification loss: 0.0023 - dann loss: 2.2205 - reconstruction loss: 0.0339128/3742 - total loss: 0.0002 - classification loss: 0.0019 - dann loss: 2.2060 - reconstruction loss: 0.0359128/3742 - total loss: 0.0002 - classification loss: 0.0017 - dann loss: 2.1782 - reconstruction loss: 0.0354128/3742 - total loss: 0.0002 - classification loss: 0.0017 - dann loss: 2.1876 - reconstruction loss: 0.0355128/3742 - total loss: 0.0002 - classification loss: 0.0018 - dann loss: 2.1943 - reconstruction loss: 0.0356128/3742 - total loss: 0.0002 - classification loss: 0.0019 - dann loss: 2.1900 - reconstruction loss: 0.0357128/3742 - total loss: 0.0002 - classification loss: 0.0018 - dann loss: 2.1963 - reconstruction loss: 0.0355128/3742 - total loss: 0.0002 - classification loss: 0.0018 - dann loss: 2.1952 - reconstruction loss: 0.0356128/3742 - total loss: 0.0002 - classification loss: 0.0017 - dann loss: 2.2000 - reconstruction loss: 0.0357128/3742 - total loss: 0.0002 - classification loss: 0.0017 - dann loss: 2.1985 - reconstruction loss: 0.0356128/3742 - total loss: 0.0002 - classification loss: 0.0017 - dann loss: 2.1985 - reconstruction loss: 0.0355128/3742 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.2006 - reconstruction loss: 0.0356128/3742 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.2010 - reconstruction loss: 0.0357128/3742 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.2006 - reconstruction loss: 0.0358128/3742 - total loss: 0.0002 - classification loss: 0.0017 - dann loss: 2.2019 - reconstruction loss: 0.0358128/3742 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.2038 - reconstruction loss: 0.0357128/3742 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.2037 - reconstruction loss: 0.0358128/3742 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.2053 - reconstruction loss: 0.0357128/3742 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.2074 - reconstruction loss: 0.0358128/3742 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.2080 - reconstruction loss: 0.0358128/3742 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.2094 - reconstruction loss: 0.0358128/3742 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.2106 - reconstruction loss: 0.0357128/3742 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.2126 - reconstruction loss: 0.0357128/3742 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.2135 - reconstruction loss: 0.0357128/3742 - total loss: 0.0002 - classification loss: 0.0015 - dann loss: 2.2115 - reconstruction loss: 0.0356128/3742 - total loss: 0.0002 - classification loss: 0.0015 - dann loss: 2.2137 - reconstruction loss: 0.0356128/3742 - total loss: 0.0002 - classification loss: 0.0015 - dann loss: 2.2134 - reconstruction loss: 0.0356128/3742 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.2139 - reconstruction loss: 0.0356128/3742 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.2114 - reconstruction loss: 0.035629/3742 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.2125 - reconstruction loss: 0.03550/3742 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.2125 - reconstruction loss: 0.0355Epoch 61/150, Current strat Epoch 7/50
use_perm = False
switching perm
128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2218 - reconstruction loss: 0.0339128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2464 - reconstruction loss: 0.0352128/3742 - total loss: 0.0001 - classification loss: 0.0013 - dann loss: 2.2210 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.2078 - reconstruction loss: 0.0359128/3742 - total loss: 0.0001 - classification loss: 0.0013 - dann loss: 2.1981 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.2043 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.2075 - reconstruction loss: 0.0353128/3742 - total loss: 0.0001 - classification loss: 0.0015 - dann loss: 2.2130 - reconstruction loss: 0.0353128/3742 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.2100 - reconstruction loss: 0.0353128/3742 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.2143 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0013 - dann loss: 2.2127 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0013 - dann loss: 2.2111 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0013 - dann loss: 2.2122 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0013 - dann loss: 2.2120 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0013 - dann loss: 2.2092 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.2100 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.2084 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.2124 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.2129 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0015 - dann loss: 2.2149 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0015 - dann loss: 2.2116 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0015 - dann loss: 2.2118 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0015 - dann loss: 2.2136 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.2134 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.2134 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.2127 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.2126 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0015 - dann loss: 2.2128 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.2116 - reconstruction loss: 0.035629/3742 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.2119 - reconstruction loss: 0.03560/3742 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.2119 - reconstruction loss: 0.0356Epoch 62/150, Current strat Epoch 8/50
use_perm = False
switching perm
128/3742 - total loss: 0.0002 - classification loss: 0.0017 - dann loss: 2.1903 - reconstruction loss: 0.0360128/3742 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.1955 - reconstruction loss: 0.0357128/3742 - total loss: 0.0002 - classification loss: 0.0024 - dann loss: 2.1860 - reconstruction loss: 0.0352128/3742 - total loss: 0.0002 - classification loss: 0.0021 - dann loss: 2.1977 - reconstruction loss: 0.0350128/3742 - total loss: 0.0002 - classification loss: 0.0019 - dann loss: 2.1955 - reconstruction loss: 0.0353128/3742 - total loss: 0.0002 - classification loss: 0.0017 - dann loss: 2.2023 - reconstruction loss: 0.0354128/3742 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.2090 - reconstruction loss: 0.0355128/3742 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.2106 - reconstruction loss: 0.0357128/3742 - total loss: 0.0002 - classification loss: 0.0017 - dann loss: 2.2087 - reconstruction loss: 0.0358128/3742 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.2142 - reconstruction loss: 0.0357128/3742 - total loss: 0.0002 - classification loss: 0.0017 - dann loss: 2.2168 - reconstruction loss: 0.0358128/3742 - total loss: 0.0002 - classification loss: 0.0017 - dann loss: 2.2179 - reconstruction loss: 0.0358128/3742 - total loss: 0.0002 - classification loss: 0.0017 - dann loss: 2.2182 - reconstruction loss: 0.0359128/3742 - total loss: 0.0002 - classification loss: 0.0016 - dann loss: 2.2149 - reconstruction loss: 0.0359128/3742 - total loss: 0.0002 - classification loss: 0.0015 - dann loss: 2.2102 - reconstruction loss: 0.0358128/3742 - total loss: 0.0002 - classification loss: 0.0015 - dann loss: 2.2096 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0015 - dann loss: 2.2093 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0015 - dann loss: 2.2131 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.2109 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.2095 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.2084 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.2077 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.2080 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.2092 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.2116 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.2095 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.2118 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.2114 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0013 - dann loss: 2.2118 - reconstruction loss: 0.035629/3742 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.2111 - reconstruction loss: 0.03560/3742 - total loss: 0.0001 - classification loss: 0.0014 - dann loss: 2.2111 - reconstruction loss: 0.0356Epoch 63/150, Current strat Epoch 9/50
use_perm = False
switching perm
128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2168 - reconstruction loss: 0.0351128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2033 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.1979 - reconstruction loss: 0.0361128/3742 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.2157 - reconstruction loss: 0.0353128/3742 - total loss: 0.0001 - classification loss: 0.0012 - dann loss: 2.2035 - reconstruction loss: 0.0352128/3742 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.2007 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.2053 - reconstruction loss: 0.0353128/3742 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.2051 - reconstruction loss: 0.0352128/3742 - total loss: 0.0001 - classification loss: 0.0012 - dann loss: 2.2079 - reconstruction loss: 0.0350128/3742 - total loss: 0.0001 - classification loss: 0.0012 - dann loss: 2.2133 - reconstruction loss: 0.0352128/3742 - total loss: 0.0001 - classification loss: 0.0012 - dann loss: 2.2122 - reconstruction loss: 0.0351128/3742 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.2123 - reconstruction loss: 0.0350128/3742 - total loss: 0.0001 - classification loss: 0.0011 - dann loss: 2.2095 - reconstruction loss: 0.0352128/3742 - total loss: 0.0001 - classification loss: 0.0010 - dann loss: 2.2075 - reconstruction loss: 0.0351128/3742 - total loss: 0.0001 - classification loss: 0.0010 - dann loss: 2.2066 - reconstruction loss: 0.0351128/3742 - total loss: 0.0001 - classification loss: 0.0010 - dann loss: 2.2066 - reconstruction loss: 0.0352128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2079 - reconstruction loss: 0.0353128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2091 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2098 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2133 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2137 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2144 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2148 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2103 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2093 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2118 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2110 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2134 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2117 - reconstruction loss: 0.035629/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2116 - reconstruction loss: 0.03560/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2116 - reconstruction loss: 0.0356Epoch 64/150, Current strat Epoch 10/50
use_perm = False
switching perm
128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2399 - reconstruction loss: 0.0365128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2465 - reconstruction loss: 0.0361128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2346 - reconstruction loss: 0.0362128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2301 - reconstruction loss: 0.0363128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2196 - reconstruction loss: 0.0361128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2085 - reconstruction loss: 0.0360128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2013 - reconstruction loss: 0.0362128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2060 - reconstruction loss: 0.0362128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2080 - reconstruction loss: 0.0361128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2085 - reconstruction loss: 0.0361128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2061 - reconstruction loss: 0.0359128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2069 - reconstruction loss: 0.0359128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2086 - reconstruction loss: 0.0358128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2084 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2090 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2129 - reconstruction loss: 0.0358128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2105 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2126 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2133 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2149 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2120 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2125 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2125 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2106 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2117 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2117 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2114 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2116 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2106 - reconstruction loss: 0.035629/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2151 - reconstruction loss: 0.03560/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2151 - reconstruction loss: 0.0356Epoch 65/150, Current strat Epoch 11/50
use_perm = False
switching perm
128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2423 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2397 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2409 - reconstruction loss: 0.0353128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2338 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2291 - reconstruction loss: 0.0358128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2313 - reconstruction loss: 0.0359128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2346 - reconstruction loss: 0.0358128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2225 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2143 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2196 - reconstruction loss: 0.0353128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2226 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2163 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2163 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2134 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2143 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2166 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2168 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2145 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2158 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2136 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2123 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2107 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2091 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2097 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2103 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2118 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2123 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2112 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2114 - reconstruction loss: 0.035629/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2123 - reconstruction loss: 0.03560/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2123 - reconstruction loss: 0.0356Epoch 66/150, Current strat Epoch 12/50
use_perm = False
switching perm
128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2439 - reconstruction loss: 0.0372128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2385 - reconstruction loss: 0.0371128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2208 - reconstruction loss: 0.0368128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2277 - reconstruction loss: 0.0370128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2165 - reconstruction loss: 0.0366128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2234 - reconstruction loss: 0.0364128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2251 - reconstruction loss: 0.0364128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2203 - reconstruction loss: 0.0362128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2233 - reconstruction loss: 0.0363128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2194 - reconstruction loss: 0.0361128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2138 - reconstruction loss: 0.0361128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2108 - reconstruction loss: 0.0359128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2098 - reconstruction loss: 0.0361128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2116 - reconstruction loss: 0.0363128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2087 - reconstruction loss: 0.0363128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2089 - reconstruction loss: 0.0363128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2095 - reconstruction loss: 0.0362128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2116 - reconstruction loss: 0.0361128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2136 - reconstruction loss: 0.0360128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2134 - reconstruction loss: 0.0359128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2152 - reconstruction loss: 0.0359128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2155 - reconstruction loss: 0.0359128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2153 - reconstruction loss: 0.0358128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2161 - reconstruction loss: 0.0359128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2159 - reconstruction loss: 0.0358128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2138 - reconstruction loss: 0.0358128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2134 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2134 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2126 - reconstruction loss: 0.035629/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2086 - reconstruction loss: 0.03570/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2086 - reconstruction loss: 0.0357Epoch 67/150, Current strat Epoch 13/50
use_perm = False
switching perm
128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.1970 - reconstruction loss: 0.0342128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2299 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2247 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2122 - reconstruction loss: 0.0360128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2207 - reconstruction loss: 0.0363128/3742 - total loss: 0.0001 - classification loss: 0.0010 - dann loss: 2.2163 - reconstruction loss: 0.0360128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2053 - reconstruction loss: 0.0363128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2072 - reconstruction loss: 0.0360128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2063 - reconstruction loss: 0.0362128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2085 - reconstruction loss: 0.0363128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2064 - reconstruction loss: 0.0362128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2059 - reconstruction loss: 0.0361128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2073 - reconstruction loss: 0.0362128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2066 - reconstruction loss: 0.0362128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2093 - reconstruction loss: 0.0361128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2110 - reconstruction loss: 0.0361128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2081 - reconstruction loss: 0.0361128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2104 - reconstruction loss: 0.0361128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2088 - reconstruction loss: 0.0360128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2059 - reconstruction loss: 0.0360128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2073 - reconstruction loss: 0.0359128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2075 - reconstruction loss: 0.0360128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2085 - reconstruction loss: 0.0359128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2087 - reconstruction loss: 0.0358128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2080 - reconstruction loss: 0.0358128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2081 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2082 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2092 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2109 - reconstruction loss: 0.035629/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2142 - reconstruction loss: 0.03560/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2142 - reconstruction loss: 0.0356Epoch 68/150, Current strat Epoch 14/50
use_perm = False
switching perm
128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.1960 - reconstruction loss: 0.0352128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2001 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2155 - reconstruction loss: 0.0350128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2036 - reconstruction loss: 0.0352128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2053 - reconstruction loss: 0.0350128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2112 - reconstruction loss: 0.0350128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2172 - reconstruction loss: 0.0352128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2193 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2203 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2213 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2223 - reconstruction loss: 0.0353128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2183 - reconstruction loss: 0.0353128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2152 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2168 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2129 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2141 - reconstruction loss: 0.0353128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2140 - reconstruction loss: 0.0353128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2115 - reconstruction loss: 0.0352128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2133 - reconstruction loss: 0.0353128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2143 - reconstruction loss: 0.0353128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2165 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2159 - reconstruction loss: 0.0353128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2152 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2140 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2159 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2146 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2159 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2126 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2116 - reconstruction loss: 0.035529/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2118 - reconstruction loss: 0.03570/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2118 - reconstruction loss: 0.0357Epoch 69/150, Current strat Epoch 15/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2861 - reconstruction loss: 0.0345128/3742 - total loss: 0.0001 - classification loss: 0.0009 - dann loss: 2.2518 - reconstruction loss: 0.0352128/3742 - total loss: 0.0001 - classification loss: 0.0008 - dann loss: 2.2390 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2323 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0007 - dann loss: 2.2338 - reconstruction loss: 0.0352128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2315 - reconstruction loss: 0.0351128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2264 - reconstruction loss: 0.0351128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2258 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2258 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2236 - reconstruction loss: 0.0353128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2221 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2257 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2270 - reconstruction loss: 0.0353128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2219 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2221 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2268 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2266 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2238 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2194 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2189 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2191 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2208 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2180 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2176 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2185 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2156 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2136 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2127 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2114 - reconstruction loss: 0.035629/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2123 - reconstruction loss: 0.03560/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2123 - reconstruction loss: 0.0356Epoch 70/150, Current strat Epoch 16/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.1820 - reconstruction loss: 0.0351128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.1802 - reconstruction loss: 0.0350128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.1786 - reconstruction loss: 0.0352128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.1882 - reconstruction loss: 0.0349128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.1910 - reconstruction loss: 0.0349128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.1942 - reconstruction loss: 0.0352128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.1980 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2006 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2039 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.1970 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.1956 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.1973 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.1985 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2024 - reconstruction loss: 0.0353128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2011 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2028 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2037 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2046 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2071 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2090 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2091 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2096 - reconstruction loss: 0.0355128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2103 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2101 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2092 - reconstruction loss: 0.0356128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2097 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2112 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2103 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2107 - reconstruction loss: 0.035629/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2148 - reconstruction loss: 0.03550/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2148 - reconstruction loss: 0.0355Epoch 71/150, Current strat Epoch 17/50
use_perm = False
switching perm
128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2531 - reconstruction loss: 0.0361128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2202 - reconstruction loss: 0.0348128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2081 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.1947 - reconstruction loss: 0.0350128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.1962 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2009 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2017 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2053 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2092 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2121 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2108 - reconstruction loss: 0.0351128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2126 - reconstruction loss: 0.0352128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2132 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2145 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2193 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2184 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2165 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2150 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2159 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2151 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2154 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2136 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2126 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2129 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2126 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2114 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2137 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2145 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2127 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2081 - reconstruction loss: 0.03560/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2081 - reconstruction loss: 0.0356Epoch 72/150, Current strat Epoch 18/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2779 - reconstruction loss: 0.0359128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2521 - reconstruction loss: 0.0351128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2290 - reconstruction loss: 0.0351128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2317 - reconstruction loss: 0.0350128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2240 - reconstruction loss: 0.0354128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2257 - reconstruction loss: 0.0353128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2179 - reconstruction loss: 0.0357128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2144 - reconstruction loss: 0.0358128/3742 - total loss: 0.0001 - classification loss: 0.0006 - dann loss: 2.2132 - reconstruction loss: 0.0358128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2161 - reconstruction loss: 0.0358128/3742 - total loss: 0.0001 - classification loss: 0.0005 - dann loss: 2.2135 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2165 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2161 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2144 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2143 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2169 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2146 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2157 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2167 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2170 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2166 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2157 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2164 - reconstruction loss: 0.0352128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2152 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2138 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2136 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2134 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2131 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2118 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2112 - reconstruction loss: 0.03560/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2112 - reconstruction loss: 0.0356Epoch 73/150, Current strat Epoch 19/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2036 - reconstruction loss: 0.0344128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2433 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2297 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2256 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2281 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2336 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2313 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2323 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2265 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2244 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2214 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2163 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2150 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2131 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2135 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2107 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2099 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2117 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2105 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2106 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2134 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2121 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2106 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2139 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2145 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2138 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2133 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2133 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2123 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2094 - reconstruction loss: 0.03560/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2094 - reconstruction loss: 0.0356Epoch 74/150, Current strat Epoch 20/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2119 - reconstruction loss: 0.0329128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2010 - reconstruction loss: 0.0342128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2258 - reconstruction loss: 0.0349128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2364 - reconstruction loss: 0.0347128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2362 - reconstruction loss: 0.0348128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2248 - reconstruction loss: 0.0349128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2267 - reconstruction loss: 0.0350128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2285 - reconstruction loss: 0.0347128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2290 - reconstruction loss: 0.0350128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2210 - reconstruction loss: 0.0351128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2239 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2273 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2212 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2172 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2159 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2163 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2132 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2164 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2158 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2175 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2173 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2179 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2157 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2141 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2152 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2155 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2154 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2117 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2123 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2095 - reconstruction loss: 0.03550/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2095 - reconstruction loss: 0.0355Epoch 75/150, Current strat Epoch 21/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2413 - reconstruction loss: 0.0371128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2323 - reconstruction loss: 0.0364128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2178 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2244 - reconstruction loss: 0.0361128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2172 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2147 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2214 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2170 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2219 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2135 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2112 - reconstruction loss: 0.0361128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2146 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2124 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2103 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2095 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2102 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2076 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2092 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2103 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2110 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2097 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2103 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2124 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2135 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2123 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2111 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2098 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2118 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2124 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2093 - reconstruction loss: 0.03550/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2093 - reconstruction loss: 0.0355Epoch 76/150, Current strat Epoch 22/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2549 - reconstruction loss: 0.0373128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2091 - reconstruction loss: 0.0366128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2045 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2108 - reconstruction loss: 0.0361128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2161 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2178 - reconstruction loss: 0.0361128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2234 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2175 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2205 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2184 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2211 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2207 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2169 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2154 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2126 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2098 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2081 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2091 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2091 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2116 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2103 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2125 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2128 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2127 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2115 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2122 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2140 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2141 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2121 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2103 - reconstruction loss: 0.03540/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2103 - reconstruction loss: 0.0354Epoch 77/150, Current strat Epoch 23/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.1836 - reconstruction loss: 0.0380128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.1967 - reconstruction loss: 0.0370128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2022 - reconstruction loss: 0.0372128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2012 - reconstruction loss: 0.0368128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2082 - reconstruction loss: 0.0368128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2169 - reconstruction loss: 0.0366128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2173 - reconstruction loss: 0.0362128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2166 - reconstruction loss: 0.0363128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2165 - reconstruction loss: 0.0363128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2206 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2151 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2168 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2173 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2178 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2190 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2190 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2178 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2191 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2174 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2140 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2120 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2122 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2115 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2114 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2117 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2113 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2119 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2114 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2118 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2113 - reconstruction loss: 0.03560/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2113 - reconstruction loss: 0.0356Epoch 78/150, Current strat Epoch 24/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2562 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0005 - dann loss: 2.2341 - reconstruction loss: 0.0374128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2272 - reconstruction loss: 0.0363128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2061 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.1941 - reconstruction loss: 0.0351128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2067 - reconstruction loss: 0.0351128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2130 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2122 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2204 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2247 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2205 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2223 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2231 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2183 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2176 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2185 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2154 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2141 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2113 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2105 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2078 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2094 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2084 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2117 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2124 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2128 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2129 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2124 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2119 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2107 - reconstruction loss: 0.03560/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2107 - reconstruction loss: 0.0356Epoch 79/150, Current strat Epoch 25/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.1787 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2178 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2020 - reconstruction loss: 0.0363128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2022 - reconstruction loss: 0.0361128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2000 - reconstruction loss: 0.0361128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2046 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2001 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2002 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2000 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2019 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2061 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2129 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2086 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2124 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2151 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2132 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2132 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2114 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2110 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2119 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2098 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2059 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2081 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2074 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2076 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2072 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2073 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2094 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2114 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2123 - reconstruction loss: 0.03560/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2123 - reconstruction loss: 0.0356Epoch 80/150, Current strat Epoch 26/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.1974 - reconstruction loss: 0.0347128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2142 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2093 - reconstruction loss: 0.0352128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2009 - reconstruction loss: 0.0351128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2040 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.1930 - reconstruction loss: 0.0352128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2006 - reconstruction loss: 0.0352128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2018 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2028 - reconstruction loss: 0.0352128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2004 - reconstruction loss: 0.0351128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.1996 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2077 - reconstruction loss: 0.0352128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2054 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2100 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2092 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2136 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2139 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2118 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2146 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2126 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2115 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2121 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2136 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2133 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2097 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2105 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2104 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2098 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2115 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2120 - reconstruction loss: 0.03560/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2120 - reconstruction loss: 0.0356Epoch 81/150, Current strat Epoch 27/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2492 - reconstruction loss: 0.0376128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2389 - reconstruction loss: 0.0366128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2280 - reconstruction loss: 0.0362128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2239 - reconstruction loss: 0.0364128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2201 - reconstruction loss: 0.0363128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2157 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2150 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2165 - reconstruction loss: 0.0362128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2179 - reconstruction loss: 0.0361128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2173 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2161 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2188 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2184 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2176 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2188 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2177 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2210 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2212 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2194 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2162 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2153 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2150 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2134 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2129 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2117 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2121 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2115 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2101 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2111 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2136 - reconstruction loss: 0.03560/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2136 - reconstruction loss: 0.0356Epoch 82/150, Current strat Epoch 28/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2232 - reconstruction loss: 0.0351128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2188 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2280 - reconstruction loss: 0.0351128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2259 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2214 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2206 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2231 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2282 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2228 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2215 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2244 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2244 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2233 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2220 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2211 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2209 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2161 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2180 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2181 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2163 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2203 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2203 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2183 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2164 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2159 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2148 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2141 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2128 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2112 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2130 - reconstruction loss: 0.03560/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2130 - reconstruction loss: 0.0356Epoch 83/150, Current strat Epoch 29/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2006 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2031 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0004 - dann loss: 2.2184 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2126 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2174 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2248 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2226 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2294 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2245 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2243 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2253 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2203 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2215 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2196 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2164 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2176 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2205 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2236 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2229 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2199 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2154 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2137 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2110 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2123 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2116 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2121 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2115 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2127 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2111 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2133 - reconstruction loss: 0.03550/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2133 - reconstruction loss: 0.0355Epoch 84/150, Current strat Epoch 30/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2235 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2010 - reconstruction loss: 0.0364128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2045 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2039 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2208 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2288 - reconstruction loss: 0.0361128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2255 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2205 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.2221 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2233 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2256 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2257 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2235 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2264 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2237 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2222 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2191 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2149 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2142 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2155 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2184 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2167 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2146 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2146 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2162 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2140 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2139 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2129 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2110 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2137 - reconstruction loss: 0.03550/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2137 - reconstruction loss: 0.0355Epoch 85/150, Current strat Epoch 31/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2651 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2125 - reconstruction loss: 0.0344128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2145 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2100 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2058 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2063 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2100 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2160 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2159 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2167 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2153 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2148 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2128 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2122 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2163 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2116 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2098 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2101 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2074 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2060 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2070 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2066 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2072 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2068 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2101 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2110 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2121 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2122 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2124 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2090 - reconstruction loss: 0.03560/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2090 - reconstruction loss: 0.0356Epoch 86/150, Current strat Epoch 32/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1745 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2153 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2104 - reconstruction loss: 0.0362128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2042 - reconstruction loss: 0.0361128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2130 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2122 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2181 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2073 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2066 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2093 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2070 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2038 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2040 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2072 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2065 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2102 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2093 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2080 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2067 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2082 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2086 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2106 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2097 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2123 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2097 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2105 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2099 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2117 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2119 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2109 - reconstruction loss: 0.03560/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2109 - reconstruction loss: 0.0356Epoch 87/150, Current strat Epoch 33/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1797 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2004 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2143 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2111 - reconstruction loss: 0.0362128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2000 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2035 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2072 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2066 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2074 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2042 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2062 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2103 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2073 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2043 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2038 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2002 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2024 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2009 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2010 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2006 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1997 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2012 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1991 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2007 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2030 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2057 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2067 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2103 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2108 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2143 - reconstruction loss: 0.03560/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2143 - reconstruction loss: 0.0356Epoch 88/150, Current strat Epoch 34/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1807 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2033 - reconstruction loss: 0.0347128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2316 - reconstruction loss: 0.0347128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2238 - reconstruction loss: 0.0348128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2241 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2211 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2197 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2196 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2176 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2174 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2144 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2159 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2132 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2123 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2106 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2127 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2133 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2106 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2090 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2077 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2092 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2104 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2116 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2096 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2110 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2106 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2088 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2088 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2114 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2124 - reconstruction loss: 0.03560/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2124 - reconstruction loss: 0.0356Epoch 89/150, Current strat Epoch 35/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2136 - reconstruction loss: 0.0373128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2186 - reconstruction loss: 0.0371128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2134 - reconstruction loss: 0.0375128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2116 - reconstruction loss: 0.0375128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2051 - reconstruction loss: 0.0371128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2138 - reconstruction loss: 0.0364128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2140 - reconstruction loss: 0.0362128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2133 - reconstruction loss: 0.0361128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2099 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2086 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2105 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2158 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2176 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2176 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2151 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2144 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2150 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2162 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2144 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2165 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2144 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2147 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2141 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2130 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2128 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2125 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2129 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2125 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2124 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2091 - reconstruction loss: 0.03550/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2091 - reconstruction loss: 0.0355Epoch 90/150, Current strat Epoch 36/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2392 - reconstruction loss: 0.0362128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2184 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2005 - reconstruction loss: 0.0350128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1955 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2016 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2115 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2078 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2160 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2158 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2185 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2186 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2171 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2144 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2158 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2122 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2130 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2094 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2082 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2092 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2117 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2111 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2091 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2084 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2086 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2113 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2119 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2112 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2122 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2108 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2143 - reconstruction loss: 0.03570/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2143 - reconstruction loss: 0.0357Epoch 91/150, Current strat Epoch 37/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2015 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2071 - reconstruction loss: 0.0348128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2009 - reconstruction loss: 0.0343128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2014 - reconstruction loss: 0.0347128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2059 - reconstruction loss: 0.0348128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2064 - reconstruction loss: 0.0350128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2002 - reconstruction loss: 0.0352128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1994 - reconstruction loss: 0.0351128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1956 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1974 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1974 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1933 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1966 - reconstruction loss: 0.0352128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1987 - reconstruction loss: 0.0352128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2020 - reconstruction loss: 0.0352128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2029 - reconstruction loss: 0.0352128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2020 - reconstruction loss: 0.0352128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2035 - reconstruction loss: 0.0352128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2031 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2024 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2020 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2024 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2049 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2083 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2083 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2106 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2108 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2119 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2114 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2124 - reconstruction loss: 0.03570/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2124 - reconstruction loss: 0.0357Epoch 92/150, Current strat Epoch 38/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2216 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2219 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2200 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2332 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2232 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2172 - reconstruction loss: 0.0352128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2220 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2222 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2124 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2139 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2172 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2158 - reconstruction loss: 0.0352128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2174 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2160 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2161 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2162 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2149 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2124 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2133 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2137 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2125 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2110 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2128 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2114 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2129 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2130 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2131 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2119 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2118 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2110 - reconstruction loss: 0.03560/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2110 - reconstruction loss: 0.0356Epoch 93/150, Current strat Epoch 39/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0000 - dann loss: 2.1977 - reconstruction loss: 0.0338128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2283 - reconstruction loss: 0.0348128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2258 - reconstruction loss: 0.0352128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2116 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2061 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2055 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2074 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2068 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2072 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2098 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2091 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2093 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2056 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2088 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2115 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2115 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2117 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2129 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2133 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2110 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2108 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2086 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2073 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2080 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2085 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2100 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2073 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2093 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2116 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2118 - reconstruction loss: 0.03550/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2118 - reconstruction loss: 0.0355Epoch 94/150, Current strat Epoch 40/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0003 - dann loss: 2.1850 - reconstruction loss: 0.0379128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.1896 - reconstruction loss: 0.0363128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1974 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1933 - reconstruction loss: 0.0361128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1879 - reconstruction loss: 0.0364128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1829 - reconstruction loss: 0.0361128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1833 - reconstruction loss: 0.0361128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1831 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1812 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1878 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1824 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1868 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1875 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1872 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1898 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1943 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1935 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1988 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2013 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2021 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2047 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2051 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2065 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2080 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2095 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2103 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2094 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2085 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2098 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2176 - reconstruction loss: 0.03560/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2176 - reconstruction loss: 0.0356Epoch 95/150, Current strat Epoch 41/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1307 - reconstruction loss: 0.0362128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1776 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1976 - reconstruction loss: 0.0362128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2034 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2051 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1898 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1919 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1963 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2004 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2013 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2020 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2022 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2057 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2045 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2040 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2060 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2089 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2078 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2100 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2116 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2119 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2121 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2138 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2141 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2123 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2104 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2115 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2125 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2115 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2122 - reconstruction loss: 0.03560/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2122 - reconstruction loss: 0.0356Epoch 96/150, Current strat Epoch 42/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1797 - reconstruction loss: 0.0348128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1718 - reconstruction loss: 0.0348128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1989 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2028 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1985 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2077 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2096 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2084 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2063 - reconstruction loss: 0.0352128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2047 - reconstruction loss: 0.0351128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2070 - reconstruction loss: 0.0352128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2108 - reconstruction loss: 0.0352128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2076 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2070 - reconstruction loss: 0.0352128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2094 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2099 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2117 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2164 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2142 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2129 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2121 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2114 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2099 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2091 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2112 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2109 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2107 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2121 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2114 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2125 - reconstruction loss: 0.03570/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2125 - reconstruction loss: 0.0357Epoch 97/150, Current strat Epoch 43/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1671 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1992 - reconstruction loss: 0.0348128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2096 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2174 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2141 - reconstruction loss: 0.0361128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2212 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2330 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2340 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2295 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2268 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2229 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2206 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2215 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2210 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2198 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2186 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2169 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2147 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2131 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2125 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2154 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2144 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2115 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2108 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2111 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2105 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2122 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2118 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2126 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2085 - reconstruction loss: 0.03550/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2085 - reconstruction loss: 0.0355Epoch 98/150, Current strat Epoch 44/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1911 - reconstruction loss: 0.0366128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1954 - reconstruction loss: 0.0352128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2012 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2039 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2174 - reconstruction loss: 0.0361128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2094 - reconstruction loss: 0.0361128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2109 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2078 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2080 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1980 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1995 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2026 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2042 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2082 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2134 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2152 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2163 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2124 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2087 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2082 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2089 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2121 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2137 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2134 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2137 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2134 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2138 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2094 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2119 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2108 - reconstruction loss: 0.03550/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2108 - reconstruction loss: 0.0355Epoch 99/150, Current strat Epoch 45/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2494 - reconstruction loss: 0.0367128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2506 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2460 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2390 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2327 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2347 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2292 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2337 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2250 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2243 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2260 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2263 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2254 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2240 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2273 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2270 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2267 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2249 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2253 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2219 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2214 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2223 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2192 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2169 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2135 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2133 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2122 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2112 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2113 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2129 - reconstruction loss: 0.03560/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2129 - reconstruction loss: 0.0356Epoch 100/150, Current strat Epoch 46/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1899 - reconstruction loss: 0.0362128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1773 - reconstruction loss: 0.0361128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2038 - reconstruction loss: 0.0368128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2127 - reconstruction loss: 0.0364128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2227 - reconstruction loss: 0.0364128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2168 - reconstruction loss: 0.0366128/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2119 - reconstruction loss: 0.0367128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2120 - reconstruction loss: 0.0367128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2119 - reconstruction loss: 0.0366128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2012 - reconstruction loss: 0.0364128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2045 - reconstruction loss: 0.0364128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2087 - reconstruction loss: 0.0362128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2110 - reconstruction loss: 0.0361128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2112 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2097 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2080 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2092 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2106 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2124 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2107 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2110 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2140 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2133 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2148 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2142 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2117 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2102 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2124 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2123 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2096 - reconstruction loss: 0.03560/3742 - total loss: 0.0000 - classification loss: 0.0002 - dann loss: 2.2096 - reconstruction loss: 0.0356Epoch 101/150, Current strat Epoch 47/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2418 - reconstruction loss: 0.0372128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2282 - reconstruction loss: 0.0376128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1945 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1960 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2086 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2150 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2216 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2270 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2144 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2170 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2133 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2145 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2201 - reconstruction loss: 0.0359128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2171 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2152 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2149 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2148 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2116 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2101 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2103 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2114 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2110 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2131 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2128 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2136 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2133 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2135 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2136 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2120 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2106 - reconstruction loss: 0.03560/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2106 - reconstruction loss: 0.0356Epoch 102/150, Current strat Epoch 48/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2332 - reconstruction loss: 0.0348128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2242 - reconstruction loss: 0.0343128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2191 - reconstruction loss: 0.0340128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2104 - reconstruction loss: 0.0344128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2108 - reconstruction loss: 0.0345128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2081 - reconstruction loss: 0.0345128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2087 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2128 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2098 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2044 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1998 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1972 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1981 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2048 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2042 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2024 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2026 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2045 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2049 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2078 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2112 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2094 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2096 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2088 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2083 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2094 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2095 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2120 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2125 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2090 - reconstruction loss: 0.03560/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2090 - reconstruction loss: 0.0356Epoch 103/150, Current strat Epoch 49/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1910 - reconstruction loss: 0.0349128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1979 - reconstruction loss: 0.0348128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2020 - reconstruction loss: 0.0351128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2182 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2172 - reconstruction loss: 0.0362128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2091 - reconstruction loss: 0.0363128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2059 - reconstruction loss: 0.0362128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2021 - reconstruction loss: 0.0361128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2001 - reconstruction loss: 0.0360128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2015 - reconstruction loss: 0.0358128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2034 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2035 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2007 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2041 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2019 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2047 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2041 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2039 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2023 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2051 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2081 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2066 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2103 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2129 - reconstruction loss: 0.0357128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2122 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2143 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2107 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2115 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2120 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2105 - reconstruction loss: 0.03570/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2105 - reconstruction loss: 0.0357Epoch 104/150, Current strat Epoch 50/50
use_perm = False
switching perm
128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1492 - reconstruction loss: 0.0329128/3742 - total loss: 0.0000 - classification loss: 0.0000 - dann loss: 2.1718 - reconstruction loss: 0.0345128/3742 - total loss: 0.0000 - classification loss: 0.0000 - dann loss: 2.1881 - reconstruction loss: 0.0351128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1894 - reconstruction loss: 0.0349128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1891 - reconstruction loss: 0.0350128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2021 - reconstruction loss: 0.0348128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2052 - reconstruction loss: 0.0351128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2048 - reconstruction loss: 0.0349128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2026 - reconstruction loss: 0.0351128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1977 - reconstruction loss: 0.0350128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1999 - reconstruction loss: 0.0351128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2001 - reconstruction loss: 0.0352128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1968 - reconstruction loss: 0.0353128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1983 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1985 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.1979 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2000 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2001 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2046 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2095 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2112 - reconstruction loss: 0.0354128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2111 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2141 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2147 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2150 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2145 - reconstruction loss: 0.0355128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2156 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2137 - reconstruction loss: 0.0356128/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2121 - reconstruction loss: 0.035629/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2101 - reconstruction loss: 0.03560/3742 - total loss: 0.0000 - classification loss: 0.0001 - dann loss: 2.2101 - reconstruction loss: 0.0356['Secretory N' 'Suprabasal N' 'Secretory' ... 'Basal' 'Multiciliated N'
 'Suprabasal']
Save adata_pred to /data/analysis/data_becavin/scmusketeers/data/Deprez-Lung-unknown-0.2-pred.h5ad
